{"id":"S542","summaryText":"[SUMMARY S542] 30.3 KB | code | 678 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\embeddings.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Dual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n3: \"\"\"\nâ†’ Use /swarm retrieve S542 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\embeddings.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Dual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n3: \"\"\"\n4: import asyncio\n5: import httpx\n6: import logging\n7: from typing import List\n8: from urllib.parse import urlparse\n9: from app.config import settings\n10: \n11: logger = logging.getLogger(__name__)\n12: \n13: \n14: class EmbeddingError(Exception):\n15:     \"\"\"Exception raised for embedding service errors.\"\"\"\n16:     pass\n17: \n18: \n19: class EmbeddingService:\n20:     \"\"\"Service for generating text embeddings via Ollama or OpenAI-compatible APIs.\"\"\"\n21:     \n22:     # Hard caps for input validation\n23:     MAX_BATCH_SIZE = 512  # Maximum number of texts per batch call\n24:     MAX_TEXT_LENGTH = 8192  # Maximum characters per text (derived from chunk_size_chars=8192)\n25:     MIN_SPLIT_CHARS = 200  # Minimum text length to attempt single-text splitting\n26: \n27:     def __init__(self):\n28:         \"\"\"Initialize the embedding service with HTTP client and provider detection.\"\"\"\n29:         base_url = settings.ollama_embedding_url\n30: \n31:         # Validate base_url\n32:         if not base_url:\n33:             raise EmbeddingError(\"Embedding service is not configured\")\n34:         if not base_url.startswith(('http://', 'https://')):\n35:             raise EmbeddingError(\"Invalid embedding URL configuration\")\n36: \n37:         # Detect provider mode based on URL path\n38:         self.provider_mode, self.embeddings_url = self._detect_provider_mode(base_url)\n39:         self.timeout = 60.0\n40:         \n41:         # Read embedding prefixes from settings\n42:         self.embedding_doc_prefix = settings.embedding_doc_prefix\n43:         self.embedding_query_prefix = settings.embedding_query_prefix\n44:         \n45:         # Auto-apply Qwen3 instruction prefixes for better retrieval quality\n46:         # With llama.cpp -ub 8192, we have plenty of headroom for these prefixes\n47:         if settings.embedding_model.lower().find(\"qwen\") >= 0:\n48:             if not self.embedding_doc_prefix:\n49:                 self.embedding_doc_prefix = \"Instruct: Represent this technical documentation passage for retrieval.\\nDocument: \"\n50:             if not self.embedding_query_prefix:\n51:                 self.embedding_query_prefix = \"Instruct: Retrieve relevant technical documentation passages.\\nQuery: \"\n52:     \n53:     def _detect_provider_mode(self, base_url: str) -> tuple:\n54:         \"\"\"\n55:         Detect which embedding provider mode to use based on URL path.\n56:         \n57:         Detection strategy:\n58:         - If URL path includes '/api/embeddings' -> Ollama mode\n59:         - If URL path includes '/v1/embeddings' -> OpenAI mode\n60:         - If no explicit embeddings path:\n61:           - Port 1234 -> OpenAI mode (LM Studio default)\n62:           - Otherwise -> Ollama mode\n63:         \n64:         Args:\n65:             base_url: The configured embedding URL\n66:             \n67:         Returns:\n68:             Tuple of (provider_mode, embeddings_url)\n69:         \"\"\"\n70:         parsed = urlparse(base_url)\n71:         path = parsed.path\n72:         \n73:         # Check for explicit paths\n74:         if '/api/embeddings' in path:\n75:             # Already has Ollama path, use as-is\n76:             return ('ollama', base_url)\n77:         elif '/v1/embeddings' in path:\n78:             # Already has OpenAI path, use as-is\n79:             return ('openai', base_url)\n80:         \n81:         # No explicit path - determine by port\n82:         port = parsed.port\n83:         if port == 1234:\n84:             # LM Studio default port - use OpenAI mode\n85:             base_url = base_url.rstrip('/') + '/v1/embeddings'\n86:             return ('openai', base_url)\n87:         else:\n88:             # Default to Ollama mode\n89:             base_url = base_url.rstrip('/') + '/api/embeddings'\n90:             return ('ollama', base_url)\n91:     \n92:     def _build_payload(self, text: str) -> dict:\n93:         \"\"\"\n94:         Build the API request payload based on provider mode.\n95:         \n96:         Args:\n97:             text: The text to embed\n98:             \n99:         Returns:\n100:             Dictionary payload for the API request\n101:         \"\"\"\n102:         if self.provider_mode == 'openai':\n103:             return {\n104:                 \"model\": settings.embedding_model,\n105:                 \"input\": text\n106:             }\n107:         else:  # ollama mode\n108:             return {\n109:                 \"model\": settings.embedding_model,\n110:                 \"prompt\": text\n111:             }\n112:     \n113:     def _extract_embedding(self, data: dict) -> List[float]:\n114:         \"\"\"\n115:         Extract embedding vector from API response based on provider mode.\n116:         \n117:         Args:\n118:             data: Parsed JSON response from the API\n119:             \n120:         Returns:\n121:             List of float values representing the embedding vector\n122:             \n123:         Raises:\n124:             EmbeddingError: If embedding cannot be extracted\n125:         \"\"\"\n126:         if self.provider_mode == 'openai':\n127:             # OpenAI format: data[0].embedding\n128:             if \"data\" not in data:\n129:                 logger.error(\"Embedding API response missing 'data' field in OpenAI mode\")\n130:                 raise EmbeddingError(\"Embedding API response is invalid\")\n131:             if not isinstance(data[\"data\"], list) or len(data[\"data\"]) == 0:\n132:                 logger.error(\"Embedding API response 'data' field is empty or invalid in OpenAI mode\")\n133:                 raise EmbeddingError(\"Embedding API response is invalid\")\n134:             embedding = data[\"data\"][0].get(\"embedding\")\n135:             if embedding is None:\n136:                 logger.error(\"Embedding API response missing 'data[0].embedding' field in OpenAI mode\")\n137:                 raise EmbeddingError(\"Embedding API response is invalid\")\n138:         else:  # ollama mode\n139:             # Ollama format: embedding\n140:             embedding = data.get(\"embedding\")\n141:             if embedding is None:\n142:                 logger.error(\"Embedding API response missing 'embedding' field in Ollama mode\")\n143:                 raise EmbeddingError(\"Embedding API response is invalid\")\n144:         \n145:         return embedding\n146:     \n147:     async def embed_single(self, text: str) -> List[float]:\n148:         \"\"\"\n149:         Generate embedding for a single text.\n150: \n151:         Applies the query prefix (if configured) to the input text before embedding.\n152:         The query prefix is used for retrieval queries and must remain constant for\n153:         a given index to ensure consistent embedding space.\n154: \n155:         Args:\n156:             text: The text to embed.\n157: \n158:         Returns:\n159:             List of float values representing the embedding vector.\n160: \n161:         Raises:\n162:             EmbeddingError: If the API request fails or returns non-200 status.\n163:         \"\"\"\n164:         # Validate text input\n165:         if text is None:\n166:             raise EmbeddingError(\"Text cannot be None\")\n167:         if not text.strip():\n168:             raise EmbeddingError(\"Text cannot be empty or whitespace only\")\n169: \n170:         # Apply query prefix for retrieval queries\n171:         text_to_embed = self.embedding_query_prefix + text if self.embedding_query_prefix else text\n172: \n173:         async with httpx.AsyncClient(timeout=self.timeout) as client:\n174:             try:\n175:                 response = await client.post(\n176:                     self.embeddings_url,\n177:                     json=self._build_payload(text_to_embed)\n178:                 )\n179:                 \n180:                 if response.status_code != 200:\n181:                     logger.warning(\n182:                         f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n183:                     )\n184:                     raise EmbeddingError(\n185:                         f\"Embedding API returned status {response.status_code}\"\n186:                     )\n187: \n188:                 try:\n189:                     data = response.json()\n190:                 except ValueError as e:\n191:                     logger.warning(\n192:                         f\"Invalid JSON response from embedding API for {self.provider_mode} mode: {e}, response: {response.text}\"\n193:                     )\n194:                     raise EmbeddingError(\"Invalid response from embedding service\")\n195: \n196:                 return self._extract_embedding(data)\n197:                 \n198:             except httpx.TimeoutException as e:\n199:                 raise EmbeddingError(\"Embedding request timed out\")\n200:             except httpx.HTTPError as e:\n201:                 raise EmbeddingError(\"Embedding HTTP error occurred\")\n202:     \n203:     async def validate_embedding_dimension(self, expected_dim: int) -> bool:\n204:         \"\"\"\n205:         Validate that the embedding dimension matches the expected value.\n206: \n207:         Args:\n208:             expected_dim: The expected dimension of the embedding vector.\n209:                 Must be a positive integer.\n210: \n211:         Returns:\n212:             True if the dimension matches.\n213: \n214:         Raises:\n215:             EmbeddingError: If expected_dim is invalid or if the dimension\n216:                 does not match the expected value.\n217:         \"\"\"\n218:         # Validate expected_dim input\n219:         if expected_dim is None:\n220:             raise EmbeddingError(\"expected_dim cannot be None\")\n221:         if not isinstance(expected_dim, int) or expected_dim <= 0:\n222:             raise EmbeddingError(f\"expected_dim must be a positive integer, got {expected_dim}\")\n223: \n224:         embedding = await self.embed_single('dimension_check')\n225:         actual_dim = len(embedding)\n226:         if actual_dim != expected_dim:\n227:             raise EmbeddingError(\n228:                 f\"Embedding dimension mismatch: expected {expected_dim}, got {actual_dim}\"\n229:             )\n230:         return True\n231: \n232:     async def embed_batch(self, texts: List[str], batch_size: int | None = None) -> List[List[float]]:\n233:         \"\"\"\n234:         Generate embeddings for a batch of texts using true API batching.\n235: \n236:         Sends multiple texts per API request for efficient GPU utilization.\n237:         Processes in batches of up to 512 (configurable) with up to 4\n238:         concurrent batch requests.\n239: \n240:         Applies the document prefix (if configured) to each input text before embedding.\n241:         The document prefix is used for document embeddings and must remain constant for\n242:         a given index to ensure consistent embedding space.\n243: \n244:         Args:\n245:             texts: List of texts to embed.\n246:             batch_size: Number of texts per API request (default: 512).\n247: \n248:         Returns:\n249:             List of embedding vectors, one for each input text, in order.\n250: \n251:         Raises:\n252:             EmbeddingError: If any API request fails.\n253:         \"\"\"\n254:         if not texts:\n255:             return []\n256:         \n257:         # Input validation guards\n258:         for idx, text in enumerate(texts):\n259:             if text is None:\n260:                 raise EmbeddingError(f\"Text at index {idx} is None\")\n261:             if not text.strip():\n262:                 raise EmbeddingError(f\"Text at index {idx} is empty or whitespace only\")\n263:             if len(text) > self.MAX_TEXT_LENGTH:\n264:                 raise EmbeddingError(\n265:                     f\"Text at index {idx} exceeds maximum length ({self.MAX_TEXT_LENGTH} characters)\"\n266:                 )\n267:         \n268:         # Use configured batch size if not specified\n269:         if batch_size is None:\n270:             batch_size = settings.embedding_batch_size\n271:         \n272:         # Clamp batch_size to valid range\n273:         batch_size = max(1, min(batch_size, self.MAX_BATCH_SIZE))\n274:         \n275:         # Apply document prefix to all texts\n276:         texts_to_embed = []\n277:         for text in texts:\n278:             if self.embedding_doc_prefix:\n279:                 texts_to_embed.append(self.embedding_doc_prefix + text)\n280:             else:\n281:                 texts_to_embed.append(text)\n282:         \n283:         # Process in batches using true API batching\n284:         all_embeddings: List[List[float]] = []\n285:         for i in range(0, len(texts_to_embed), batch_size):\n286:             batch = texts_to_embed[i:i + batch_size]\n287:             embeddings = await self._embed_batch_api(batch)\n288:             all_embeddings.extend(embeddings)\n289:         \n290:         return all_embeddings\n291: \n292:     async def _embed_batch_api(self, texts: List[str]) -> List[List[float]]:\n293:         \"\"\"\n294:         Send a batch of texts to the embedding API in a single request.\n295:         \n296:         Implements adaptive batching: automatically retries with smaller sub-batches\n297:         when llama.cpp token overflow errors occur.\n298: \n299:         Args:\n300:             texts: List of texts to embed (already prefixed).\n301: \n302:         Returns:\n303:             List of embedding vectors in the same order as input texts.\n304: \n305:         Raises:\n306:             EmbeddingError: If the API request fails after all retries.\n307:         \"\"\"\n308:         max_retries = settings.embedding_batch_max_retries\n309:         min_sub_size = settings.embedding_batch_min_sub_size\n310:         \n311:         async with httpx.AsyncClient(timeout=self.timeout) as client:\n312:             return await self._embed_batch_with_retry(client, texts, max_retries, min_sub_size)\n313:     \n314:     async def _embed_batch_with_retry(self, client: httpx.AsyncClient, texts: List[str], \n315:                                       max_retries: int, min_sub_size: int, retry_count: int = 0) -> List[List[float]]:\n316:         \"\"\"\n317:         Internal method that handles the retry logic for adaptive batching.\n318:         \n319:         Args:\n320:             client: HTTP client for making requests\n321:             texts: List of texts to embed\n322:             max_retries: Maximum number of retry attempts\n323:             min_sub_size: Minimum sub-batch size before giving up\n324:             \n325:         Returns:\n326:             List of embedding vectors\n327:             \n328:         Raises:\n329:             EmbeddingError: If all retries fail\n330:         \"\"\"\n331:         # Empty-input guard\n332:         if not texts:\n333:             return []\n334:         \n335:         try:\n336:             # Build payload with array of inputs\n337:             if self.provider_mode == 'openai':\n338:                 payload = {\n339:                     \"model\": settings.embedding_model,\n340:                     \"input\": texts\n341:                 }\n342:             else:  # ollama mode\n343:                 payload = {\n344:                     \"model\": settings.embedding_model,\n345:                     \"input\": texts\n346:                 }\n347:             \n348:             response = await client.post(\n349:                 self.embeddings_url,\n350:                 json=payload\n351:             )\n352:             \n353:             # Check for token overflow error in HTTP 500 responses\n354:             if response.status_code == 500:\n355:                 error_text = response.text.lower()\n356:                 if self._is_token_overflow_error(error_text):\n357:                     logger.warning(\n358:                         f\"Token overflow error for {self.provider_mode} mode: {response.text}\"\n359:                     )\n360:                     # Handle overflow using the shared helper\n361:                     return await self._handle_overflow_retry(\n362:                         client, texts, max_retries, min_sub_size, retry_count\n363:                     )\n364:             \n365:             if response.status_code != 200:\n366:                 logger.warning(\n367:                     f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n368:                 )\n369:                 raise EmbeddingError(\n370:                     f\"Embedding API returned status {response.status_code}\"\n371:                 )\n372: \n373:             data = response.json()\n374:             \n375:             # Extract embeddings from response\n376:             if self.provider_mode == 'openai':\n377:                 # OpenAI format: data[].embedding\n378:                 embeddings = [item['embedding'] for item in data['data']]\n379:             else:\n380:                 # Ollama format may vary - try common formats\n381:                 if 'embeddings' in data:\n382:                     embeddings = data['embeddings']\n383:                 elif 'embedding' in data:\n384:                     # Single embedding returned - shouldn't happen with batch\n385:                     embeddings = [data['embedding']]\n386:                 else:\n387:                     logger.error(\n388:                         f\"Unexpected response format for {self.provider_mode} mode: {data.keys()}\"\n389:                     )\n390:                     raise EmbeddingError(\"Unexpected response from embedding service\")\n391:             \n392:             # Validate embedding structure\n393:             if not isinstance(embeddings, list):\n394:                 logger.error(\"Embedding API response 'embeddings' is not a list\")\n395:                 raise EmbeddingError(\"Embedding API response is invalid\")\n396:             for i, emb in enumerate(embeddings):\n397:                 if not isinstance(emb, list):\n398:                     logger.error(f\"Embedding at index {i} is not a list\")\n399:                     raise EmbeddingError(\"Embedding API response is invalid\")\n400:                 for j, val in enumerate(emb):\n401:                     if not isinstance(val, (int, float)):\n402:                         logger.error(f\"Embedding value at [{i}][{j}] is not a number\")\n403:                         raise EmbeddingError(\"Embedding API response is invalid\")\n404:             \n405:             # Validate embedding count matches input count\n406:             if len(embeddings) != len(texts):\n407:                 logger.error(\n408:                     f\"Embedding count mismatch for {self.provider_mode} mode: expected {len(texts)}, got {len(embeddings)}\"\n409:                 )\n410:                 raise EmbeddingError(\n411:                     f\"Embedding count mismatch: expected {len(texts)}, got {len(embeddings)}\"\n412:                 )\n413:             \n414:             return embeddings\n415:             \n416:         except httpx.TimeoutException as e:\n417:             logger.warning(\n418:                 f\"Embedding batch request timed out for {self.provider_mode} mode: {e}\"\n419:             )\n420:             raise EmbeddingError(\"Embedding batch request timed out\")\n421:         except httpx.HTTPError as e:\n422:             # Check if this is a token overflow error\n423:             error_msg = str(e)\n424:             response_text = \"\"\n425:             \n426:             # Try to get response text from the exception if available\n427:             try:\n428:                 resp = getattr(e, 'response', None)\n429:                 if resp is not None:\n430:                     response_text = resp.text.lower()\n431:             except Exception:\n432:                 pass\n433:             \n434:             if self._is_token_overflow_error(error_msg) or self._is_token_overflow_error(response_text):\n435:                 logger.warning(\n436:                     f\"Token overflow error for {self.provider_mode} mode: {response_text}\"\n437:                 )\n438:                 # Handle overflow using the shared helper\n439:                 return await self._handle_overflow_retry(\n440:                     client, texts, max_retries, min_sub_size, retry_count\n441:                 )\n442:             else:\n443:                 # Not a token overflow error, re-raise\n444:                 logger.error(\n445:                     f\"Embedding batch HTTP error for {self.provider_mode} mode: {e}\"\n446:                 )\n447:                 raise EmbeddingError(\"Embedding batch HTTP error occurred\")\n448: \n449:     def _split_text_at_midpoint(self, text: str) -> tuple:\n450:         \"\"\"\n451:         Split a single text into two parts at a boundary-aware midpoint.\n452:         \n453:         Prefers splitting at newline or space characters near the midpoint\n454:         to produce more natural splits. Falls back to strict midpoint if\n455:         boundary-aware split would result in empty sides.\n456:         \n457:         Args:\n458:             text: The text to split\n459:             \n460:         Returns:\n461:             Tuple of (left_text, right_text), both non-empty for splittable text\n462:         \"\"\"\n463:         if len(text) <= 1:\n464:             return (text, \"\")\n465:         \n466:         midpoint = len(text) // 2\n467:         \n468:         # Try to find a better boundary near the midpoint\n469:         # Look for newline first, then space\n470:         search_start = max(0, midpoint - 50)\n471:         search_end = min(len(text), midpoint + 50)\n472:         \n473:         # Search for newline near midpoint (forward)\n474:         for i in range(midpoint, search_end):\n475:             if text[i] == '\\n':\n476:                 left, right = text[:i+1], text[i+1:]\n477:                 # Ensure both sides are non-empty for splittable text\n478:                 if left and right:\n479:                     return (left, right)\n480:         \n481:         # Search for newline near midpoint (backward)\n482:         for i in range(midpoint - 1, search_start - 1, -1):\n483:             if text[i] == '\\n':\n484:                 left, right = text[:i+1], text[i+1:]\n485:                 if left and right:\n486:                     return (left, right)\n487:         \n488:         # Search for space near midpoint (forward)\n489:         for i in range(midpoint, search_end):\n490:             if text[i] == ' ':\n491:                 left, right = text[:i], text[i:]\n492:                 if left and right:\n493:                     return (left, right)\n494:         \n495:         # Search for space near midpoint (backward)\n496:         for i in range(midpoint - 1, search_start - 1, -1):\n497:             if text[i] == ' ':\n498:                 left, right = text[:i], text[i:]\n499:                 if left and right:\n500:                     return (left, right)\n501:         \n502:         # Fall back to strict midpoint (guaranteed non-empty for len > 1)\n503:         left, right = text[:midpoint], text[midpoint:]\n504:         # Double-check: if either is empty, adjust to ensure both non-empty\n505:         if not left:\n506:             left = text[:1]\n507:             right = text[1:]\n508:         elif not right:\n509:             right = text[-1:]\n510:             left = text[:-1]\n511:         \n512:         return (left, right)\n513:     \n514:     def _mean_pool_embeddings(self, emb1: List[float], emb2: List[float]) -> List[float]:\n515:         \"\"\"\n516:         Mean-pool two embedding vectors into one.\n517:         \n518:         Args:\n519:             emb1: First embedding vector\n520:             emb2: Second embedding vector\n521:             \n522:         Returns:\n523:             Mean-pooled embedding vector\n524:         \"\"\"\n525:         if len(emb1) != len(emb2):\n526:             raise EmbeddingError(\n527:                 f\"Cannot mean-pool embeddings of different dimensions: {len(emb1)} vs {len(emb2)}\"\n528:             )\n529:         \n530:         return [(a + b) / 2.0 for a, b in zip(emb1, emb2)]\n531:     \n532:     async def _handle_overflow_retry(self, client: httpx.AsyncClient, texts: List[str],\n533:                                      max_retries: int, min_sub_size: int, retry_count: int) -> List[List[float]]:\n534:         \"\"\"\n535:         Helper method to handle overflow retry logic with bounded retries and minimum split size.\n536:         \n537:         For single-item overflow, attempts to split the text and mean-pool the results.\n538:         For multi-item overflow, splits the batch and processes each half.\n539:         \n540:         Args:\n541:             client: HTTP client for making requests\n542:             texts: List of texts to embed\n543:             max_retries: Maximum number of retry attempts\n544:             min_sub_size: Minimum sub-batch size before giving up\n545:             retry_count: Current retry attempt count\n546:             \n547:         Returns:\n548:             List of embedding vectors\n549:             \n550:         Raises:\n551:             EmbeddingError: If bounded retries exhausted or split size too small\n552:         \"\"\"\n553:         # Check if we've exhausted retries\n554:         if retry_count > max_retries:\n555:             logger.error(\n556:                 f\"Max retries ({max_retries}) exhausted for embedding batch in {self.provider_mode} mode\"\n557:             )\n558:             raise EmbeddingError(\n559:                 f\"Max retries ({max_retries}) exhausted for embedding batch\"\n560:             )\n561:         \n562:         # Handle single-item overflow with text splitting\n563:         if len(texts) == 1:\n564:             single_text = texts[0]\n565:             \n566:             # Check if text is too short to split - raise actionable error\n567:             if len(single_text) < self.MIN_SPLIT_CHARS:\n568:                 logger.warning(\n569:                     f\"Single input ({len(single_text)} chars) is below minimum split threshold ({self.MIN_SPLIT_CHARS}) in {self.provider_mode} mode\"\n570:                 )\n571:                 raise EmbeddingError(\n572:                     f\"Single input ({len(single_text)} chars) exceeds token limit and is too short to split. \"\n573:                     f\"Ensure chunk_size_chars is below server batch size limit (minimum {self.MIN_SPLIT_CHARS} chars required for recovery).\"\n574:                 )\n575:             \n576:             # Split text at boundary-aware midpoint and recurse\n577:             left_text, right_text = self._split_text_at_midpoint(single_text)\n578:             \n579:             # Guard: if either side is empty after split, raise actionable error\n580:             if not left_text or not right_text:\n581:                 logger.error(\n582:                     f\"Text split produced empty side (left={len(left_text)}, right={len(right_text)}) for text of length {len(single_text)}\"\n583:                 )\n584:                 raise EmbeddingError(\n585:                     f\"Cannot split text for embedding recovery: split produced empty segment. \"\n586:                     f\"Ensure chunk_size_chars is within server limits.\"\n587:                 )\n588:             \n589:             logger.info(\n590:                 f\"Splitting single input ({len(single_text)} chars) into parts ({len(left_text)} + {len(right_text)} chars), retry {retry_count}\"\n591:             )\n592:             \n593:             # Small bounded async backoff\n594:             backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n595:             await asyncio.sleep(backoff_delay)\n596:             \n597:             # Recurse on each part with incremented retry count\n598:             left_embeddings = await self._embed_batch_with_retry(\n599:                 client, [left_text], max_retries, min_sub_size, retry_count=retry_count + 1\n600:             )\n601:             right_embeddings = await self._embed_batch_with_retry(\n602:                 client, [right_text], max_retries, min_sub_size, retry_count=retry_count + 1\n603:             )\n604:             \n605:             # Mean-pool the two embeddings into one\n606:             logger.debug(\"Using mean-pooling for overflow recovery\")\n607:             pooled = self._mean_pool_embeddings(left_embeddings[0], right_embeddings[0])\n608:             \n609:             # Return single embedding to preserve one-embedding-per-input contract\n610:             return [pooled]\n611:         \n612:         # Multi-item batch overflow - use existing split behavior\n613:         # Check if we've reached minimum split size\n614:         if len(texts) <= min_sub_size:\n615:             logger.warning(\n616:                 f\"Cannot split batch further in {self.provider_mode} mode: {len(texts)} items below minimum split size ({min_sub_size})\"\n617:             )\n618:             raise EmbeddingError(\n619:                 f\"Cannot split batch further: {len(texts)} items below minimum split size\"\n620:             )\n621:         \n622:         # Split at midpoint and recurse with backoff\n623:         midpoint = len(texts) // 2\n624:         left_texts = texts[:midpoint]\n625:         right_texts = texts[midpoint:]\n626:         \n627:         # Small bounded async backoff (exponential, capped at 1s)\n628:         backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n629:         await asyncio.sleep(backoff_delay)\n630:         \n631:         # Recurse on left then right to preserve order\n632:         left_embeddings = await self._embed_batch_with_retry(\n633:             client, left_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n634:         )\n635:         right_embeddings = await self._embed_batch_with_retry(\n636:             client, right_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n637:         )\n638:         \n639:         return left_embeddings + right_embeddings\n640:     \n641:     def _is_token_overflow_error(self, error_msg: str) -> bool:\n642:         \"\"\"\n643:         Detect if an error message indicates a token overflow from llama.cpp.\n644:         \n645:         Args:\n646:             error_msg: The error message string\n647:             \n648:         Returns:\n649:             True if this is a token overflow error, False otherwise\n650:         \"\"\"\n651:         error_lower = error_msg.lower()\n652:         \n653:         # Check for common llama.cpp token overflow patterns\n654:         # Pattern 1: \"input (X tokens) is too large\" - typical llama.cpp error\n655:         if \"input (\" in error_lower and \"tokens) is too large\" in error_lower:\n656:             return True\n657:         \n658:         # Pattern 2: \"too large to process\" with \"current batch size\" - OpenAI mode error\n659:         if \"too large to process\" in error_lower and \"current batch size\" in error_lower:\n660:             return True\n661:         \n662:         # Pattern 3: \"token limit exceeded\"\n663:         if \"token limit exceeded\" in error_lower:\n664:             return True\n665:         \n666:         # Pattern 4: \"batch size too small\"\n667:         if \"batch size too small\" in error_lower:\n668:             return True\n669:         \n670:         return False\n671: \n672: \n673: \n\n(End of file - total 673 lines)\n</content>","timestamp":1771937351302,"originalBytes":31038}