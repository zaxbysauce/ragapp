{"id":"S46","summaryText":"[SUMMARY S46] 30.4 KB | code | 738 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\api\\routes\\documents.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Documents API routes for file management and processing.\n3: \nâ†’ Use /swarm retrieve S46 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\api\\routes\\documents.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Documents API routes for file management and processing.\n3: \n4: Provides endpoints for listing documents, uploading files, scanning directories,\n5: and managing document processing status.\n6: \"\"\"\n7: import asyncio\n8: import hashlib\n9: import hmac\n10: import logging\n11: import os\n12: import re\n13: import sqlite3\n14: from pathlib import Path\n15: from typing import Any, Dict, List, Optional\n16: \n17: import aiofiles\n18: from fastapi import APIRouter, Depends, HTTPException, Request, UploadFile, File, Query, Body\n19: from fastapi.exceptions import RequestValidationError\n20: from pydantic import BaseModel, ConfigDict, Field\n21: \n22: from app.config import settings, Settings\n23: from app.services.document_processor import DocumentProcessor, DocumentProcessingError, DuplicateFileError\n24: from app.services.vector_store import VectorStore\n25: from app.services.embeddings import EmbeddingService\n26: from app.services.secret_manager import SecretManager\n27: from app.models.database import SQLiteConnectionPool\n28: from app.api.deps import get_secret_manager, get_background_processor, get_vector_store, get_embedding_service, get_settings, get_db, get_db_pool\n29: from app.security import csrf_protect, require_scope, require_auth\n30: from app.limiter import limiter\n31: from app.services.background_tasks import BackgroundProcessor\n32: \n33: \n34: def secure_filename(filename: str) -> str:\n35:     \"\"\"\n36:     Sanitize a filename to prevent security issues.\n37:     \n38:     - Strips paths using os.path.basename\n39:     - Removes non-ASCII characters\n40:     - Replaces spaces with underscores\n41:     - Allows only alphanumeric, dots, hyphens, and underscores\n42:     \"\"\"\n43:     # Strip paths\n44:     filename = os.path.basename(filename)\n45:     \n46:     # Replace spaces with underscores\n47:     filename = filename.replace(\" \", \"_\")\n48:     \n49:     # Remove non-ASCII characters\n50:     filename = filename.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n51:     \n52:     # Allow only alphanumeric, dots, hyphens, and underscores\n53:     filename = re.sub(r\"[^a-zA-Z0-9._-]\", \"\", filename)\n54:     \n55:     return filename\n56: \n57: \n58: logger = logging.getLogger(__name__)\n59: \n60: \n61: router = APIRouter(prefix=\"/documents\", tags=[\"documents\"])\n62: \n63: \n64: def _sanitize_metadata(metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n65:     if not isinstance(metadata, dict):\n66:         return {}\n67:     sanitized: Dict[str, Any] = {}\n68:     for key, value in metadata.items():\n69:         if not isinstance(key, str):\n70:             continue\n71:         if key.lower() in {\"password\", \"ssn\", \"secret\", \"token\"}:\n72:             continue\n73:         if isinstance(value, str) and len(value) > 256:\n74:             sanitized[key] = value[:256]\n75:         else:\n76:             sanitized[key] = value\n77:     return sanitized\n78: \n79: \n80: def _record_document_action(\n81:     file_id: int,\n82:     action: str,\n83:     status: str,\n84:     user_id: str,\n85:     secret_manager: SecretManager,\n86:     conn: sqlite3.Connection,\n87: ) -> None:\n88:     key, key_version = secret_manager.get_hmac_key()\n89:     message = f\"{file_id}|{action}|{status}|{user_id}\"\n90:     digest = hmac.new(key, message.encode(\"utf-8\"), hashlib.sha256).hexdigest()\n91:     conn.execute(\n92:         \"\"\"\n93:         INSERT INTO document_actions(file_id, action, status, user_id, hmac_sha256)\n94:         VALUES (?, ?, ?, ?, ?)\n95:         \"\"\",\n96:         (file_id, action, status, user_id, digest),\n97:     )\n98: \n99: \n100: @router.post(\"/admin/retry/{file_id}\")\n101: @limiter.limit(settings.admin_rate_limit)\n102: async def retry_document(\n103:     file_id: int,\n104:     request: Request,\n105:     conn: sqlite3.Connection = Depends(get_db),\n106:     auth: dict = Depends(require_scope(\"documents:manage\")),\n107:     csrf_token: str = Depends(csrf_protect),\n108:     secret_manager: SecretManager = Depends(get_secret_manager),\n109:     background_processor: BackgroundProcessor = Depends(get_background_processor),\n110: ) -> dict:\n111:     try:\n112:         cursor = await asyncio.to_thread(conn.execute, \"SELECT file_path FROM files WHERE id = ?\", (file_id,))\n113:         row = await asyncio.to_thread(cursor.fetchone)\n114:         if not row:\n115:             raise HTTPException(status_code=404, detail=\"Document not found\")\n116: \n117:         # Ensure processor is running\n118:         if not background_processor.is_running:\n119:             await background_processor.start()\n120: \n121:         await background_processor.enqueue(row[\"file_path\"])\n122: \n123:         await asyncio.to_thread(\n124:             _record_document_action,\n125:             file_id,\n126:             \"retry\",\n127:             \"scheduled\",\n128:             auth.get(\"user_id\", \"unknown\"),\n129:             secret_manager,\n130:             conn,\n131:         )\n132:         await asyncio.to_thread(conn.commit)\n133:         return {\"file_id\": file_id, \"status\": \"scheduled\"}\n134:     except HTTPException:\n135:         raise\n136:     except Exception as exc:\n137:         logger.exception(\"Error reprocessing document %d\", file_id)\n138:         await asyncio.to_thread(\n139:             _record_document_action,\n140:             file_id,\n141:             \"retry\",\n142:             \"error\",\n143:             auth.get(\"user_id\", \"unknown\"),\n144:             secret_manager,\n145:             conn,\n146:         )\n147:         await asyncio.to_thread(conn.commit)\n148:         raise HTTPException(status_code=500, detail=f\"Retry failed: {exc}\")\n149: \n150: \n151: class DocumentResponse(BaseModel):\n152:     \"\"\"Response model for a document record - frontend compatible.\"\"\"\n153:     id: int\n154:     file_name: str\n155:     filename: str  # Frontend alias\n156:     file_path: str\n157:     status: str\n158:     chunk_count: int\n159:     size: Optional[int] = None  # Frontend expects size\n160:     created_at: Optional[str]\n161:     processed_at: Optional[str]\n162:     metadata: Optional[dict] = None  # Frontend expects metadata\n163: \n164:     model_config = ConfigDict(from_attributes=True)\n165: \n166: \n167: class DocumentListResponse(BaseModel):\n168:     \"\"\"Response model for listing documents - frontend compatible with total.\"\"\"\n169:     documents: List[DocumentResponse]\n170:     total: int\n171: \n172: \n173: class DocumentStatsResponse(BaseModel):\n174:     \"\"\"Response model for document statistics - frontend compatible.\"\"\"\n175:     total_documents: int  # Frontend expects this field\n176:     total_chunks: int\n177:     total_size_bytes: int = 0  # Frontend expects this field\n178:     documents_by_status: dict = Field(default_factory=dict)  # Frontend expects this field\n179:     total_files: int = 0  # Backward compatibility alias\n180:     status: str = \"success\"\n181: \n182: \n183: class UploadResponse(BaseModel):\n184:     \"\"\"Response model for file upload - frontend compatible.\"\"\"\n185:     file_id: int\n186:     file_name: str\n187:     id: int  # Frontend alias for file_id\n188:     filename: str  # Frontend alias for file_name\n189:     status: str\n190:     message: str\n191: \n192: \n193: class ScanResponse(BaseModel):\n194:     \"\"\"Response model for directory scan - frontend compatible.\"\"\"\n195:     files_enqueued: int\n196:     status: str\n197:     message: str\n198:     added: int  # Frontend alias for files_enqueued\n199:     scanned: int  # Frontend expects this field (total files scanned)\n200:     errors: List[str] = Field(default_factory=list)  # Frontend expects this field\n201: \n202: \n203: class DeleteResponse(BaseModel):\n204:     \"\"\"Response model for document deletion.\"\"\"\n205:     file_id: int\n206:     status: str\n207:     message: str\n208: \n209: \n210: class BatchDeleteResponse(BaseModel):\n211:     \"\"\"Response model for batch document deletion.\"\"\"\n212:     deleted_count: int\n213:     failed_ids: List[str]\n214: \n215: \n216: class DeleteAllVaultResponse(BaseModel):\n217:     \"\"\"Response model for deleting all documents in a vault.\"\"\"\n218:     deleted_count: int\n219:     vault_id: int\n220: \n221: \n222: def _row_to_document_response(row: sqlite3.Row) -> DocumentResponse:\n223:     \"\"\"Convert a database row to a DocumentResponse.\"\"\"\n224:     file_name = row[\"file_name\"]\n225:     chunk_count = row[\"chunk_count\"] or 0\n226:     status = row[\"status\"]\n227:     return DocumentResponse(\n228:         id=row[\"id\"],\n229:         file_name=file_name,\n230:         filename=file_name,  # Frontend alias\n231:         file_path=row[\"file_path\"],\n232:         status=status,\n233:         chunk_count=chunk_count,\n234:         size=row[\"file_size\"] if \"file_size\" in row.keys() and row[\"file_size\"] is not None else None,\n235:         created_at=row[\"created_at\"],\n236:         processed_at=row[\"processed_at\"],\n237:         metadata={\n238:             \"status\": status,\n239:             \"chunk_count\": chunk_count,\n240:             \"chunks\": chunk_count,  # Backward compatibility\n241:         },\n242:     )\n243: \n244: \n245: @router.get(\"\", response_model=DocumentListResponse)\n246: @router.get(\"/\", response_model=DocumentListResponse)\n247: async def list_documents(\n248:     vault_id: Optional[int] = Query(None, description=\"Filter by vault ID\"),\n249:     conn: sqlite3.Connection = Depends(get_db),\n250: ):\n251:     \"\"\"\n252:     List all documents from the files table.\n253: \n254:     Returns a list of all files with their id, file_name, file_path, status,\n255:     chunk_count, created_at, and processed_at fields.\n256:     Optionally filter by vault_id.\n257:     \"\"\"\n258:     if vault_id is not None:\n259:         cursor = await asyncio.to_thread(\n260:             conn.execute,\n261:             \"\"\"\n262:             SELECT id, file_name, file_path, status, chunk_count, file_size, created_at, processed_at\n263:             FROM files\n264:             WHERE vault_id = ?\n265:             ORDER BY created_at DESC\n266:             \"\"\",\n267:             (vault_id,),\n268:         )\n269:     else:\n270:         cursor = await asyncio.to_thread(\n271:             conn.execute,\n272:             \"\"\"\n273:             SELECT id, file_name, file_path, status, chunk_count, file_size, created_at, processed_at\n274:             FROM files\n275:             ORDER BY created_at DESC\n276:             \"\"\"\n277:         )\n278:     rows = await asyncio.to_thread(cursor.fetchall)\n279:     \n280:     documents = [_row_to_document_response(row) for row in rows]\n281:     \n282:     return DocumentListResponse(documents=documents, total=len(documents))\n283: \n284: \n285: @router.get(\"/stats\", response_model=DocumentStatsResponse)\n286: async def get_document_stats(\n287:     vault_id: Optional[int] = Query(None, description=\"Filter by vault ID\"),\n288:     conn: sqlite3.Connection = Depends(get_db),\n289: ):\n290:     \"\"\"\n291:     Get counts of files and chunks.\n292: \n293:     Returns total number of files in the database, total chunks,\n294:     total size in bytes, and documents grouped by status.\n295:     Optionally filter by vault_id.\n296:     \"\"\"\n297:     # Get total files count\n298:     if vault_id is not None:\n299:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COUNT(*) as total_files FROM files WHERE vault_id = ?\", (vault_id,))\n300:     else:\n301:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COUNT(*) as total_files FROM files\")\n302:     row = await asyncio.to_thread(cursor.fetchone)\n303:     total_files = row[\"total_files\"]\n304:     \n305:     # Get total chunks count\n306:     if vault_id is not None:\n307:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(chunk_count), 0) as total_chunks FROM files WHERE vault_id = ?\", (vault_id,))\n308:     else:\n309:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(chunk_count), 0) as total_chunks FROM files\")\n310:     row = await asyncio.to_thread(cursor.fetchone)\n311:     total_chunks = row[\"total_chunks\"]\n312:     \n313:     # Get total size (sum of file_size if column exists, otherwise 0)\n314:     try:\n315:         if vault_id is not None:\n316:             cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(file_size), 0) as total_size FROM files WHERE vault_id = ?\", (vault_id,))\n317:         else:\n318:             cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(file_size), 0) as total_size FROM files\")\n319:         row = await asyncio.to_thread(cursor.fetchone)\n320:         total_size_bytes = row[\"total_size\"] or 0\n321:     except sqlite3.OperationalError:\n322:         total_size_bytes = 0\n323:     \n324:     # Get documents grouped by status\n325:     if vault_id is not None:\n326:         cursor = await asyncio.to_thread(conn.execute, \"SELECT status, COUNT(*) as count FROM files WHERE vault_id = ? GROUP BY status\", (vault_id,))\n327:     else:\n328:         cursor = await asyncio.to_thread(conn.execute, \"SELECT status, COUNT(*) as count FROM files GROUP BY status\")\n329:     rows = await asyncio.to_thread(cursor.fetchall)\n330:     documents_by_status = {row[\"status\"]: row[\"count\"] for row in rows}\n331:     \n332:     return DocumentStatsResponse(\n333:         total_documents=total_files,  # Frontend field\n334:         total_chunks=total_chunks,\n335:         total_size_bytes=total_size_bytes,\n336:         documents_by_status=documents_by_status,\n337:         total_files=total_files,  # Backward compatibility\n338:     )\n339: \n340: \n341: @router.post(\"\", response_model=UploadResponse)\n342: @router.post(\"/\", response_model=UploadResponse)\n343: async def upload_document_root(\n344:     request: Request,\n345:     file: Optional[UploadFile] = None,\n346:     vault_id: int = Query(1, description=\"Target vault ID\"),\n347:     settings_dep: Settings = Depends(get_settings),\n348:     vector_store: VectorStore = Depends(get_vector_store),\n349:     embedding_service: EmbeddingService = Depends(get_embedding_service),\n350:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n351:     auth: dict = Depends(require_auth),\n352: ):\n353:     \"\"\"\n354:     Upload endpoint at root /documents for frontend compatibility.\n355:     Delegates to the main upload handler.\n356:     \"\"\"\n357:     return await _do_upload(request, file, settings_dep, vector_store, embedding_service, db_pool, vault_id)\n358: \n359: \n360: @router.post(\"/upload\", response_model=UploadResponse)\n361: async def upload_document(\n362:     request: Request,\n363:     file: Optional[UploadFile] = None,\n364:     vault_id: int = Query(1, description=\"Target vault ID\"),\n365:     settings_dep: Settings = Depends(get_settings),\n366:     vector_store: VectorStore = Depends(get_vector_store),\n367:     embedding_service: EmbeddingService = Depends(get_embedding_service),\n368:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n369:     auth: dict = Depends(require_auth),\n370: ):\n371:     \"\"\"\n372:     Upload a file and process it with strict security controls.\n373: \n374:     Validates filename, extension, and file size before saving.\n375:     Saves the uploaded file to settings.uploads_dir using aiofiles,\n376:     then processes it via DocumentProcessor.process_file in asyncio.to_thread.\n377:     \"\"\"\n378:     return await _do_upload(request, file, settings_dep, vector_store, embedding_service, db_pool, vault_id)\n379: \n380: \n381: async def _do_upload(\n382:     request: Request,\n383:     file: Optional[UploadFile],\n384:     settings_dep: Settings,\n385:     vector_store: VectorStore,\n386:     embedding_service: EmbeddingService,\n387:     db_pool: SQLiteConnectionPool,\n388:     vault_id: int,\n389: ) -> UploadResponse:\n390:     # Validate file is provided\n391:     if file is None:\n392:         raise HTTPException(status_code=400, detail=\"No file provided\")\n393: \n394:     # Validate filename is not empty\n395:     if not file.filename:\n396:         raise HTTPException(status_code=400, detail=\"Filename cannot be empty\")\n397: \n398:     # Ensure uploads directory exists\n399:     uploads_dir = settings_dep.uploads_dir\n400:     uploads_dir.mkdir(parents=True, exist_ok=True)\n401:     \n402:     # Sanitize filename\n403:     file_name = secure_filename(file.filename or \"unnamed_file\")\n404:     if not file_name:\n405:         file_name = \"unnamed_file.txt\"\n406: \n407:     # Ensure file has an extension for validation\n408:     if not Path(file_name).suffix:\n409:         file_name = f\"{file_name}.txt\"\n410:     \n411:     # Validate file extension\n412:     file_suffix = Path(file_name).suffix.lower()\n413:     if file_suffix not in settings_dep.allowed_extensions:\n414:         raise HTTPException(\n415:             status_code=400,\n416:             detail=f\"File extension '{file_suffix}' not allowed. Allowed: {settings_dep.allowed_extensions}\"\n417:         )\n418: \n419:     # Validate file size from content-length header\n420:     max_size_bytes = settings_dep.max_file_size_mb * 1024 * 1024\n421:     content_length = request.headers.get(\"content-length\")\n422:     if content_length:\n423:         try:\n424:             if int(content_length) > max_size_bytes:\n425:                 raise HTTPException(status_code=413, detail=f\"File too large. Max size: {settings.max_file_size_mb}MB\")\n426:         except ValueError:\n427:             pass  # Invalid content-length header, will check during streaming\n428:     \n429:     # Generate safe file path\n430:     file_path = uploads_dir / file_name\n431:     \n432:     # Handle duplicate file names\n433:     counter = 1\n434:     original_path = file_path\n435:     while file_path.exists():\n436:         stem = original_path.stem\n437:         suffix = original_path.suffix\n438:         file_path = uploads_dir / f\"{stem}_{counter}{suffix}\"\n439:         counter += 1\n440:     \n441:     # Path safety: ensure file_path is within uploads_dir\n442:     try:\n443:         resolved_path = file_path.resolve()\n444:         resolved_uploads_dir = uploads_dir.resolve()\n445:         if not str(resolved_path).startswith(str(resolved_uploads_dir)):\n446:             raise HTTPException(status_code=400, detail=\"Invalid file path\")\n447:     except (OSError, ValueError):\n448:         raise HTTPException(status_code=400, detail=\"Invalid file path\")\n449:     \n450:     temp_file_path = None\n451:     try:\n452:         # Save file using aiofiles with chunked reading and size validation\n453:         total_bytes = 0\n454:         temp_file_path = file_path\n455:         async with aiofiles.open(temp_file_path, \"wb\") as f:\n456:             while chunk := await file.read(1024 * 1024):  # Read 1MB chunks\n457:                 total_bytes += len(chunk)\n458:                 if total_bytes > max_size_bytes:\n459:                     # Close and delete partial file\n460:                     await f.close()\n461:                     if temp_file_path.exists():\n462:                         temp_file_path.unlink(missing_ok=True)\n463:                     raise HTTPException(status_code=413, detail=f\"File too large. Max size: {settings.max_file_size_mb}MB\")\n464:                 await f.write(chunk)\n465:         \n466:         # Process file with injected dependencies\n467:         processor = DocumentProcessor(\n468:             chunk_size_chars=settings_dep.chunk_size_chars,\n469:             chunk_overlap_chars=settings_dep.chunk_overlap_chars,\n470:             vector_store=vector_store,\n471:             embedding_service=embedding_service,\n472:             pool=db_pool,\n473:         )\n474: \n475:         try:\n476:             result = await processor.process_file(str(file_path), vault_id=vault_id)\n477:             \n478:             return UploadResponse(\n479:                 file_id=result.file_id,\n480:                 file_name=file_name,\n481:                 id=result.file_id,  # Frontend alias\n482:                 filename=file_name,  # Frontend alias\n483:                 status=\"indexed\",\n484:                 message=f\"File '{file_name}' uploaded and processed successfully with {len(result.chunks)} chunks\",\n485:             )\n486:         except DuplicateFileError as e:\n487:             # File is a duplicate, remove the uploaded file\n488:             file_path.unlink(missing_ok=True)\n489:             raise HTTPException(status_code=409, detail=f\"{e} (uploaded file was cleaned up)\")\n490:         except HTTPException:\n491:             # Clean up partial file if it exists\n492:             if temp_file_path and temp_file_path.exists():\n493:                 temp_file_path.unlink(missing_ok=True)\n494:             raise\n495:         except DocumentProcessingError as e:\n496:             logger.exception(\"Document processing error for file: %s\", file_name)\n497:             raise HTTPException(status_code=500, detail=f\"Processing error: {e}\")\n498:         except Exception as e:\n499:             logger.exception(\"Unexpected error processing file: %s\", file_name)\n500:             raise HTTPException(status_code=500, detail=f\"Server error: {e}\")\n501:     except Exception as e:\n502:         logger.exception(\"Error uploading file: %s\", file_name)\n503:         # Clean up file if it was created\n504:         if temp_file_path and temp_file_path.exists():\n505:             temp_file_path.unlink(missing_ok=True)\n506:         raise HTTPException(status_code=500, detail=f\"Upload failed: {e}\")\n507: \n508: \n509: @router.post(\"/scan\", response_model=ScanResponse)\n510: async def scan_directories(\n511:     request: Request,\n512:     background_processor: BackgroundProcessor = Depends(get_background_processor),\n513:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n514:     auth: dict = Depends(require_auth),\n515: ):\n516:     \"\"\"\n517:     Trigger a scan of configured directories for new files.\n518: \n519:     Calls FileWatcher.scan_once() to find and enqueue new files\n520:     from uploads_dir and library_dir that are not in the database.\n521: \n522:     Uses the singleton BackgroundProcessor that runs continuously in the background.\n523:     \"\"\"\n524:     from app.services.file_watcher import FileWatcher\n525: \n526:     # Ensure processor is running (it should be from lifespan, but double-check)\n527:     if not background_processor.is_running:\n528:         await background_processor.start()\n529: \n530:     try:\n531:         watcher = FileWatcher(background_processor, pool=db_pool)\n532: \n533:         # Perform scan\n534:         files_enqueued = await watcher.scan_once()\n535: \n536:         if files_enqueued > 0:\n537:             message = f\"Scan complete: {files_enqueued} new files enqueued for processing\"\n538:         else:\n539:             message = \"Scan complete: no new files found\"\n540: \n541:         return ScanResponse(\n542:             files_enqueued=files_enqueued,\n543:             status=\"success\",\n544:             message=message,\n545:             added=files_enqueued,  # Frontend alias\n546:             scanned=files_enqueued,  # Frontend expects this (at least files_enqueued)\n547:             errors=[],  # Frontend expects this field\n548:         )\n549:     except Exception as e:\n550:         logger.exception(\"Error during directory scan\")\n551:         raise HTTPException(status_code=500, detail=f\"Scan failed: {e}\")\n552:     # Note: No finally block to stop processor - it runs continuously\n553: \n554: \n555: @router.delete(\"/{file_id}\", response_model=DeleteResponse)\n556: async def delete_document(\n557:     file_id: int,\n558:     request: Request,\n559:     conn: sqlite3.Connection = Depends(get_db),\n560:     auth: dict = Depends(require_auth),\n561:     vector_store: VectorStore = Depends(get_vector_store),\n562: ):\n563:     \"\"\"\n564:     Delete a document by ID.\n565: \n566:     Deletes the file record from the database and removes all associated\n567:     chunks from the vector store. Returns 404 if the file is not found.\n568:     \"\"\"\n569:     # Check if file exists\n570:     cursor = await asyncio.to_thread(conn.execute, \"SELECT id, file_name FROM files WHERE id = ?\", (file_id,))\n571:     row = await asyncio.to_thread(cursor.fetchone)\n572:     \n573:     if row is None:\n574:         raise HTTPException(status_code=404, detail=f\"Document with id {file_id} not found\")\n575:     \n576:     file_name = row[\"file_name\"]\n577: \n578:     try:\n579:         # Delete from vector store first (wrapped in to_thread to avoid blocking)\n580:         try:\n581:             db = vector_store.db\n582:             if db is not None and \"chunks\" in db.table_names():\n583:                 vector_store.table = db.open_table(\"chunks\")\n584:                 deleted_chunks = await asyncio.to_thread(vector_store.delete_by_file, str(file_id))\n585:                 logger.info(\"Deleted %d chunks from vector store for file_id %s\", deleted_chunks, file_id)\n586:             else:\n587:                 logger.debug(\"Chunks table not found, skipping vector store deletion for file_id %s\", file_id)\n588:         except Exception as e:\n589:             logger.warning(\"Error deleting chunks from vector store: %s\", e)\n590:             # Continue with database deletion even if vector store fails\n591: \n592:         # Delete from database\n593:         await asyncio.to_thread(conn.execute, \"DELETE FROM files WHERE id = ?\", (file_id,))\n594:         await asyncio.to_thread(conn.commit)\n595:         \n596:         return DeleteResponse(\n597:             file_id=file_id,\n598:             status=\"success\",\n599:             message=f\"Document '{file_name}' (id: {file_id}) deleted successfully\",\n600:         )\n601:     except HTTPException:\n602:         raise\n603:     except Exception as e:\n604:         await asyncio.to_thread(conn.rollback)\n605:         logger.exception(\"Error deleting document %d\", file_id)\n606:         raise HTTPException(status_code=500, detail=f\"Delete failed: {e}\")\n607: \n608: \n609: @router.post(\"/batch\", response_model=BatchDeleteResponse)\n610: async def batch_delete_documents(\n611:     request: Request,\n612:     file_ids: List[str] = Body(..., embed=True, description=\"List of file IDs to delete\"),\n613:     conn: sqlite3.Connection = Depends(get_db),\n614:     auth: dict = Depends(require_auth),\n615:     vector_store: VectorStore = Depends(get_vector_store),\n616: ):\n617:     \"\"\"\n618:     Batch delete documents by IDs.\n619: \n620:     Deletes multiple documents from the database and removes all associated\n621:     chunks from the vector store. Returns count of successfully deleted\n622:     documents and any failed IDs.\n623:     \"\"\"\n624:     deleted_count = 0\n625:     failed_ids: List[str] = []\n626: \n627:     for file_id in file_ids:\n628:         try:\n629:             # Check if file exists\n630:             cursor = await asyncio.to_thread(conn.execute, \"SELECT id, file_name FROM files WHERE id = ?\", (file_id,))\n631:             row = await asyncio.to_thread(cursor.fetchone)\n632: \n633:             if row is None:\n634:                 failed_ids.append(file_id)\n635:                 continue\n636: \n637:             file_name = row[\"file_name\"]\n638: \n639:             # Delete from vector store first\n640:             try:\n641:                 db = vector_store.db\n642:                 if db is not None and \"chunks\" in db.table_names():\n643:                     vector_store.table = db.open_table(\"chunks\")\n644:                     await asyncio.to_thread(vector_store.delete_by_file, str(file_id))\n645:             except Exception as e:\n646:                 logger.warning(\"Error deleting chunks from vector store for file_id %d: %s\", file_id, e)\n647: \n648:             # Delete from database\n649:             await asyncio.to_thread(conn.execute, \"DELETE FROM files WHERE id = ?\", (file_id,))\n650:             await asyncio.to_thread(conn.commit)\n651:             deleted_count += 1\n652:             logger.info(\"Deleted document '%s' (id: %d)\", file_name, file_id)\n653: \n654:         except Exception as e:\n655:             logger.exception(\"Error deleting document %d\", file_id)\n656:             failed_ids.append(file_id)\n657: \n658:     return BatchDeleteResponse(\n659:         deleted_count=deleted_count,\n660:         failed_ids=failed_ids,\n661:     )\n662: \n663: \n664: @router.delete(\"/vault/{vault_id}/all\", response_model=DeleteAllVaultResponse)\n665: async def delete_all_vault_documents(\n666:     vault_id: int,\n667:     request: Request,\n668:     conn: sqlite3.Connection = Depends(get_db),\n669:     auth: dict = Depends(require_auth),\n670:     vector_store: VectorStore = Depends(get_vector_store),\n671: ):\n672:     \"\"\"\n673:     Delete all documents in a vault.\n674: \n675:     Deletes all file records from the database and removes all associated\n676:     chunks from the vector store for the specified vault.\n677:     \"\"\"\n678:     # Get all file IDs in the vault\n679:     cursor = await asyncio.to_thread(conn.execute, \"SELECT id FROM files WHERE vault_id = ?\", (vault_id,))\n680:     rows = await asyncio.to_thread(cursor.fetchall)\n681: \n682:     file_ids = [row[\"id\"] for row in rows]\n683:     deleted_count = 0\n684: \n685:     for file_id in file_ids:\n686:         try:\n687:             # Delete from vector store first\n688:             try:\n689:                 db = vector_store.db\n690:                 if db is not None and \"chunks\" in db.table_names():\n691:                     vector_store.table = db.open_table(\"chunks\")\n692:                     await asyncio.to_thread(vector_store.delete_by_file, str(file_id))\n693:             except Exception as e:\n694:                 logger.warning(\"Error deleting chunks from vector store for file_id %d: %s\", file_id, e)\n695: \n696:             # Delete from database\n697:             await asyncio.to_thread(conn.execute, \"DELETE FROM files WHERE id = ?\", (file_id,))\n698:             await asyncio.to_thread(conn.commit)\n699:             deleted_count += 1\n700: \n701:         except Exception as e:\n702:             logger.exception(\"Error deleting document %d from vault %d\", file_id, vault_id)\n703: \n704:     logger.info(\"Deleted %d documents from vault %d\", deleted_count, vault_id)\n705: \n706:     return DeleteAllVaultResponse(\n707:         deleted_count=deleted_count,\n708:         vault_id=vault_id,\n709:     )\n710: \n711: \n712: # Exception handler for validation errors (e.g., empty filename)\n713: # This is registered at the app level in main.py\n714: async def validation_exception_handler(request: Request, exc: RequestValidationError):\n715:     \"\"\"Convert validation errors to 400 for empty filename cases only.\"\"\"\n716:     errors = exc.errors()\n717:     for error in errors:\n718:         if error.get(\"loc\") == (\"body\", \"file\") and \"filename\" in str(error.get(\"input\", \"\")).lower():\n719:             raise HTTPException(status_code=400, detail=\"Filename cannot be empty\")\n720:     # For all other validation errors, return standard 422\n721:     # Convert errors to dict format for JSON serialization\n722:     from fastapi.responses import JSONResponse\n723:     error_dicts = [\n724:         {\n725:             \"loc\": error.get(\"loc\"),\n726:             \"msg\": error.get(\"msg\"),\n727:             \"type\": error.get(\"type\"),\n728:             \"input\": error.get(\"input\")\n729:         }\n730:         for error in errors\n731:     ]\n732:     return JSONResponse(status_code=422, content={\"detail\": error_dicts})\n733: \n\n(End of file - total 733 lines)\n</content>","timestamp":1771960717902,"originalBytes":31109}