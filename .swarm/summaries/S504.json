{"id":"S504","summaryText":"[SUMMARY S504] 28.6 KB | code | 646 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\embeddings.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Dual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n3: \"\"\"\nâ†’ Use /swarm retrieve S504 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\embeddings.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Dual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n3: \"\"\"\n4: import asyncio\n5: import httpx\n6: import logging\n7: from typing import List\n8: from urllib.parse import urlparse\n9: from app.config import settings\n10: \n11: logger = logging.getLogger(__name__)\n12: \n13: \n14: class EmbeddingError(Exception):\n15:     \"\"\"Exception raised for embedding service errors.\"\"\"\n16:     pass\n17: \n18: \n19: class EmbeddingService:\n20:     \"\"\"Service for generating text embeddings via Ollama or OpenAI-compatible APIs.\"\"\"\n21:     \n22:     # Hard caps for input validation\n23:     MAX_BATCH_SIZE = 512  # Maximum number of texts per batch call\n24:     MAX_TEXT_LENGTH = 8192  # Maximum characters per text (derived from chunk_size_chars=8192)\n25:     MIN_SPLIT_CHARS = 100  # Minimum text length to attempt single-text splitting\n26: \n27:     def __init__(self):\n28:         \"\"\"Initialize the embedding service with HTTP client and provider detection.\"\"\"\n29:         base_url = settings.ollama_embedding_url\n30: \n31:         # Validate base_url\n32:         if not base_url:\n33:             raise EmbeddingError(\"Embedding service is not configured\")\n34:         if not base_url.startswith(('http://', 'https://')):\n35:             raise EmbeddingError(\"Invalid embedding URL configuration\")\n36: \n37:         # Detect provider mode based on URL path\n38:         self.provider_mode, self.embeddings_url = self._detect_provider_mode(base_url)\n39:         self.timeout = 60.0\n40:         \n41:         # Read embedding prefixes from settings\n42:         self.embedding_doc_prefix = settings.embedding_doc_prefix\n43:         self.embedding_query_prefix = settings.embedding_query_prefix\n44:         \n45:         # Auto-apply Qwen3 instruction prefixes for better retrieval quality\n46:         # With llama.cpp -ub 8192, we have plenty of headroom for these prefixes\n47:         if settings.embedding_model.lower().find(\"qwen\") >= 0:\n48:             if not self.embedding_doc_prefix:\n49:                 self.embedding_doc_prefix = \"Instruct: Represent this technical documentation passage for retrieval.\\nDocument: \"\n50:             if not self.embedding_query_prefix:\n51:                 self.embedding_query_prefix = \"Instruct: Retrieve relevant technical documentation passages.\\nQuery: \"\n52:     \n53:     def _detect_provider_mode(self, base_url: str) -> tuple:\n54:         \"\"\"\n55:         Detect which embedding provider mode to use based on URL path.\n56:         \n57:         Detection strategy:\n58:         - If URL path includes '/api/embeddings' -> Ollama mode\n59:         - If URL path includes '/v1/embeddings' -> OpenAI mode\n60:         - If no explicit embeddings path:\n61:           - Port 1234 -> OpenAI mode (LM Studio default)\n62:           - Otherwise -> Ollama mode\n63:         \n64:         Args:\n65:             base_url: The configured embedding URL\n66:             \n67:         Returns:\n68:             Tuple of (provider_mode, embeddings_url)\n69:         \"\"\"\n70:         parsed = urlparse(base_url)\n71:         path = parsed.path\n72:         \n73:         # Check for explicit paths\n74:         if '/api/embeddings' in path:\n75:             # Already has Ollama path, use as-is\n76:             return ('ollama', base_url)\n77:         elif '/v1/embeddings' in path:\n78:             # Already has OpenAI path, use as-is\n79:             return ('openai', base_url)\n80:         \n81:         # No explicit path - determine by port\n82:         port = parsed.port\n83:         if port == 1234:\n84:             # LM Studio default port - use OpenAI mode\n85:             base_url = base_url.rstrip('/') + '/v1/embeddings'\n86:             return ('openai', base_url)\n87:         else:\n88:             # Default to Ollama mode\n89:             base_url = base_url.rstrip('/') + '/api/embeddings'\n90:             return ('ollama', base_url)\n91:     \n92:     def _build_payload(self, text: str) -> dict:\n93:         \"\"\"\n94:         Build the API request payload based on provider mode.\n95:         \n96:         Args:\n97:             text: The text to embed\n98:             \n99:         Returns:\n100:             Dictionary payload for the API request\n101:         \"\"\"\n102:         if self.provider_mode == 'openai':\n103:             return {\n104:                 \"model\": settings.embedding_model,\n105:                 \"input\": text\n106:             }\n107:         else:  # ollama mode\n108:             return {\n109:                 \"model\": settings.embedding_model,\n110:                 \"prompt\": text\n111:             }\n112:     \n113:     def _extract_embedding(self, data: dict) -> List[float]:\n114:         \"\"\"\n115:         Extract embedding vector from API response based on provider mode.\n116:         \n117:         Args:\n118:             data: Parsed JSON response from the API\n119:             \n120:         Returns:\n121:             List of float values representing the embedding vector\n122:             \n123:         Raises:\n124:             EmbeddingError: If embedding cannot be extracted\n125:         \"\"\"\n126:         if self.provider_mode == 'openai':\n127:             # OpenAI format: data[0].embedding\n128:             if \"data\" not in data:\n129:                 logger.error(\"Embedding API response missing 'data' field in OpenAI mode\")\n130:                 raise EmbeddingError(\"Embedding API response is invalid\")\n131:             if not isinstance(data[\"data\"], list) or len(data[\"data\"]) == 0:\n132:                 logger.error(\"Embedding API response 'data' field is empty or invalid in OpenAI mode\")\n133:                 raise EmbeddingError(\"Embedding API response is invalid\")\n134:             embedding = data[\"data\"][0].get(\"embedding\")\n135:             if embedding is None:\n136:                 logger.error(\"Embedding API response missing 'data[0].embedding' field in OpenAI mode\")\n137:                 raise EmbeddingError(\"Embedding API response is invalid\")\n138:         else:  # ollama mode\n139:             # Ollama format: embedding\n140:             embedding = data.get(\"embedding\")\n141:             if embedding is None:\n142:                 logger.error(\"Embedding API response missing 'embedding' field in Ollama mode\")\n143:                 raise EmbeddingError(\"Embedding API response is invalid\")\n144:         \n145:         return embedding\n146:     \n147:     async def embed_single(self, text: str) -> List[float]:\n148:         \"\"\"\n149:         Generate embedding for a single text.\n150: \n151:         Applies the query prefix (if configured) to the input text before embedding.\n152:         The query prefix is used for retrieval queries and must remain constant for\n153:         a given index to ensure consistent embedding space.\n154: \n155:         Args:\n156:             text: The text to embed.\n157: \n158:         Returns:\n159:             List of float values representing the embedding vector.\n160: \n161:         Raises:\n162:             EmbeddingError: If the API request fails or returns non-200 status.\n163:         \"\"\"\n164:         # Validate text input\n165:         if text is None:\n166:             raise EmbeddingError(\"Text cannot be None\")\n167:         if not text.strip():\n168:             raise EmbeddingError(\"Text cannot be empty or whitespace only\")\n169: \n170:         # Apply query prefix for retrieval queries\n171:         text_to_embed = self.embedding_query_prefix + text if self.embedding_query_prefix else text\n172: \n173:         async with httpx.AsyncClient(timeout=self.timeout) as client:\n174:             try:\n175:                 response = await client.post(\n176:                     self.embeddings_url,\n177:                     json=self._build_payload(text_to_embed)\n178:                 )\n179:                 \n180:                 if response.status_code != 200:\n181:                     logger.warning(\n182:                         f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n183:                     )\n184:                     raise EmbeddingError(\n185:                         f\"Embedding API returned status {response.status_code}\"\n186:                     )\n187: \n188:                 try:\n189:                     data = response.json()\n190:                 except ValueError as e:\n191:                     logger.warning(\n192:                         f\"Invalid JSON response from embedding API for {self.provider_mode} mode: {e}, response: {response.text}\"\n193:                     )\n194:                     raise EmbeddingError(\"Invalid response from embedding service\")\n195: \n196:                 return self._extract_embedding(data)\n197:                 \n198:             except httpx.TimeoutException as e:\n199:                 raise EmbeddingError(\"Embedding request timed out\")\n200:             except httpx.HTTPError as e:\n201:                 raise EmbeddingError(\"Embedding HTTP error occurred\")\n202:     \n203:     async def validate_embedding_dimension(self, expected_dim: int) -> bool:\n204:         \"\"\"\n205:         Validate that the embedding dimension matches the expected value.\n206: \n207:         Args:\n208:             expected_dim: The expected dimension of the embedding vector.\n209:                 Must be a positive integer.\n210: \n211:         Returns:\n212:             True if the dimension matches.\n213: \n214:         Raises:\n215:             EmbeddingError: If expected_dim is invalid or if the dimension\n216:                 does not match the expected value.\n217:         \"\"\"\n218:         # Validate expected_dim input\n219:         if expected_dim is None:\n220:             raise EmbeddingError(\"expected_dim cannot be None\")\n221:         if not isinstance(expected_dim, int) or expected_dim <= 0:\n222:             raise EmbeddingError(f\"expected_dim must be a positive integer, got {expected_dim}\")\n223: \n224:         embedding = await self.embed_single('dimension_check')\n225:         actual_dim = len(embedding)\n226:         if actual_dim != expected_dim:\n227:             raise EmbeddingError(\n228:                 f\"Embedding dimension mismatch: expected {expected_dim}, got {actual_dim}\"\n229:             )\n230:         return True\n231: \n232:     async def embed_batch(self, texts: List[str], batch_size: int | None = None) -> List[List[float]]:\n233:         \"\"\"\n234:         Generate embeddings for a batch of texts using true API batching.\n235: \n236:         Sends multiple texts per API request for efficient GPU utilization.\n237:         Processes in batches of up to 512 (configurable) with up to 4\n238:         concurrent batch requests.\n239: \n240:         Applies the document prefix (if configured) to each input text before embedding.\n241:         The document prefix is used for document embeddings and must remain constant for\n242:         a given index to ensure consistent embedding space.\n243: \n244:         Args:\n245:             texts: List of texts to embed.\n246:             batch_size: Number of texts per API request (default: 512).\n247: \n248:         Returns:\n249:             List of embedding vectors, one for each input text, in order.\n250: \n251:         Raises:\n252:             EmbeddingError: If any API request fails.\n253:         \"\"\"\n254:         if not texts:\n255:             return []\n256:         \n257:         # Input validation guards\n258:         for idx, text in enumerate(texts):\n259:             if text is None:\n260:                 raise EmbeddingError(f\"Text at index {idx} is None\")\n261:             if not text.strip():\n262:                 raise EmbeddingError(f\"Text at index {idx} is empty or whitespace only\")\n263:             if len(text) > self.MAX_TEXT_LENGTH:\n264:                 raise EmbeddingError(\n265:                     f\"Text at index {idx} exceeds maximum length ({self.MAX_TEXT_LENGTH} characters)\"\n266:                 )\n267:         \n268:         # Use configured batch size if not specified\n269:         if batch_size is None:\n270:             batch_size = settings.embedding_batch_size\n271:         \n272:         # Clamp batch_size to valid range\n273:         batch_size = max(1, min(batch_size, self.MAX_BATCH_SIZE))\n274:         \n275:         # Apply document prefix to all texts\n276:         texts_to_embed = []\n277:         for text in texts:\n278:             if self.embedding_doc_prefix:\n279:                 texts_to_embed.append(self.embedding_doc_prefix + text)\n280:             else:\n281:                 texts_to_embed.append(text)\n282:         \n283:         # Process in batches using true API batching\n284:         all_embeddings: List[List[float]] = []\n285:         for i in range(0, len(texts_to_embed), batch_size):\n286:             batch = texts_to_embed[i:i + batch_size]\n287:             embeddings = await self._embed_batch_api(batch)\n288:             all_embeddings.extend(embeddings)\n289:         \n290:         return all_embeddings\n291: \n292:     async def _embed_batch_api(self, texts: List[str]) -> List[List[float]]:\n293:         \"\"\"\n294:         Send a batch of texts to the embedding API in a single request.\n295:         \n296:         Implements adaptive batching: automatically retries with smaller sub-batches\n297:         when llama.cpp token overflow errors occur.\n298: \n299:         Args:\n300:             texts: List of texts to embed (already prefixed).\n301: \n302:         Returns:\n303:             List of embedding vectors in the same order as input texts.\n304: \n305:         Raises:\n306:             EmbeddingError: If the API request fails after all retries.\n307:         \"\"\"\n308:         max_retries = settings.embedding_batch_max_retries\n309:         min_sub_size = settings.embedding_batch_min_sub_size\n310:         \n311:         async with httpx.AsyncClient(timeout=self.timeout) as client:\n312:             return await self._embed_batch_with_retry(client, texts, max_retries, min_sub_size)\n313:     \n314:     async def _embed_batch_with_retry(self, client: httpx.AsyncClient, texts: List[str], \n315:                                       max_retries: int, min_sub_size: int, retry_count: int = 0) -> List[List[float]]:\n316:         \"\"\"\n317:         Internal method that handles the retry logic for adaptive batching.\n318:         \n319:         Args:\n320:             client: HTTP client for making requests\n321:             texts: List of texts to embed\n322:             max_retries: Maximum number of retry attempts\n323:             min_sub_size: Minimum sub-batch size before giving up\n324:             \n325:         Returns:\n326:             List of embedding vectors\n327:             \n328:         Raises:\n329:             EmbeddingError: If all retries fail\n330:         \"\"\"\n331:         # Empty-input guard\n332:         if not texts:\n333:             return []\n334:         \n335:         try:\n336:             # Build payload with array of inputs\n337:             if self.provider_mode == 'openai':\n338:                 payload = {\n339:                     \"model\": settings.embedding_model,\n340:                     \"input\": texts\n341:                 }\n342:             else:  # ollama mode\n343:                 payload = {\n344:                     \"model\": settings.embedding_model,\n345:                     \"input\": texts\n346:                 }\n347:             \n348:             response = await client.post(\n349:                 self.embeddings_url,\n350:                 json=payload\n351:             )\n352:             \n353:             # Check for token overflow error in HTTP 500 responses\n354:             if response.status_code == 500:\n355:                 error_text = response.text.lower()\n356:                 if self._is_token_overflow_error(error_text):\n357:                     logger.warning(\n358:                         f\"Token overflow error for {self.provider_mode} mode: {response.text}\"\n359:                     )\n360:                     # Handle overflow using the shared helper\n361:                     return await self._handle_overflow_retry(\n362:                         client, texts, max_retries, min_sub_size, retry_count\n363:                     )\n364:             \n365:             if response.status_code != 200:\n366:                 logger.warning(\n367:                     f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n368:                 )\n369:                 raise EmbeddingError(\n370:                     f\"Embedding API returned status {response.status_code}\"\n371:                 )\n372: \n373:             data = response.json()\n374:             \n375:             # Extract embeddings from response\n376:             if self.provider_mode == 'openai':\n377:                 # OpenAI format: data[].embedding\n378:                 embeddings = [item['embedding'] for item in data['data']]\n379:             else:\n380:                 # Ollama format may vary - try common formats\n381:                 if 'embeddings' in data:\n382:                     embeddings = data['embeddings']\n383:                 elif 'embedding' in data:\n384:                     # Single embedding returned - shouldn't happen with batch\n385:                     embeddings = [data['embedding']]\n386:                 else:\n387:                     logger.error(\n388:                         f\"Unexpected response format for {self.provider_mode} mode: {data.keys()}\"\n389:                     )\n390:                     raise EmbeddingError(\"Unexpected response from embedding service\")\n391:             \n392:             # Validate embedding structure\n393:             if not isinstance(embeddings, list):\n394:                 logger.error(\"Embedding API response 'embeddings' is not a list\")\n395:                 raise EmbeddingError(\"Embedding API response is invalid\")\n396:             for i, emb in enumerate(embeddings):\n397:                 if not isinstance(emb, list):\n398:                     logger.error(f\"Embedding at index {i} is not a list\")\n399:                     raise EmbeddingError(\"Embedding API response is invalid\")\n400:                 for j, val in enumerate(emb):\n401:                     if not isinstance(val, (int, float)):\n402:                         logger.error(f\"Embedding value at [{i}][{j}] is not a number\")\n403:                         raise EmbeddingError(\"Embedding API response is invalid\")\n404:             \n405:             # Validate embedding count matches input count\n406:             if len(embeddings) != len(texts):\n407:                 logger.error(\n408:                     f\"Embedding count mismatch for {self.provider_mode} mode: expected {len(texts)}, got {len(embeddings)}\"\n409:                 )\n410:                 raise EmbeddingError(\n411:                     f\"Embedding count mismatch: expected {len(texts)}, got {len(embeddings)}\"\n412:                 )\n413:             \n414:             return embeddings\n415:             \n416:         except httpx.TimeoutException as e:\n417:             logger.warning(\n418:                 f\"Embedding batch request timed out for {self.provider_mode} mode: {e}\"\n419:             )\n420:             raise EmbeddingError(\"Embedding batch request timed out\")\n421:         except httpx.HTTPError as e:\n422:             # Check if this is a token overflow error\n423:             error_msg = str(e)\n424:             response_text = \"\"\n425:             \n426:             # Try to get response text from the exception if available\n427:             try:\n428:                 resp = getattr(e, 'response', None)\n429:                 if resp is not None:\n430:                     response_text = resp.text.lower()\n431:             except Exception:\n432:                 pass\n433:             \n434:             if self._is_token_overflow_error(error_msg) or self._is_token_overflow_error(response_text):\n435:                 logger.warning(\n436:                     f\"Token overflow error for {self.provider_mode} mode: {response_text}\"\n437:                 )\n438:                 # Handle overflow using the shared helper\n439:                 return await self._handle_overflow_retry(\n440:                     client, texts, max_retries, min_sub_size, retry_count\n441:                 )\n442:             else:\n443:                 # Not a token overflow error, re-raise\n444:                 logger.error(\n445:                     f\"Embedding batch HTTP error for {self.provider_mode} mode: {e}\"\n446:                 )\n447:                 raise EmbeddingError(\"Embedding batch HTTP error occurred\")\n448: \n449:     def _split_text_at_midpoint(self, text: str) -> tuple:\n450:         \"\"\"\n451:         Split a single text into two parts at a boundary-aware midpoint.\n452:         \n453:         Prefers splitting at newline or space characters near the midpoint\n454:         to produce more natural splits.\n455:         \n456:         Args:\n457:             text: The text to split\n458:             \n459:         Returns:\n460:             Tuple of (left_text, right_text)\n461:         \"\"\"\n462:         if len(text) <= 1:\n463:             return (text, \"\")\n464:         \n465:         midpoint = len(text) // 2\n466:         \n467:         # Try to find a better boundary near the midpoint\n468:         # Look for newline first, then space\n469:         search_start = max(0, midpoint - 50)\n470:         search_end = min(len(text), midpoint + 50)\n471:         \n472:         # Search for newline near midpoint\n473:         for i in range(midpoint, search_end):\n474:             if text[i] == '\\n':\n475:                 return (text[:i+1], text[i+1:])\n476:         \n477:         for i in range(midpoint - 1, search_start - 1, -1):\n478:             if text[i] == '\\n':\n479:                 return (text[:i+1], text[i+1:])\n480:         \n481:         # Search for space near midpoint\n482:         for i in range(midpoint, search_end):\n483:             if text[i] == ' ':\n484:                 return (text[:i], text[i:])\n485:         \n486:         for i in range(midpoint - 1, search_start - 1, -1):\n487:             if text[i] == ' ':\n488:                 return (text[:i], text[i:])\n489:         \n490:         # Fall back to strict midpoint\n491:         return (text[:midpoint], text[midpoint:])\n492:     \n493:     def _mean_pool_embeddings(self, emb1: List[float], emb2: List[float]) -> List[float]:\n494:         \"\"\"\n495:         Mean-pool two embedding vectors into one.\n496:         \n497:         Args:\n498:             emb1: First embedding vector\n499:             emb2: Second embedding vector\n500:             \n501:         Returns:\n502:             Mean-pooled embedding vector\n503:         \"\"\"\n504:         if len(emb1) != len(emb2):\n505:             raise EmbeddingError(\n506:                 f\"Cannot mean-pool embeddings of different dimensions: {len(emb1)} vs {len(emb2)}\"\n507:             )\n508:         \n509:         return [(a + b) / 2.0 for a, b in zip(emb1, emb2)]\n510:     \n511:     async def _handle_overflow_retry(self, client: httpx.AsyncClient, texts: List[str],\n512:                                      max_retries: int, min_sub_size: int, retry_count: int) -> List[List[float]]:\n513:         \"\"\"\n514:         Helper method to handle overflow retry logic with bounded retries and minimum split size.\n515:         \n516:         For single-item overflow, attempts to split the text and mean-pool the results.\n517:         For multi-item overflow, splits the batch and processes each half.\n518:         \n519:         Args:\n520:             client: HTTP client for making requests\n521:             texts: List of texts to embed\n522:             max_retries: Maximum number of retry attempts\n523:             min_sub_size: Minimum sub-batch size before giving up\n524:             retry_count: Current retry attempt count\n525:             \n526:         Returns:\n527:             List of embedding vectors\n528:             \n529:         Raises:\n530:             EmbeddingError: If bounded retries exhausted or split size too small\n531:         \"\"\"\n532:         # Check if we've exhausted retries\n533:         if retry_count > max_retries:\n534:             logger.error(\n535:                 f\"Max retries ({max_retries}) exhausted for embedding batch in {self.provider_mode} mode\"\n536:             )\n537:             raise EmbeddingError(\n538:                 f\"Max retries ({max_retries}) exhausted for embedding batch\"\n539:             )\n540:         \n541:         # Handle single-item overflow with text splitting\n542:         if len(texts) == 1:\n543:             single_text = texts[0]\n544:             \n545:             # Check if text is too short to split - raise actionable error\n546:             if len(single_text) < self.MIN_SPLIT_CHARS:\n547:                 logger.warning(\n548:                     f\"Single input ({len(single_text)} chars) is below minimum split threshold ({self.MIN_SPLIT_CHARS}) in {self.provider_mode} mode\"\n549:                 )\n550:                 raise EmbeddingError(\n551:                     f\"Single input ({len(single_text)} chars) exceeds token limit and is too short to split. \"\n552:                     f\"Ensure chunk_size_chars is below server batch size limit (minimum {self.MIN_SPLIT_CHARS} chars required for recovery).\"\n553:                 )\n554:             \n555:             # Split text at boundary-aware midpoint and recurse\n556:             left_text, right_text = self._split_text_at_midpoint(single_text)\n557:             \n558:             logger.info(\n559:                 f\"Splitting single input ({len(single_text)} chars) into parts ({len(left_text)} + {len(right_text)} chars), retry {retry_count}\"\n560:             )\n561:             \n562:             # Small bounded async backoff\n563:             backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n564:             await asyncio.sleep(backoff_delay)\n565:             \n566:             # Recurse on each part with incremented retry count\n567:             left_embeddings = await self._embed_batch_with_retry(\n568:                 client, [left_text], max_retries, min_sub_size, retry_count=retry_count + 1\n569:             )\n570:             right_embeddings = await self._embed_batch_with_retry(\n571:                 client, [right_text], max_retries, min_sub_size, retry_count=retry_count + 1\n572:             )\n573:             \n574:             # Mean-pool the two embeddings into one\n575:             pooled = self._mean_pool_embeddings(left_embeddings[0], right_embeddings[0])\n576:             \n577:             # Return single embedding to preserve one-embedding-per-input contract\n578:             return [pooled]\n579:         \n580:         # Multi-item batch overflow - use existing split behavior\n581:         # Check if we've reached minimum split size\n582:         if len(texts) <= min_sub_size:\n583:             logger.warning(\n584:                 f\"Cannot split batch further in {self.provider_mode} mode: {len(texts)} items below minimum split size ({min_sub_size})\"\n585:             )\n586:             raise EmbeddingError(\n587:                 f\"Cannot split batch further: {len(texts)} items below minimum split size\"\n588:             )\n589:         \n590:         # Split at midpoint and recurse with backoff\n591:         midpoint = len(texts) // 2\n592:         left_texts = texts[:midpoint]\n593:         right_texts = texts[midpoint:]\n594:         \n595:         # Small bounded async backoff (exponential, capped at 1s)\n596:         backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n597:         await asyncio.sleep(backoff_delay)\n598:         \n599:         # Recurse on left then right to preserve order\n600:         left_embeddings = await self._embed_batch_with_retry(\n601:             client, left_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n602:         )\n603:         right_embeddings = await self._embed_batch_with_retry(\n604:             client, right_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n605:         )\n606:         \n607:         return left_embeddings + right_embeddings\n608:     \n609:     def _is_token_overflow_error(self, error_msg: str) -> bool:\n610:         \"\"\"\n611:         Detect if an error message indicates a token overflow from llama.cpp.\n612:         \n613:         Args:\n614:             error_msg: The error message string\n615:             \n616:         Returns:\n617:             True if this is a token overflow error, False otherwise\n618:         \"\"\"\n619:         error_lower = error_msg.lower()\n620:         \n621:         # Check for common llama.cpp token overflow patterns\n622:         # Pattern 1: \"input (X tokens) is too large\" - typical llama.cpp error\n623:         if \"input (\" in error_lower and \"tokens) is too large\" in error_lower:\n624:             return True\n625:         \n626:         # Pattern 2: \"too large to process\" with \"current batch size\" - OpenAI mode error\n627:         if \"too large to process\" in error_lower and \"current batch size\" in error_lower:\n628:             return True\n629:         \n630:         # Pattern 3: \"token limit exceeded\"\n631:         if \"token limit exceeded\" in error_lower:\n632:             return True\n633:         \n634:         # Pattern 4: \"batch size too small\"\n635:         if \"batch size too small\" in error_lower:\n636:             return True\n637:         \n638:         return False\n639: \n640: \n641: \n\n(End of file - total 641 lines)\n</content>","timestamp":1771937118155,"originalBytes":29292}