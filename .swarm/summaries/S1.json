{"id":"S1","summaryText":"[SUMMARY S1] 26.0 KB | code | 624 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\api\\routes\\documents.py</path>\n<type>file</type>\n<content>1: \"\"\"\r\n2: Documents API routes for file management and processing.\r\n3: \r\nâ†’ Use /swarm retrieve S1 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\api\\routes\\documents.py</path>\n<type>file</type>\n<content>1: \"\"\"\r\n2: Documents API routes for file management and processing.\r\n3: \r\n4: Provides endpoints for listing documents, uploading files, scanning directories,\r\n5: and managing document processing status.\r\n6: \"\"\"\r\n7: import asyncio\n8: import hashlib\n9: import hmac\n10: import logging\r\n11: import os\r\n12: import re\r\n13: import sqlite3\r\n14: from pathlib import Path\r\n15: from typing import Any, Dict, List, Optional\r\n16: \r\n17: import aiofiles\r\n18: from fastapi import APIRouter, Depends, HTTPException, Request, UploadFile, File, Query\n19: from fastapi.exceptions import RequestValidationError\r\n20: from pydantic import BaseModel, ConfigDict, Field\r\n21: \r\n22: from app.config import settings, Settings\n23: from app.services.document_processor import DocumentProcessor, DocumentProcessingError, DuplicateFileError\n24: from app.services.vector_store import VectorStore\n25: from app.services.embeddings import EmbeddingService\n26: from app.services.secret_manager import SecretManager\n27: from app.models.database import SQLiteConnectionPool\n28: from app.api.deps import get_secret_manager, get_background_processor, get_vector_store, get_embedding_service, get_settings, get_db, get_db_pool\n29: from app.security import csrf_protect, require_scope, require_auth\n30: from app.limiter import limiter\n31: from app.services.background_tasks import BackgroundProcessor\n32: \r\n33: \r\n34: def secure_filename(filename: str) -> str:\r\n35:     \"\"\"\r\n36:     Sanitize a filename to prevent security issues.\r\n37:     \r\n38:     - Strips paths using os.path.basename\r\n39:     - Removes non-ASCII characters\r\n40:     - Replaces spaces with underscores\r\n41:     - Allows only alphanumeric, dots, hyphens, and underscores\r\n42:     \"\"\"\r\n43:     # Strip paths\r\n44:     filename = os.path.basename(filename)\r\n45:     \r\n46:     # Replace spaces with underscores\r\n47:     filename = filename.replace(\" \", \"_\")\r\n48:     \r\n49:     # Remove non-ASCII characters\r\n50:     filename = filename.encode(\"ascii\", \"ignore\").decode(\"ascii\")\r\n51:     \r\n52:     # Allow only alphanumeric, dots, hyphens, and underscores\r\n53:     filename = re.sub(r\"[^a-zA-Z0-9._-]\", \"\", filename)\r\n54:     \r\n55:     return filename\r\n56: \r\n57: \r\n58: logger = logging.getLogger(__name__)\r\n59: \r\n60: \r\n61: router = APIRouter(prefix=\"/documents\", tags=[\"documents\"])\r\n62: \r\n63: \r\n64: def _sanitize_metadata(metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:\r\n65:     if not isinstance(metadata, dict):\r\n66:         return {}\r\n67:     sanitized: Dict[str, Any] = {}\r\n68:     for key, value in metadata.items():\r\n69:         if not isinstance(key, str):\r\n70:             continue\r\n71:         if key.lower() in {\"password\", \"ssn\", \"secret\", \"token\"}:\r\n72:             continue\r\n73:         if isinstance(value, str) and len(value) > 256:\r\n74:             sanitized[key] = value[:256]\r\n75:         else:\r\n76:             sanitized[key] = value\r\n77:     return sanitized\r\n78: \r\n79: \r\n80: def _record_document_action(\n81:     file_id: int,\n82:     action: str,\n83:     status: str,\n84:     user_id: str,\n85:     secret_manager: SecretManager,\n86:     conn: sqlite3.Connection,\n87: ) -> None:\n88:     key, key_version = secret_manager.get_hmac_key()\n89:     message = f\"{file_id}|{action}|{status}|{user_id}\"\n90:     digest = hmac.new(key, message.encode(\"utf-8\"), hashlib.sha256).hexdigest()\n91:     conn.execute(\n92:         \"\"\"\n93:         INSERT INTO document_actions(file_id, action, status, user_id, hmac_sha256)\n94:         VALUES (?, ?, ?, ?, ?)\n95:         \"\"\",\n96:         (file_id, action, status, user_id, digest),\n97:     )\n98: \n99: \n100: @router.post(\"/admin/retry/{file_id}\")\n101: @limiter.limit(settings.admin_rate_limit)\n102: async def retry_document(\n103:     file_id: int,\n104:     request: Request,\n105:     conn: sqlite3.Connection = Depends(get_db),\n106:     auth: dict = Depends(require_scope(\"documents:manage\")),\n107:     csrf_token: str = Depends(csrf_protect),\n108:     secret_manager: SecretManager = Depends(get_secret_manager),\n109:     background_processor: BackgroundProcessor = Depends(get_background_processor),\n110: ) -> dict:\n111:     try:\n112:         cursor = await asyncio.to_thread(conn.execute, \"SELECT file_path FROM files WHERE id = ?\", (file_id,))\n113:         row = await asyncio.to_thread(cursor.fetchone)\n114:         if not row:\n115:             raise HTTPException(status_code=404, detail=\"Document not found\")\n116: \n117:         # Ensure processor is running\n118:         if not background_processor.is_running:\n119:             await background_processor.start()\n120: \n121:         await background_processor.enqueue(row[\"file_path\"])\n122: \n123:         await asyncio.to_thread(\n124:             _record_document_action,\n125:             file_id,\n126:             \"retry\",\n127:             \"scheduled\",\n128:             auth.get(\"user_id\", \"unknown\"),\n129:             secret_manager,\n130:             conn,\n131:         )\n132:         await asyncio.to_thread(conn.commit)\n133:         return {\"file_id\": file_id, \"status\": \"scheduled\"}\n134:     except HTTPException:\n135:         raise\n136:     except Exception as exc:\n137:         logger.exception(\"Error reprocessing document %d\", file_id)\n138:         await asyncio.to_thread(\n139:             _record_document_action,\n140:             file_id,\n141:             \"retry\",\n142:             \"error\",\n143:             auth.get(\"user_id\", \"unknown\"),\n144:             secret_manager,\n145:             conn,\n146:         )\n147:         await asyncio.to_thread(conn.commit)\n148:         raise HTTPException(status_code=500, detail=f\"Retry failed: {exc}\")\n149: \r\n150: \r\n151: class DocumentResponse(BaseModel):\r\n152:     \"\"\"Response model for a document record - frontend compatible.\"\"\"\r\n153:     id: int\r\n154:     file_name: str\r\n155:     filename: str  # Frontend alias\r\n156:     file_path: str\r\n157:     status: str\r\n158:     chunk_count: int\r\n159:     size: Optional[int] = None  # Frontend expects size\r\n160:     created_at: Optional[str]\r\n161:     processed_at: Optional[str]\r\n162:     metadata: Optional[dict] = None  # Frontend expects metadata\r\n163: \r\n164:     model_config = ConfigDict(from_attributes=True)\n165: \r\n166: \r\n167: class DocumentListResponse(BaseModel):\r\n168:     \"\"\"Response model for listing documents - frontend compatible with total.\"\"\"\r\n169:     documents: List[DocumentResponse]\r\n170:     total: int\r\n171: \r\n172: \r\n173: class DocumentStatsResponse(BaseModel):\r\n174:     \"\"\"Response model for document statistics - frontend compatible.\"\"\"\r\n175:     total_documents: int  # Frontend expects this field\r\n176:     total_chunks: int\r\n177:     total_size_bytes: int = 0  # Frontend expects this field\r\n178:     documents_by_status: dict = Field(default_factory=dict)  # Frontend expects this field\r\n179:     total_files: int = 0  # Backward compatibility alias\r\n180:     status: str = \"success\"\r\n181: \r\n182: \r\n183: class UploadResponse(BaseModel):\r\n184:     \"\"\"Response model for file upload - frontend compatible.\"\"\"\r\n185:     file_id: int\r\n186:     file_name: str\r\n187:     id: int  # Frontend alias for file_id\r\n188:     filename: str  # Frontend alias for file_name\r\n189:     status: str\r\n190:     message: str\r\n191: \r\n192: \r\n193: class ScanResponse(BaseModel):\r\n194:     \"\"\"Response model for directory scan - frontend compatible.\"\"\"\r\n195:     files_enqueued: int\r\n196:     status: str\r\n197:     message: str\r\n198:     added: int  # Frontend alias for files_enqueued\r\n199:     scanned: int  # Frontend expects this field (total files scanned)\r\n200:     errors: List[str] = Field(default_factory=list)  # Frontend expects this field\r\n201: \r\n202: \r\n203: class DeleteResponse(BaseModel):\r\n204:     \"\"\"Response model for document deletion.\"\"\"\r\n205:     file_id: int\r\n206:     status: str\r\n207:     message: str\r\n208: \r\n209: \r\n210: def _row_to_document_response(row: sqlite3.Row) -> DocumentResponse:\r\n211:     \"\"\"Convert a database row to a DocumentResponse.\"\"\"\r\n212:     file_name = row[\"file_name\"]\r\n213:     chunk_count = row[\"chunk_count\"] or 0\r\n214:     status = row[\"status\"]\r\n215:     return DocumentResponse(\r\n216:         id=row[\"id\"],\r\n217:         file_name=file_name,\r\n218:         filename=file_name,  # Frontend alias\r\n219:         file_path=row[\"file_path\"],\r\n220:         status=status,\r\n221:         chunk_count=chunk_count,\r\n222:         size=row[\"file_size\"] if \"file_size\" in row.keys() and row[\"file_size\"] is not None else None,\r\n223:         created_at=row[\"created_at\"],\r\n224:         processed_at=row[\"processed_at\"],\r\n225:         metadata={\r\n226:             \"status\": status,\r\n227:             \"chunk_count\": chunk_count,\r\n228:             \"chunks\": chunk_count,  # Backward compatibility\r\n229:         },\r\n230:     )\r\n231: \r\n232: \r\n233: @router.get(\"\", response_model=DocumentListResponse)\n234: @router.get(\"/\", response_model=DocumentListResponse)\n235: async def list_documents(\n236:     vault_id: Optional[int] = Query(None, description=\"Filter by vault ID\"),\n237:     conn: sqlite3.Connection = Depends(get_db),\n238: ):\n239:     \"\"\"\n240:     List all documents from the files table.\n241: \n242:     Returns a list of all files with their id, file_name, file_path, status,\n243:     chunk_count, created_at, and processed_at fields.\n244:     Optionally filter by vault_id.\n245:     \"\"\"\n246:     if vault_id is not None:\n247:         cursor = await asyncio.to_thread(\n248:             conn.execute,\n249:             \"\"\"\n250:             SELECT id, file_name, file_path, status, chunk_count, created_at, processed_at\n251:             FROM files\n252:             WHERE vault_id = ?\n253:             ORDER BY created_at DESC\n254:             \"\"\",\n255:             (vault_id,),\n256:         )\n257:     else:\n258:         cursor = await asyncio.to_thread(\n259:             conn.execute,\n260:             \"\"\"\n261:             SELECT id, file_name, file_path, status, chunk_count, created_at, processed_at\n262:             FROM files\n263:             ORDER BY created_at DESC\n264:             \"\"\"\n265:         )\n266:     rows = await asyncio.to_thread(cursor.fetchall)\n267:     \n268:     documents = [_row_to_document_response(row) for row in rows]\n269:     \n270:     return DocumentListResponse(documents=documents, total=len(documents))\n271: \r\n272: \r\n273: @router.get(\"/stats\", response_model=DocumentStatsResponse)\n274: async def get_document_stats(\n275:     vault_id: Optional[int] = Query(None, description=\"Filter by vault ID\"),\n276:     conn: sqlite3.Connection = Depends(get_db),\n277: ):\n278:     \"\"\"\n279:     Get counts of files and chunks.\n280: \n281:     Returns total number of files in the database, total chunks,\n282:     total size in bytes, and documents grouped by status.\n283:     Optionally filter by vault_id.\n284:     \"\"\"\n285:     # Get total files count\n286:     if vault_id is not None:\n287:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COUNT(*) as total_files FROM files WHERE vault_id = ?\", (vault_id,))\n288:     else:\n289:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COUNT(*) as total_files FROM files\")\n290:     row = await asyncio.to_thread(cursor.fetchone)\n291:     total_files = row[\"total_files\"]\n292:     \n293:     # Get total chunks count\n294:     if vault_id is not None:\n295:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(chunk_count), 0) as total_chunks FROM files WHERE vault_id = ?\", (vault_id,))\n296:     else:\n297:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(chunk_count), 0) as total_chunks FROM files\")\n298:     row = await asyncio.to_thread(cursor.fetchone)\n299:     total_chunks = row[\"total_chunks\"]\n300:     \n301:     # Get total size (sum of file_size if column exists, otherwise 0)\n302:     try:\n303:         if vault_id is not None:\n304:             cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(file_size), 0) as total_size FROM files WHERE vault_id = ?\", (vault_id,))\n305:         else:\n306:             cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(file_size), 0) as total_size FROM files\")\n307:         row = await asyncio.to_thread(cursor.fetchone)\n308:         total_size_bytes = row[\"total_size\"] or 0\n309:     except sqlite3.OperationalError:\n310:         total_size_bytes = 0\n311:     \n312:     # Get documents grouped by status\n313:     if vault_id is not None:\n314:         cursor = await asyncio.to_thread(conn.execute, \"SELECT status, COUNT(*) as count FROM files WHERE vault_id = ? GROUP BY status\", (vault_id,))\n315:     else:\n316:         cursor = await asyncio.to_thread(conn.execute, \"SELECT status, COUNT(*) as count FROM files GROUP BY status\")\n317:     rows = await asyncio.to_thread(cursor.fetchall)\n318:     documents_by_status = {row[\"status\"]: row[\"count\"] for row in rows}\n319:     \n320:     return DocumentStatsResponse(\n321:         total_documents=total_files,  # Frontend field\n322:         total_chunks=total_chunks,\n323:         total_size_bytes=total_size_bytes,\n324:         documents_by_status=documents_by_status,\n325:         total_files=total_files,  # Backward compatibility\n326:     )\n327: \r\n328: \r\n329: @router.post(\"\", response_model=UploadResponse)\n330: @router.post(\"/\", response_model=UploadResponse)\n331: async def upload_document_root(\n332:     request: Request,\n333:     file: Optional[UploadFile] = None,\n334:     vault_id: int = Query(1, description=\"Target vault ID\"),\n335:     settings_dep: Settings = Depends(get_settings),\n336:     vector_store: VectorStore = Depends(get_vector_store),\n337:     embedding_service: EmbeddingService = Depends(get_embedding_service),\n338:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n339:     auth: dict = Depends(require_auth),\n340: ):\n341:     \"\"\"\n342:     Upload endpoint at root /documents for frontend compatibility.\n343:     Delegates to the main upload handler.\n344:     \"\"\"\n345:     return await _do_upload(request, file, settings_dep, vector_store, embedding_service, db_pool, vault_id)\n346: \n347: \n348: @router.post(\"/upload\", response_model=UploadResponse)\n349: async def upload_document(\n350:     request: Request,\n351:     file: Optional[UploadFile] = None,\n352:     vault_id: int = Query(1, description=\"Target vault ID\"),\n353:     settings_dep: Settings = Depends(get_settings),\n354:     vector_store: VectorStore = Depends(get_vector_store),\n355:     embedding_service: EmbeddingService = Depends(get_embedding_service),\n356:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n357:     auth: dict = Depends(require_auth),\n358: ):\n359:     \"\"\"\n360:     Upload a file and process it with strict security controls.\n361: \n362:     Validates filename, extension, and file size before saving.\n363:     Saves the uploaded file to settings.uploads_dir using aiofiles,\n364:     then processes it via DocumentProcessor.process_file in asyncio.to_thread.\n365:     \"\"\"\n366:     return await _do_upload(request, file, settings_dep, vector_store, embedding_service, db_pool, vault_id)\n367: \n368: \n369: async def _do_upload(\n370:     request: Request,\n371:     file: Optional[UploadFile],\n372:     settings_dep: Settings,\n373:     vector_store: VectorStore,\n374:     embedding_service: EmbeddingService,\n375:     db_pool: SQLiteConnectionPool,\n376:     vault_id: int,\n377: ) -> UploadResponse:\n378:     # Validate file is provided\n379:     if file is None:\n380:         raise HTTPException(status_code=400, detail=\"No file provided\")\n381: \n382:     # Validate filename is not empty\n383:     if not file.filename:\n384:         raise HTTPException(status_code=400, detail=\"Filename cannot be empty\")\n385: \n386:     # Ensure uploads directory exists\n387:     uploads_dir = settings_dep.uploads_dir\n388:     uploads_dir.mkdir(parents=True, exist_ok=True)\n389:     \n390:     # Sanitize filename\n391:     file_name = secure_filename(file.filename or \"unnamed_file\")\n392:     if not file_name:\n393:         file_name = \"unnamed_file.txt\"\n394: \n395:     # Ensure file has an extension for validation\n396:     if not Path(file_name).suffix:\n397:         file_name = f\"{file_name}.txt\"\n398:     \r\n399:     # Validate file extension\n400:     file_suffix = Path(file_name).suffix.lower()\n401:     if file_suffix not in settings_dep.allowed_extensions:\n402:         raise HTTPException(\n403:             status_code=400,\n404:             detail=f\"File extension '{file_suffix}' not allowed. Allowed: {settings_dep.allowed_extensions}\"\n405:         )\n406: \n407:     # Validate file size from content-length header\n408:     max_size_bytes = settings_dep.max_file_size_mb * 1024 * 1024\n409:     content_length = request.headers.get(\"content-length\")\r\n410:     if content_length:\r\n411:         try:\r\n412:             if int(content_length) > max_size_bytes:\r\n413:                 raise HTTPException(status_code=413, detail=f\"File too large. Max size: {settings.max_file_size_mb}MB\")\r\n414:         except ValueError:\r\n415:             pass  # Invalid content-length header, will check during streaming\r\n416:     \r\n417:     # Generate safe file path\r\n418:     file_path = uploads_dir / file_name\r\n419:     \r\n420:     # Handle duplicate file names\r\n421:     counter = 1\r\n422:     original_path = file_path\r\n423:     while file_path.exists():\r\n424:         stem = original_path.stem\r\n425:         suffix = original_path.suffix\r\n426:         file_path = uploads_dir / f\"{stem}_{counter}{suffix}\"\r\n427:         counter += 1\r\n428:     \r\n429:     # Path safety: ensure file_path is within uploads_dir\r\n430:     try:\r\n431:         resolved_path = file_path.resolve()\r\n432:         resolved_uploads_dir = uploads_dir.resolve()\r\n433:         if not str(resolved_path).startswith(str(resolved_uploads_dir)):\r\n434:             raise HTTPException(status_code=400, detail=\"Invalid file path\")\r\n435:     except (OSError, ValueError):\r\n436:         raise HTTPException(status_code=400, detail=\"Invalid file path\")\r\n437:     \r\n438:     temp_file_path = None\r\n439:     try:\r\n440:         # Save file using aiofiles with chunked reading and size validation\r\n441:         total_bytes = 0\r\n442:         temp_file_path = file_path\r\n443:         async with aiofiles.open(temp_file_path, \"wb\") as f:\r\n444:             while chunk := await file.read(1024 * 1024):  # Read 1MB chunks\r\n445:                 total_bytes += len(chunk)\r\n446:                 if total_bytes > max_size_bytes:\r\n447:                     # Close and delete partial file\r\n448:                     await f.close()\r\n449:                     if temp_file_path.exists():\r\n450:                         temp_file_path.unlink(missing_ok=True)\r\n451:                     raise HTTPException(status_code=413, detail=f\"File too large. Max size: {settings.max_file_size_mb}MB\")\r\n452:                 await f.write(chunk)\r\n453:         \r\n454:         # Process file with injected dependencies\n455:         processor = DocumentProcessor(\n456:             chunk_size=settings_dep.chunk_size,\n457:             chunk_overlap=settings_dep.chunk_overlap,\n458:             vector_store=vector_store,\n459:             embedding_service=embedding_service,\n460:             pool=db_pool,\n461:         )\n462: \r\n463:         try:\n464:             result = await processor.process_file(str(file_path), vault_id=vault_id)\n465:             \r\n466:             return UploadResponse(\r\n467:                 file_id=result.file_id,\r\n468:                 file_name=file_name,\r\n469:                 id=result.file_id,  # Frontend alias\r\n470:                 filename=file_name,  # Frontend alias\r\n471:                 status=\"indexed\",\r\n472:                 message=f\"File '{file_name}' uploaded and processed successfully with {len(result.chunks)} chunks\",\r\n473:             )\r\n474:         except DuplicateFileError as e:\n475:             # File is a duplicate, remove the uploaded file\n476:             file_path.unlink(missing_ok=True)\n477:             raise HTTPException(status_code=409, detail=f\"{e} (uploaded file was cleaned up)\")\n478:         except HTTPException:\n479:             # Clean up partial file if it exists\n480:             if temp_file_path and temp_file_path.exists():\n481:                 temp_file_path.unlink(missing_ok=True)\n482:             raise\n483:         except DocumentProcessingError as e:\n484:             logger.exception(\"Document processing error for file: %s\", file_name)\n485:             raise HTTPException(status_code=500, detail=f\"Processing error: {e}\")\n486:         except Exception as e:\n487:             logger.exception(\"Unexpected error processing file: %s\", file_name)\n488:             raise HTTPException(status_code=500, detail=f\"Server error: {e}\")\n489:     except Exception as e:\r\n490:         logger.exception(\"Error uploading file: %s\", file_name)\r\n491:         # Clean up file if it was created\r\n492:         if temp_file_path and temp_file_path.exists():\r\n493:             temp_file_path.unlink(missing_ok=True)\r\n494:         raise HTTPException(status_code=500, detail=f\"Upload failed: {e}\")\r\n495: \r\n496: \r\n497: @router.post(\"/scan\", response_model=ScanResponse)\n498: async def scan_directories(\n499:     request: Request,\n500:     background_processor: BackgroundProcessor = Depends(get_background_processor),\n501:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n502:     auth: dict = Depends(require_auth),\n503: ):\n504:     \"\"\"\n505:     Trigger a scan of configured directories for new files.\n506: \n507:     Calls FileWatcher.scan_once() to find and enqueue new files\n508:     from uploads_dir and library_dir that are not in the database.\n509: \n510:     Uses the singleton BackgroundProcessor that runs continuously in the background.\n511:     \"\"\"\n512:     from app.services.file_watcher import FileWatcher\n513: \n514:     # Ensure processor is running (it should be from lifespan, but double-check)\n515:     if not background_processor.is_running:\n516:         await background_processor.start()\n517: \n518:     try:\n519:         watcher = FileWatcher(background_processor, pool=db_pool)\n520: \n521:         # Perform scan\n522:         files_enqueued = await watcher.scan_once()\n523: \n524:         if files_enqueued > 0:\n525:             message = f\"Scan complete: {files_enqueued} new files enqueued for processing\"\n526:         else:\n527:             message = \"Scan complete: no new files found\"\n528: \n529:         return ScanResponse(\n530:             files_enqueued=files_enqueued,\n531:             status=\"success\",\n532:             message=message,\n533:             added=files_enqueued,  # Frontend alias\n534:             scanned=files_enqueued,  # Frontend expects this (at least files_enqueued)\n535:             errors=[],  # Frontend expects this field\n536:         )\n537:     except Exception as e:\n538:         logger.exception(\"Error during directory scan\")\n539:         raise HTTPException(status_code=500, detail=f\"Scan failed: {e}\")\n540:     # Note: No finally block to stop processor - it runs continuously\n541: \r\n542: \r\n543: @router.delete(\"/{file_id}\", response_model=DeleteResponse)\n544: async def delete_document(\n545:     file_id: int,\n546:     request: Request,\n547:     conn: sqlite3.Connection = Depends(get_db),\n548:     auth: dict = Depends(require_auth),\n549:     vector_store: VectorStore = Depends(get_vector_store),\n550: ):\n551:     \"\"\"\n552:     Delete a document by ID.\n553: \n554:     Deletes the file record from the database and removes all associated\n555:     chunks from the vector store. Returns 404 if the file is not found.\n556:     \"\"\"\n557:     # Check if file exists\n558:     cursor = await asyncio.to_thread(conn.execute, \"SELECT id, file_name FROM files WHERE id = ?\", (file_id,))\n559:     row = await asyncio.to_thread(cursor.fetchone)\n560:     \n561:     if row is None:\n562:         raise HTTPException(status_code=404, detail=f\"Document with id {file_id} not found\")\n563:     \n564:     file_name = row[\"file_name\"]\n565: \n566:     try:\n567:         # Delete from vector store first (wrapped in to_thread to avoid blocking)\n568:         try:\n569:             db = vector_store.db\n570:             if db is not None and \"chunks\" in db.table_names():\n571:                 vector_store.table = db.open_table(\"chunks\")\n572:                 deleted_chunks = await asyncio.to_thread(vector_store.delete_by_file, str(file_id))\n573:                 logger.info(\"Deleted %d chunks from vector store for file_id %s\", deleted_chunks, file_id)\n574:             else:\n575:                 logger.debug(\"Chunks table not found, skipping vector store deletion for file_id %s\", file_id)\n576:         except Exception as e:\n577:             logger.warning(\"Error deleting chunks from vector store: %s\", e)\n578:             # Continue with database deletion even if vector store fails\n579: \n580:         # Delete from database\n581:         await asyncio.to_thread(conn.execute, \"DELETE FROM files WHERE id = ?\", (file_id,))\n582:         await asyncio.to_thread(conn.commit)\n583:         \n584:         return DeleteResponse(\n585:             file_id=file_id,\n586:             status=\"success\",\n587:             message=f\"Document '{file_name}' (id: {file_id}) deleted successfully\",\n588:         )\n589:     except HTTPException:\n590:         raise\n591:     except Exception as e:\n592:         await asyncio.to_thread(conn.rollback)\n593:         logger.exception(\"Error deleting document %d\", file_id)\n594:         raise HTTPException(status_code=500, detail=f\"Delete failed: {e}\")\n595: \n596: \n597: # Exception handler for validation errors (e.g., empty filename)\n598: # This is registered at the app level in main.py\n599: async def validation_exception_handler(request: Request, exc: RequestValidationError):\n600:     \"\"\"Convert validation errors to 400 for empty filename cases only.\"\"\"\n601:     errors = exc.errors()\n602:     for error in errors:\n603:         if error.get(\"loc\") == (\"body\", \"file\") and \"filename\" in str(error.get(\"input\", \"\")).lower():\n604:             raise HTTPException(status_code=400, detail=\"Filename cannot be empty\")\n605:     # For all other validation errors, return standard 422\n606:     # Convert errors to dict format for JSON serialization\n607:     from fastapi.responses import JSONResponse\n608:     error_dicts = [\n609:         {\n610:             \"loc\": error.get(\"loc\"),\n611:             \"msg\": error.get(\"msg\"),\n612:             \"type\": error.get(\"type\"),\n613:             \"input\": error.get(\"input\")\n614:         }\n615:         for error in errors\n616:     ]\n617:     return JSONResponse(status_code=422, content={\"detail\": error_dicts})\n618: \n619: \n\n(End of file - total 619 lines)\n</content>","timestamp":1771492745647,"originalBytes":26598}