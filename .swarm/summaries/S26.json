{"id":"S26","summaryText":"[SUMMARY S26] 25.8 KB | code | 623 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\api\\routes\\documents.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Documents API routes for file management and processing.\n3: \nâ†’ Use /swarm retrieve S26 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\api\\routes\\documents.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Documents API routes for file management and processing.\n3: \n4: Provides endpoints for listing documents, uploading files, scanning directories,\n5: and managing document processing status.\n6: \"\"\"\n7: import asyncio\n8: import hashlib\n9: import hmac\n10: import logging\n11: import os\n12: import re\n13: import sqlite3\n14: from pathlib import Path\n15: from typing import Any, Dict, List, Optional\n16: \n17: import aiofiles\n18: from fastapi import APIRouter, Depends, HTTPException, Request, UploadFile, File, Query\n19: from fastapi.exceptions import RequestValidationError\n20: from pydantic import BaseModel, ConfigDict, Field\n21: \n22: from app.config import settings, Settings\n23: from app.services.document_processor import DocumentProcessor, DocumentProcessingError, DuplicateFileError\n24: from app.services.vector_store import VectorStore\n25: from app.services.embeddings import EmbeddingService\n26: from app.services.secret_manager import SecretManager\n27: from app.models.database import SQLiteConnectionPool\n28: from app.api.deps import get_secret_manager, get_background_processor, get_vector_store, get_embedding_service, get_settings, get_db, get_db_pool\n29: from app.security import csrf_protect, require_scope, require_auth\n30: from app.limiter import limiter\n31: from app.services.background_tasks import BackgroundProcessor\n32: \n33: \n34: def secure_filename(filename: str) -> str:\n35:     \"\"\"\n36:     Sanitize a filename to prevent security issues.\n37:     \n38:     - Strips paths using os.path.basename\n39:     - Removes non-ASCII characters\n40:     - Replaces spaces with underscores\n41:     - Allows only alphanumeric, dots, hyphens, and underscores\n42:     \"\"\"\n43:     # Strip paths\n44:     filename = os.path.basename(filename)\n45:     \n46:     # Replace spaces with underscores\n47:     filename = filename.replace(\" \", \"_\")\n48:     \n49:     # Remove non-ASCII characters\n50:     filename = filename.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n51:     \n52:     # Allow only alphanumeric, dots, hyphens, and underscores\n53:     filename = re.sub(r\"[^a-zA-Z0-9._-]\", \"\", filename)\n54:     \n55:     return filename\n56: \n57: \n58: logger = logging.getLogger(__name__)\n59: \n60: \n61: router = APIRouter(prefix=\"/documents\", tags=[\"documents\"])\n62: \n63: \n64: def _sanitize_metadata(metadata: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n65:     if not isinstance(metadata, dict):\n66:         return {}\n67:     sanitized: Dict[str, Any] = {}\n68:     for key, value in metadata.items():\n69:         if not isinstance(key, str):\n70:             continue\n71:         if key.lower() in {\"password\", \"ssn\", \"secret\", \"token\"}:\n72:             continue\n73:         if isinstance(value, str) and len(value) > 256:\n74:             sanitized[key] = value[:256]\n75:         else:\n76:             sanitized[key] = value\n77:     return sanitized\n78: \n79: \n80: def _record_document_action(\n81:     file_id: int,\n82:     action: str,\n83:     status: str,\n84:     user_id: str,\n85:     secret_manager: SecretManager,\n86:     conn: sqlite3.Connection,\n87: ) -> None:\n88:     key, key_version = secret_manager.get_hmac_key()\n89:     message = f\"{file_id}|{action}|{status}|{user_id}\"\n90:     digest = hmac.new(key, message.encode(\"utf-8\"), hashlib.sha256).hexdigest()\n91:     conn.execute(\n92:         \"\"\"\n93:         INSERT INTO document_actions(file_id, action, status, user_id, hmac_sha256)\n94:         VALUES (?, ?, ?, ?, ?)\n95:         \"\"\",\n96:         (file_id, action, status, user_id, digest),\n97:     )\n98: \n99: \n100: @router.post(\"/admin/retry/{file_id}\")\n101: @limiter.limit(settings.admin_rate_limit)\n102: async def retry_document(\n103:     file_id: int,\n104:     request: Request,\n105:     conn: sqlite3.Connection = Depends(get_db),\n106:     auth: dict = Depends(require_scope(\"documents:manage\")),\n107:     csrf_token: str = Depends(csrf_protect),\n108:     secret_manager: SecretManager = Depends(get_secret_manager),\n109:     background_processor: BackgroundProcessor = Depends(get_background_processor),\n110: ) -> dict:\n111:     try:\n112:         cursor = await asyncio.to_thread(conn.execute, \"SELECT file_path FROM files WHERE id = ?\", (file_id,))\n113:         row = await asyncio.to_thread(cursor.fetchone)\n114:         if not row:\n115:             raise HTTPException(status_code=404, detail=\"Document not found\")\n116: \n117:         # Ensure processor is running\n118:         if not background_processor.is_running:\n119:             await background_processor.start()\n120: \n121:         await background_processor.enqueue(row[\"file_path\"])\n122: \n123:         await asyncio.to_thread(\n124:             _record_document_action,\n125:             file_id,\n126:             \"retry\",\n127:             \"scheduled\",\n128:             auth.get(\"user_id\", \"unknown\"),\n129:             secret_manager,\n130:             conn,\n131:         )\n132:         await asyncio.to_thread(conn.commit)\n133:         return {\"file_id\": file_id, \"status\": \"scheduled\"}\n134:     except HTTPException:\n135:         raise\n136:     except Exception as exc:\n137:         logger.exception(\"Error reprocessing document %d\", file_id)\n138:         await asyncio.to_thread(\n139:             _record_document_action,\n140:             file_id,\n141:             \"retry\",\n142:             \"error\",\n143:             auth.get(\"user_id\", \"unknown\"),\n144:             secret_manager,\n145:             conn,\n146:         )\n147:         await asyncio.to_thread(conn.commit)\n148:         raise HTTPException(status_code=500, detail=f\"Retry failed: {exc}\")\n149: \n150: \n151: class DocumentResponse(BaseModel):\n152:     \"\"\"Response model for a document record - frontend compatible.\"\"\"\n153:     id: int\n154:     file_name: str\n155:     filename: str  # Frontend alias\n156:     file_path: str\n157:     status: str\n158:     chunk_count: int\n159:     size: Optional[int] = None  # Frontend expects size\n160:     created_at: Optional[str]\n161:     processed_at: Optional[str]\n162:     metadata: Optional[dict] = None  # Frontend expects metadata\n163: \n164:     model_config = ConfigDict(from_attributes=True)\n165: \n166: \n167: class DocumentListResponse(BaseModel):\n168:     \"\"\"Response model for listing documents - frontend compatible with total.\"\"\"\n169:     documents: List[DocumentResponse]\n170:     total: int\n171: \n172: \n173: class DocumentStatsResponse(BaseModel):\n174:     \"\"\"Response model for document statistics - frontend compatible.\"\"\"\n175:     total_documents: int  # Frontend expects this field\n176:     total_chunks: int\n177:     total_size_bytes: int = 0  # Frontend expects this field\n178:     documents_by_status: dict = Field(default_factory=dict)  # Frontend expects this field\n179:     total_files: int = 0  # Backward compatibility alias\n180:     status: str = \"success\"\n181: \n182: \n183: class UploadResponse(BaseModel):\n184:     \"\"\"Response model for file upload - frontend compatible.\"\"\"\n185:     file_id: int\n186:     file_name: str\n187:     id: int  # Frontend alias for file_id\n188:     filename: str  # Frontend alias for file_name\n189:     status: str\n190:     message: str\n191: \n192: \n193: class ScanResponse(BaseModel):\n194:     \"\"\"Response model for directory scan - frontend compatible.\"\"\"\n195:     files_enqueued: int\n196:     status: str\n197:     message: str\n198:     added: int  # Frontend alias for files_enqueued\n199:     scanned: int  # Frontend expects this field (total files scanned)\n200:     errors: List[str] = Field(default_factory=list)  # Frontend expects this field\n201: \n202: \n203: class DeleteResponse(BaseModel):\n204:     \"\"\"Response model for document deletion.\"\"\"\n205:     file_id: int\n206:     status: str\n207:     message: str\n208: \n209: \n210: def _row_to_document_response(row: sqlite3.Row) -> DocumentResponse:\n211:     \"\"\"Convert a database row to a DocumentResponse.\"\"\"\n212:     file_name = row[\"file_name\"]\n213:     chunk_count = row[\"chunk_count\"] or 0\n214:     status = row[\"status\"]\n215:     return DocumentResponse(\n216:         id=row[\"id\"],\n217:         file_name=file_name,\n218:         filename=file_name,  # Frontend alias\n219:         file_path=row[\"file_path\"],\n220:         status=status,\n221:         chunk_count=chunk_count,\n222:         size=row[\"file_size\"] if \"file_size\" in row.keys() and row[\"file_size\"] is not None else None,\n223:         created_at=row[\"created_at\"],\n224:         processed_at=row[\"processed_at\"],\n225:         metadata={\n226:             \"status\": status,\n227:             \"chunk_count\": chunk_count,\n228:             \"chunks\": chunk_count,  # Backward compatibility\n229:         },\n230:     )\n231: \n232: \n233: @router.get(\"\", response_model=DocumentListResponse)\n234: @router.get(\"/\", response_model=DocumentListResponse)\n235: async def list_documents(\n236:     vault_id: Optional[int] = Query(None, description=\"Filter by vault ID\"),\n237:     conn: sqlite3.Connection = Depends(get_db),\n238: ):\n239:     \"\"\"\n240:     List all documents from the files table.\n241: \n242:     Returns a list of all files with their id, file_name, file_path, status,\n243:     chunk_count, created_at, and processed_at fields.\n244:     Optionally filter by vault_id.\n245:     \"\"\"\n246:     if vault_id is not None:\n247:         cursor = await asyncio.to_thread(\n248:             conn.execute,\n249:             \"\"\"\n250:             SELECT id, file_name, file_path, status, chunk_count, created_at, processed_at\n251:             FROM files\n252:             WHERE vault_id = ?\n253:             ORDER BY created_at DESC\n254:             \"\"\",\n255:             (vault_id,),\n256:         )\n257:     else:\n258:         cursor = await asyncio.to_thread(\n259:             conn.execute,\n260:             \"\"\"\n261:             SELECT id, file_name, file_path, status, chunk_count, created_at, processed_at\n262:             FROM files\n263:             ORDER BY created_at DESC\n264:             \"\"\"\n265:         )\n266:     rows = await asyncio.to_thread(cursor.fetchall)\n267:     \n268:     documents = [_row_to_document_response(row) for row in rows]\n269:     \n270:     return DocumentListResponse(documents=documents, total=len(documents))\n271: \n272: \n273: @router.get(\"/stats\", response_model=DocumentStatsResponse)\n274: async def get_document_stats(\n275:     vault_id: Optional[int] = Query(None, description=\"Filter by vault ID\"),\n276:     conn: sqlite3.Connection = Depends(get_db),\n277: ):\n278:     \"\"\"\n279:     Get counts of files and chunks.\n280: \n281:     Returns total number of files in the database, total chunks,\n282:     total size in bytes, and documents grouped by status.\n283:     Optionally filter by vault_id.\n284:     \"\"\"\n285:     # Get total files count\n286:     if vault_id is not None:\n287:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COUNT(*) as total_files FROM files WHERE vault_id = ?\", (vault_id,))\n288:     else:\n289:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COUNT(*) as total_files FROM files\")\n290:     row = await asyncio.to_thread(cursor.fetchone)\n291:     total_files = row[\"total_files\"]\n292:     \n293:     # Get total chunks count\n294:     if vault_id is not None:\n295:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(chunk_count), 0) as total_chunks FROM files WHERE vault_id = ?\", (vault_id,))\n296:     else:\n297:         cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(chunk_count), 0) as total_chunks FROM files\")\n298:     row = await asyncio.to_thread(cursor.fetchone)\n299:     total_chunks = row[\"total_chunks\"]\n300:     \n301:     # Get total size (sum of file_size if column exists, otherwise 0)\n302:     try:\n303:         if vault_id is not None:\n304:             cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(file_size), 0) as total_size FROM files WHERE vault_id = ?\", (vault_id,))\n305:         else:\n306:             cursor = await asyncio.to_thread(conn.execute, \"SELECT COALESCE(SUM(file_size), 0) as total_size FROM files\")\n307:         row = await asyncio.to_thread(cursor.fetchone)\n308:         total_size_bytes = row[\"total_size\"] or 0\n309:     except sqlite3.OperationalError:\n310:         total_size_bytes = 0\n311:     \n312:     # Get documents grouped by status\n313:     if vault_id is not None:\n314:         cursor = await asyncio.to_thread(conn.execute, \"SELECT status, COUNT(*) as count FROM files WHERE vault_id = ? GROUP BY status\", (vault_id,))\n315:     else:\n316:         cursor = await asyncio.to_thread(conn.execute, \"SELECT status, COUNT(*) as count FROM files GROUP BY status\")\n317:     rows = await asyncio.to_thread(cursor.fetchall)\n318:     documents_by_status = {row[\"status\"]: row[\"count\"] for row in rows}\n319:     \n320:     return DocumentStatsResponse(\n321:         total_documents=total_files,  # Frontend field\n322:         total_chunks=total_chunks,\n323:         total_size_bytes=total_size_bytes,\n324:         documents_by_status=documents_by_status,\n325:         total_files=total_files,  # Backward compatibility\n326:     )\n327: \n328: \n329: @router.post(\"\", response_model=UploadResponse)\n330: @router.post(\"/\", response_model=UploadResponse)\n331: async def upload_document_root(\n332:     request: Request,\n333:     file: Optional[UploadFile] = None,\n334:     vault_id: int = Query(1, description=\"Target vault ID\"),\n335:     settings_dep: Settings = Depends(get_settings),\n336:     vector_store: VectorStore = Depends(get_vector_store),\n337:     embedding_service: EmbeddingService = Depends(get_embedding_service),\n338:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n339:     auth: dict = Depends(require_auth),\n340: ):\n341:     \"\"\"\n342:     Upload endpoint at root /documents for frontend compatibility.\n343:     Delegates to the main upload handler.\n344:     \"\"\"\n345:     return await _do_upload(request, file, settings_dep, vector_store, embedding_service, db_pool, vault_id)\n346: \n347: \n348: @router.post(\"/upload\", response_model=UploadResponse)\n349: async def upload_document(\n350:     request: Request,\n351:     file: Optional[UploadFile] = None,\n352:     vault_id: int = Query(1, description=\"Target vault ID\"),\n353:     settings_dep: Settings = Depends(get_settings),\n354:     vector_store: VectorStore = Depends(get_vector_store),\n355:     embedding_service: EmbeddingService = Depends(get_embedding_service),\n356:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n357:     auth: dict = Depends(require_auth),\n358: ):\n359:     \"\"\"\n360:     Upload a file and process it with strict security controls.\n361: \n362:     Validates filename, extension, and file size before saving.\n363:     Saves the uploaded file to settings.uploads_dir using aiofiles,\n364:     then processes it via DocumentProcessor.process_file in asyncio.to_thread.\n365:     \"\"\"\n366:     return await _do_upload(request, file, settings_dep, vector_store, embedding_service, db_pool, vault_id)\n367: \n368: \n369: async def _do_upload(\n370:     request: Request,\n371:     file: Optional[UploadFile],\n372:     settings_dep: Settings,\n373:     vector_store: VectorStore,\n374:     embedding_service: EmbeddingService,\n375:     db_pool: SQLiteConnectionPool,\n376:     vault_id: int,\n377: ) -> UploadResponse:\n378:     # Validate file is provided\n379:     if file is None:\n380:         raise HTTPException(status_code=400, detail=\"No file provided\")\n381: \n382:     # Validate filename is not empty\n383:     if not file.filename:\n384:         raise HTTPException(status_code=400, detail=\"Filename cannot be empty\")\n385: \n386:     # Ensure uploads directory exists\n387:     uploads_dir = settings_dep.uploads_dir\n388:     uploads_dir.mkdir(parents=True, exist_ok=True)\n389:     \n390:     # Sanitize filename\n391:     file_name = secure_filename(file.filename or \"unnamed_file\")\n392:     if not file_name:\n393:         file_name = \"unnamed_file.txt\"\n394: \n395:     # Ensure file has an extension for validation\n396:     if not Path(file_name).suffix:\n397:         file_name = f\"{file_name}.txt\"\n398:     \n399:     # Validate file extension\n400:     file_suffix = Path(file_name).suffix.lower()\n401:     if file_suffix not in settings_dep.allowed_extensions:\n402:         raise HTTPException(\n403:             status_code=400,\n404:             detail=f\"File extension '{file_suffix}' not allowed. Allowed: {settings_dep.allowed_extensions}\"\n405:         )\n406: \n407:     # Validate file size from content-length header\n408:     max_size_bytes = settings_dep.max_file_size_mb * 1024 * 1024\n409:     content_length = request.headers.get(\"content-length\")\n410:     if content_length:\n411:         try:\n412:             if int(content_length) > max_size_bytes:\n413:                 raise HTTPException(status_code=413, detail=f\"File too large. Max size: {settings.max_file_size_mb}MB\")\n414:         except ValueError:\n415:             pass  # Invalid content-length header, will check during streaming\n416:     \n417:     # Generate safe file path\n418:     file_path = uploads_dir / file_name\n419:     \n420:     # Handle duplicate file names\n421:     counter = 1\n422:     original_path = file_path\n423:     while file_path.exists():\n424:         stem = original_path.stem\n425:         suffix = original_path.suffix\n426:         file_path = uploads_dir / f\"{stem}_{counter}{suffix}\"\n427:         counter += 1\n428:     \n429:     # Path safety: ensure file_path is within uploads_dir\n430:     try:\n431:         resolved_path = file_path.resolve()\n432:         resolved_uploads_dir = uploads_dir.resolve()\n433:         if not str(resolved_path).startswith(str(resolved_uploads_dir)):\n434:             raise HTTPException(status_code=400, detail=\"Invalid file path\")\n435:     except (OSError, ValueError):\n436:         raise HTTPException(status_code=400, detail=\"Invalid file path\")\n437:     \n438:     temp_file_path = None\n439:     try:\n440:         # Save file using aiofiles with chunked reading and size validation\n441:         total_bytes = 0\n442:         temp_file_path = file_path\n443:         async with aiofiles.open(temp_file_path, \"wb\") as f:\n444:             while chunk := await file.read(1024 * 1024):  # Read 1MB chunks\n445:                 total_bytes += len(chunk)\n446:                 if total_bytes > max_size_bytes:\n447:                     # Close and delete partial file\n448:                     await f.close()\n449:                     if temp_file_path.exists():\n450:                         temp_file_path.unlink(missing_ok=True)\n451:                     raise HTTPException(status_code=413, detail=f\"File too large. Max size: {settings.max_file_size_mb}MB\")\n452:                 await f.write(chunk)\n453:         \n454:         # Process file with injected dependencies\n455:         processor = DocumentProcessor(\n456:             chunk_size_chars=settings_dep.chunk_size_chars,\n457:             chunk_overlap_chars=settings_dep.chunk_overlap_chars,\n458:             vector_store=vector_store,\n459:             embedding_service=embedding_service,\n460:             pool=db_pool,\n461:         )\n462: \n463:         try:\n464:             result = await processor.process_file(str(file_path), vault_id=vault_id)\n465:             \n466:             return UploadResponse(\n467:                 file_id=result.file_id,\n468:                 file_name=file_name,\n469:                 id=result.file_id,  # Frontend alias\n470:                 filename=file_name,  # Frontend alias\n471:                 status=\"indexed\",\n472:                 message=f\"File '{file_name}' uploaded and processed successfully with {len(result.chunks)} chunks\",\n473:             )\n474:         except DuplicateFileError as e:\n475:             # File is a duplicate, remove the uploaded file\n476:             file_path.unlink(missing_ok=True)\n477:             raise HTTPException(status_code=409, detail=f\"{e} (uploaded file was cleaned up)\")\n478:         except HTTPException:\n479:             # Clean up partial file if it exists\n480:             if temp_file_path and temp_file_path.exists():\n481:                 temp_file_path.unlink(missing_ok=True)\n482:             raise\n483:         except DocumentProcessingError as e:\n484:             logger.exception(\"Document processing error for file: %s\", file_name)\n485:             raise HTTPException(status_code=500, detail=f\"Processing error: {e}\")\n486:         except Exception as e:\n487:             logger.exception(\"Unexpected error processing file: %s\", file_name)\n488:             raise HTTPException(status_code=500, detail=f\"Server error: {e}\")\n489:     except Exception as e:\n490:         logger.exception(\"Error uploading file: %s\", file_name)\n491:         # Clean up file if it was created\n492:         if temp_file_path and temp_file_path.exists():\n493:             temp_file_path.unlink(missing_ok=True)\n494:         raise HTTPException(status_code=500, detail=f\"Upload failed: {e}\")\n495: \n496: \n497: @router.post(\"/scan\", response_model=ScanResponse)\n498: async def scan_directories(\n499:     request: Request,\n500:     background_processor: BackgroundProcessor = Depends(get_background_processor),\n501:     db_pool: SQLiteConnectionPool = Depends(get_db_pool),\n502:     auth: dict = Depends(require_auth),\n503: ):\n504:     \"\"\"\n505:     Trigger a scan of configured directories for new files.\n506: \n507:     Calls FileWatcher.scan_once() to find and enqueue new files\n508:     from uploads_dir and library_dir that are not in the database.\n509: \n510:     Uses the singleton BackgroundProcessor that runs continuously in the background.\n511:     \"\"\"\n512:     from app.services.file_watcher import FileWatcher\n513: \n514:     # Ensure processor is running (it should be from lifespan, but double-check)\n515:     if not background_processor.is_running:\n516:         await background_processor.start()\n517: \n518:     try:\n519:         watcher = FileWatcher(background_processor, pool=db_pool)\n520: \n521:         # Perform scan\n522:         files_enqueued = await watcher.scan_once()\n523: \n524:         if files_enqueued > 0:\n525:             message = f\"Scan complete: {files_enqueued} new files enqueued for processing\"\n526:         else:\n527:             message = \"Scan complete: no new files found\"\n528: \n529:         return ScanResponse(\n530:             files_enqueued=files_enqueued,\n531:             status=\"success\",\n532:             message=message,\n533:             added=files_enqueued,  # Frontend alias\n534:             scanned=files_enqueued,  # Frontend expects this (at least files_enqueued)\n535:             errors=[],  # Frontend expects this field\n536:         )\n537:     except Exception as e:\n538:         logger.exception(\"Error during directory scan\")\n539:         raise HTTPException(status_code=500, detail=f\"Scan failed: {e}\")\n540:     # Note: No finally block to stop processor - it runs continuously\n541: \n542: \n543: @router.delete(\"/{file_id}\", response_model=DeleteResponse)\n544: async def delete_document(\n545:     file_id: int,\n546:     request: Request,\n547:     conn: sqlite3.Connection = Depends(get_db),\n548:     auth: dict = Depends(require_auth),\n549:     vector_store: VectorStore = Depends(get_vector_store),\n550: ):\n551:     \"\"\"\n552:     Delete a document by ID.\n553: \n554:     Deletes the file record from the database and removes all associated\n555:     chunks from the vector store. Returns 404 if the file is not found.\n556:     \"\"\"\n557:     # Check if file exists\n558:     cursor = await asyncio.to_thread(conn.execute, \"SELECT id, file_name FROM files WHERE id = ?\", (file_id,))\n559:     row = await asyncio.to_thread(cursor.fetchone)\n560:     \n561:     if row is None:\n562:         raise HTTPException(status_code=404, detail=f\"Document with id {file_id} not found\")\n563:     \n564:     file_name = row[\"file_name\"]\n565: \n566:     try:\n567:         # Delete from vector store first (wrapped in to_thread to avoid blocking)\n568:         try:\n569:             db = vector_store.db\n570:             if db is not None and \"chunks\" in db.table_names():\n571:                 vector_store.table = db.open_table(\"chunks\")\n572:                 deleted_chunks = await asyncio.to_thread(vector_store.delete_by_file, str(file_id))\n573:                 logger.info(\"Deleted %d chunks from vector store for file_id %s\", deleted_chunks, file_id)\n574:             else:\n575:                 logger.debug(\"Chunks table not found, skipping vector store deletion for file_id %s\", file_id)\n576:         except Exception as e:\n577:             logger.warning(\"Error deleting chunks from vector store: %s\", e)\n578:             # Continue with database deletion even if vector store fails\n579: \n580:         # Delete from database\n581:         await asyncio.to_thread(conn.execute, \"DELETE FROM files WHERE id = ?\", (file_id,))\n582:         await asyncio.to_thread(conn.commit)\n583:         \n584:         return DeleteResponse(\n585:             file_id=file_id,\n586:             status=\"success\",\n587:             message=f\"Document '{file_name}' (id: {file_id}) deleted successfully\",\n588:         )\n589:     except HTTPException:\n590:         raise\n591:     except Exception as e:\n592:         await asyncio.to_thread(conn.rollback)\n593:         logger.exception(\"Error deleting document %d\", file_id)\n594:         raise HTTPException(status_code=500, detail=f\"Delete failed: {e}\")\n595: \n596: \n597: # Exception handler for validation errors (e.g., empty filename)\n598: # This is registered at the app level in main.py\n599: async def validation_exception_handler(request: Request, exc: RequestValidationError):\n600:     \"\"\"Convert validation errors to 400 for empty filename cases only.\"\"\"\n601:     errors = exc.errors()\n602:     for error in errors:\n603:         if error.get(\"loc\") == (\"body\", \"file\") and \"filename\" in str(error.get(\"input\", \"\")).lower():\n604:             raise HTTPException(status_code=400, detail=\"Filename cannot be empty\")\n605:     # For all other validation errors, return standard 422\n606:     # Convert errors to dict format for JSON serialization\n607:     from fastapi.responses import JSONResponse\n608:     error_dicts = [\n609:         {\n610:             \"loc\": error.get(\"loc\"),\n611:             \"msg\": error.get(\"msg\"),\n612:             \"type\": error.get(\"type\"),\n613:             \"input\": error.get(\"input\")\n614:         }\n615:         for error in errors\n616:     ]\n617:     return JSONResponse(status_code=422, content={\"detail\": error_dicts})\n618: \n\n(End of file - total 618 lines)\n</content>","timestamp":1771904154494,"originalBytes":26398}