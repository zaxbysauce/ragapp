{"id":"S488","summaryText":"[SUMMARY S488] 25.4 KB | code | 642 lines\n\"\"\"\nDual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n\"\"\"\nimport asyncio\nimport httpx\nâ†’ Use /swarm retrieve S488 for full content","fullOutput":"\"\"\"\nDual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n\"\"\"\nimport asyncio\nimport httpx\nimport logging\nfrom typing import List\nfrom urllib.parse import urlparse\nfrom app.config import settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingError(Exception):\n    \"\"\"Exception raised for embedding service errors.\"\"\"\n    pass\n\n\nclass EmbeddingService:\n    \"\"\"Service for generating text embeddings via Ollama or OpenAI-compatible APIs.\"\"\"\n    \n    # Hard caps for input validation\n    MAX_BATCH_SIZE = 512  # Maximum number of texts per batch call\n    MAX_TEXT_LENGTH = 8192  # Maximum characters per text (derived from chunk_size_chars=8192)\n    MIN_SPLIT_CHARS = 100  # Minimum text length to attempt single-text splitting\n\n    def __init__(self):\n        \"\"\"Initialize the embedding service with HTTP client and provider detection.\"\"\"\n        base_url = settings.ollama_embedding_url\n\n        # Validate base_url\n        if not base_url:\n            raise EmbeddingError(\"Embedding service is not configured\")\n        if not base_url.startswith(('http://', 'https://')):\n            raise EmbeddingError(\"Invalid embedding URL configuration\")\n\n        # Detect provider mode based on URL path\n        self.provider_mode, self.embeddings_url = self._detect_provider_mode(base_url)\n        self.timeout = 60.0\n        \n        # Read embedding prefixes from settings\n        self.embedding_doc_prefix = settings.embedding_doc_prefix\n        self.embedding_query_prefix = settings.embedding_query_prefix\n        \n        # Auto-apply Qwen3 instruction prefixes for better retrieval quality\n        # With llama.cpp -ub 8192, we have plenty of headroom for these prefixes\n        if settings.embedding_model.lower().find(\"qwen\") >= 0:\n            if not self.embedding_doc_prefix:\n                self.embedding_doc_prefix = \"Instruct: Represent this technical documentation passage for retrieval.\\nDocument: \"\n            if not self.embedding_query_prefix:\n                self.embedding_query_prefix = \"Instruct: Retrieve relevant technical documentation passages.\\nQuery: \"\n    \n    def _detect_provider_mode(self, base_url: str) -> tuple:\n        \"\"\"\n        Detect which embedding provider mode to use based on URL path.\n        \n        Detection strategy:\n        - If URL path includes '/api/embeddings' -> Ollama mode\n        - If URL path includes '/v1/embeddings' -> OpenAI mode\n        - If no explicit embeddings path:\n          - Port 1234 -> OpenAI mode (LM Studio default)\n          - Otherwise -> Ollama mode\n        \n        Args:\n            base_url: The configured embedding URL\n            \n        Returns:\n            Tuple of (provider_mode, embeddings_url)\n        \"\"\"\n        parsed = urlparse(base_url)\n        path = parsed.path\n        \n        # Check for explicit paths\n        if '/api/embeddings' in path:\n            # Already has Ollama path, use as-is\n            return ('ollama', base_url)\n        elif '/v1/embeddings' in path:\n            # Already has OpenAI path, use as-is\n            return ('openai', base_url)\n        \n        # No explicit path - determine by port\n        port = parsed.port\n        if port == 1234:\n            # LM Studio default port - use OpenAI mode\n            base_url = base_url.rstrip('/') + '/v1/embeddings'\n            return ('openai', base_url)\n        else:\n            # Default to Ollama mode\n            base_url = base_url.rstrip('/') + '/api/embeddings'\n            return ('ollama', base_url)\n    \n    def _build_payload(self, text: str) -> dict:\n        \"\"\"\n        Build the API request payload based on provider mode.\n        \n        Args:\n            text: The text to embed\n            \n        Returns:\n            Dictionary payload for the API request\n        \"\"\"\n        if self.provider_mode == 'openai':\n            return {\n                \"model\": settings.embedding_model,\n                \"input\": text\n            }\n        else:  # ollama mode\n            return {\n                \"model\": settings.embedding_model,\n                \"prompt\": text\n            }\n    \n    def _extract_embedding(self, data: dict) -> List[float]:\n        \"\"\"\n        Extract embedding vector from API response based on provider mode.\n        \n        Args:\n            data: Parsed JSON response from the API\n            \n        Returns:\n            List of float values representing the embedding vector\n            \n        Raises:\n            EmbeddingError: If embedding cannot be extracted\n        \"\"\"\n        if self.provider_mode == 'openai':\n            # OpenAI format: data[0].embedding\n            if \"data\" not in data:\n                logger.error(\"Embedding API response missing 'data' field in OpenAI mode\")\n                raise EmbeddingError(\"Embedding API response is invalid\")\n            if not isinstance(data[\"data\"], list) or len(data[\"data\"]) == 0:\n                logger.error(\"Embedding API response 'data' field is empty or invalid in OpenAI mode\")\n                raise EmbeddingError(\"Embedding API response is invalid\")\n            embedding = data[\"data\"][0].get(\"embedding\")\n            if embedding is None:\n                logger.error(\"Embedding API response missing 'data[0].embedding' field in OpenAI mode\")\n                raise EmbeddingError(\"Embedding API response is invalid\")\n        else:  # ollama mode\n            # Ollama format: embedding\n            embedding = data.get(\"embedding\")\n            if embedding is None:\n                logger.error(\"Embedding API response missing 'embedding' field in Ollama mode\")\n                raise EmbeddingError(\"Embedding API response is invalid\")\n        \n        return embedding\n    \n    async def embed_single(self, text: str) -> List[float]:\n        \"\"\"\n        Generate embedding for a single text.\n\n        Applies the query prefix (if configured) to the input text before embedding.\n        The query prefix is used for retrieval queries and must remain constant for\n        a given index to ensure consistent embedding space.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            List of float values representing the embedding vector.\n\n        Raises:\n            EmbeddingError: If the API request fails or returns non-200 status.\n        \"\"\"\n        # Validate text input\n        if text is None:\n            raise EmbeddingError(\"Text cannot be None\")\n        if not text.strip():\n            raise EmbeddingError(\"Text cannot be empty or whitespace only\")\n\n        # Apply query prefix for retrieval queries\n        text_to_embed = self.embedding_query_prefix + text if self.embedding_query_prefix else text\n\n        async with httpx.AsyncClient(timeout=self.timeout) as client:\n            try:\n                response = await client.post(\n                    self.embeddings_url,\n                    json=self._build_payload(text_to_embed)\n                )\n                \n                if response.status_code != 200:\n                    logger.warning(\n                        f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n                    )\n                    raise EmbeddingError(\n                        f\"Embedding API returned status {response.status_code}\"\n                    )\n\n                try:\n                    data = response.json()\n                except ValueError as e:\n                    logger.warning(\n                        f\"Invalid JSON response from embedding API for {self.provider_mode} mode: {e}, response: {response.text}\"\n                    )\n                    raise EmbeddingError(\"Invalid response from embedding service\")\n\n                return self._extract_embedding(data)\n                \n            except httpx.TimeoutException as e:\n                raise EmbeddingError(\"Embedding request timed out\")\n            except httpx.HTTPError as e:\n                raise EmbeddingError(\"Embedding HTTP error occurred\")\n    \n    async def validate_embedding_dimension(self, expected_dim: int) -> bool:\n        \"\"\"\n        Validate that the embedding dimension matches the expected value.\n\n        Args:\n            expected_dim: The expected dimension of the embedding vector.\n                Must be a positive integer.\n\n        Returns:\n            True if the dimension matches.\n\n        Raises:\n            EmbeddingError: If expected_dim is invalid or if the dimension\n                does not match the expected value.\n        \"\"\"\n        # Validate expected_dim input\n        if expected_dim is None:\n            raise EmbeddingError(\"expected_dim cannot be None\")\n        if not isinstance(expected_dim, int) or expected_dim <= 0:\n            raise EmbeddingError(f\"expected_dim must be a positive integer, got {expected_dim}\")\n\n        embedding = await self.embed_single('dimension_check')\n        actual_dim = len(embedding)\n        if actual_dim != expected_dim:\n            raise EmbeddingError(\n                f\"Embedding dimension mismatch: expected {expected_dim}, got {actual_dim}\"\n            )\n        return True\n\n    async def embed_batch(self, texts: List[str], batch_size: int | None = None) -> List[List[float]]:\n        \"\"\"\n        Generate embeddings for a batch of texts using true API batching.\n\n        Sends multiple texts per API request for efficient GPU utilization.\n        Processes in batches of up to 512 (configurable) with up to 4\n        concurrent batch requests.\n\n        Applies the document prefix (if configured) to each input text before embedding.\n        The document prefix is used for document embeddings and must remain constant for\n        a given index to ensure consistent embedding space.\n\n        Args:\n            texts: List of texts to embed.\n            batch_size: Number of texts per API request (default: 512).\n\n        Returns:\n            List of embedding vectors, one for each input text, in order.\n\n        Raises:\n            EmbeddingError: If any API request fails.\n        \"\"\"\n        if not texts:\n            return []\n        \n        # Input validation guards\n        for idx, text in enumerate(texts):\n            if text is None:\n                raise EmbeddingError(f\"Text at index {idx} is None\")\n            if not text.strip():\n                raise EmbeddingError(f\"Text at index {idx} is empty or whitespace only\")\n            if len(text) > self.MAX_TEXT_LENGTH:\n                raise EmbeddingError(\n                    f\"Text at index {idx} exceeds maximum length ({self.MAX_TEXT_LENGTH} characters)\"\n                )\n        \n        # Use configured batch size if not specified\n        if batch_size is None:\n            batch_size = settings.embedding_batch_size\n        \n        # Clamp batch_size to valid range\n        batch_size = max(1, min(batch_size, self.MAX_BATCH_SIZE))\n        \n        # Apply document prefix to all texts\n        texts_to_embed = []\n        for text in texts:\n            if self.embedding_doc_prefix:\n                texts_to_embed.append(self.embedding_doc_prefix + text)\n            else:\n                texts_to_embed.append(text)\n        \n        # Process in batches using true API batching\n        all_embeddings: List[List[float]] = []\n        for i in range(0, len(texts_to_embed), batch_size):\n            batch = texts_to_embed[i:i + batch_size]\n            embeddings = await self._embed_batch_api(batch)\n            all_embeddings.extend(embeddings)\n        \n        return all_embeddings\n\n    async def _embed_batch_api(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"\n        Send a batch of texts to the embedding API in a single request.\n        \n        Implements adaptive batching: automatically retries with smaller sub-batches\n        when llama.cpp token overflow errors occur.\n\n        Args:\n            texts: List of texts to embed (already prefixed).\n\n        Returns:\n            List of embedding vectors in the same order as input texts.\n\n        Raises:\n            EmbeddingError: If the API request fails after all retries.\n        \"\"\"\n        max_retries = settings.embedding_batch_max_retries\n        min_sub_size = settings.embedding_batch_min_sub_size\n        \n        async with httpx.AsyncClient(timeout=self.timeout) as client:\n            return await self._embed_batch_with_retry(client, texts, max_retries, min_sub_size)\n    \n    async def _embed_batch_with_retry(self, client: httpx.AsyncClient, texts: List[str], \n                                      max_retries: int, min_sub_size: int, retry_count: int = 0) -> List[List[float]]:\n        \"\"\"\n        Internal method that handles the retry logic for adaptive batching.\n        \n        Args:\n            client: HTTP client for making requests\n            texts: List of texts to embed\n            max_retries: Maximum number of retry attempts\n            min_sub_size: Minimum sub-batch size before giving up\n            \n        Returns:\n            List of embedding vectors\n            \n        Raises:\n            EmbeddingError: If all retries fail\n        \"\"\"\n        # Empty-input guard\n        if not texts:\n            return []\n        \n        try:\n            # Build payload with array of inputs\n            if self.provider_mode == 'openai':\n                payload = {\n                    \"model\": settings.embedding_model,\n                    \"input\": texts\n                }\n            else:  # ollama mode\n                payload = {\n                    \"model\": settings.embedding_model,\n                    \"input\": texts\n                }\n            \n            response = await client.post(\n                self.embeddings_url,\n                json=payload\n            )\n            \n            # Check for token overflow error in HTTP 500 responses\n            if response.status_code == 500:\n                error_text = response.text.lower()\n                if self._is_token_overflow_error(error_text):\n                    logger.warning(\n                        f\"Token overflow error for {self.provider_mode} mode: {response.text}\"\n                    )\n                    # Handle overflow using the shared helper\n                    return await self._handle_overflow_retry(\n                        client, texts, max_retries, min_sub_size, retry_count\n                    )\n            \n            if response.status_code != 200:\n                logger.warning(\n                    f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n                )\n                raise EmbeddingError(\n                    f\"Embedding API returned status {response.status_code}\"\n                )\n\n            data = response.json()\n            \n            # Extract embeddings from response\n            if self.provider_mode == 'openai':\n                # OpenAI format: data[].embedding\n                embeddings = [item['embedding'] for item in data['data']]\n            else:\n                # Ollama format may vary - try common formats\n                if 'embeddings' in data:\n                    embeddings = data['embeddings']\n                elif 'embedding' in data:\n                    # Single embedding returned - shouldn't happen with batch\n                    embeddings = [data['embedding']]\n                else:\n                    logger.error(\n                        f\"Unexpected response format for {self.provider_mode} mode: {data.keys()}\"\n                    )\n                    raise EmbeddingError(\"Unexpected response from embedding service\")\n            \n            # Validate embedding structure\n            if not isinstance(embeddings, list):\n                logger.error(\"Embedding API response 'embeddings' is not a list\")\n                raise EmbeddingError(\"Embedding API response is invalid\")\n            for i, emb in enumerate(embeddings):\n                if not isinstance(emb, list):\n                    logger.error(f\"Embedding at index {i} is not a list\")\n                    raise EmbeddingError(\"Embedding API response is invalid\")\n                for j, val in enumerate(emb):\n                    if not isinstance(val, (int, float)):\n                        logger.error(f\"Embedding value at [{i}][{j}] is not a number\")\n                        raise EmbeddingError(\"Embedding API response is invalid\")\n            \n            # Validate embedding count matches input count\n            if len(embeddings) != len(texts):\n                logger.error(\n                    f\"Embedding count mismatch for {self.provider_mode} mode: expected {len(texts)}, got {len(embeddings)}\"\n                )\n                raise EmbeddingError(\n                    f\"Embedding count mismatch: expected {len(texts)}, got {len(embeddings)}\"\n                )\n            \n            return embeddings\n            \n        except httpx.TimeoutException as e:\n            logger.warning(\n                f\"Embedding batch request timed out for {self.provider_mode} mode: {e}\"\n            )\n            raise EmbeddingError(\"Embedding batch request timed out\")\n        except httpx.HTTPError as e:\n            # Check if this is a token overflow error\n            error_msg = str(e)\n            response_text = \"\"\n            \n            # Try to get response text from the exception if available\n            try:\n                resp = getattr(e, 'response', None)\n                if resp is not None:\n                    response_text = resp.text.lower()\n            except Exception:\n                pass\n            \n            if self._is_token_overflow_error(error_msg) or self._is_token_overflow_error(response_text):\n                logger.warning(\n                    f\"Token overflow error for {self.provider_mode} mode: {response_text}\"\n                )\n                # Handle overflow using the shared helper\n                return await self._handle_overflow_retry(\n                    client, texts, max_retries, min_sub_size, retry_count\n                )\n            else:\n                # Not a token overflow error, re-raise\n                logger.error(\n                    f\"Embedding batch HTTP error for {self.provider_mode} mode: {e}\"\n                )\n                raise EmbeddingError(\"Embedding batch HTTP error occurred\")\n\n    def _split_text_at_midpoint(self, text: str) -> tuple:\n        \"\"\"\n        Split a single text into two parts at a boundary-aware midpoint.\n        \n        Prefers splitting at newline or space characters near the midpoint\n        to produce more natural splits.\n        \n        Args:\n            text: The text to split\n            \n        Returns:\n            Tuple of (left_text, right_text)\n        \"\"\"\n        if len(text) <= 1:\n            return (text, \"\")\n        \n        midpoint = len(text) // 2\n        \n        # Try to find a better boundary near the midpoint\n        # Look for newline first, then space\n        search_start = max(0, midpoint - 50)\n        search_end = min(len(text), midpoint + 50)\n        \n        # Search for newline near midpoint\n        for i in range(midpoint, search_end):\n            if text[i] == '\\n':\n                return (text[:i+1], text[i+1:])\n        \n        for i in range(midpoint - 1, search_start - 1, -1):\n            if text[i] == '\\n':\n                return (text[:i+1], text[i+1:])\n        \n        # Search for space near midpoint\n        for i in range(midpoint, search_end):\n            if text[i] == ' ':\n                return (text[:i], text[i:])\n        \n        for i in range(midpoint - 1, search_start - 1, -1):\n            if text[i] == ' ':\n                return (text[:i], text[i:])\n        \n        # Fall back to strict midpoint\n        return (text[:midpoint], text[midpoint:])\n    \n    def _mean_pool_embeddings(self, emb1: List[float], emb2: List[float]) -> List[float]:\n        \"\"\"\n        Mean-pool two embedding vectors into one.\n        \n        Args:\n            emb1: First embedding vector\n            emb2: Second embedding vector\n            \n        Returns:\n            Mean-pooled embedding vector\n        \"\"\"\n        if len(emb1) != len(emb2):\n            raise EmbeddingError(\n                f\"Cannot mean-pool embeddings of different dimensions: {len(emb1)} vs {len(emb2)}\"\n            )\n        \n        return [(a + b) / 2.0 for a, b in zip(emb1, emb2)]\n    \n    async def _handle_overflow_retry(self, client: httpx.AsyncClient, texts: List[str],\n                                     max_retries: int, min_sub_size: int, retry_count: int) -> List[List[float]]:\n        \"\"\"\n        Helper method to handle overflow retry logic with bounded retries and minimum split size.\n        \n        For single-item overflow, attempts to split the text and mean-pool the results.\n        For multi-item overflow, splits the batch and processes each half.\n        \n        Args:\n            client: HTTP client for making requests\n            texts: List of texts to embed\n            max_retries: Maximum number of retry attempts\n            min_sub_size: Minimum sub-batch size before giving up\n            retry_count: Current retry attempt count\n            \n        Returns:\n            List of embedding vectors\n            \n        Raises:\n            EmbeddingError: If bounded retries exhausted or split size too small\n        \"\"\"\n        # Check if we've exhausted retries\n        if retry_count > max_retries:\n            logger.error(\n                f\"Max retries ({max_retries}) exhausted for embedding batch in {self.provider_mode} mode\"\n            )\n            raise EmbeddingError(\n                f\"Max retries ({max_retries}) exhausted for embedding batch\"\n            )\n        \n        # Handle single-item overflow with text splitting\n        if len(texts) == 1:\n            single_text = texts[0]\n            \n            # Check if text is too short to split - raise actionable error\n            if len(single_text) < self.MIN_SPLIT_CHARS:\n                logger.warning(\n                    f\"Single input ({len(single_text)} chars) is below minimum split threshold ({self.MIN_SPLIT_CHARS}) in {self.provider_mode} mode\"\n                )\n                raise EmbeddingError(\n                    f\"Single input ({len(single_text)} chars) exceeds token limit and is too short to split. \"\n                    f\"Ensure chunk_size_chars is below server batch size limit (minimum {self.MIN_SPLIT_CHARS} chars required for recovery).\"\n                )\n            \n            # Split text at boundary-aware midpoint and recurse\n            left_text, right_text = self._split_text_at_midpoint(single_text)\n            \n            logger.info(\n                f\"Splitting single input ({len(single_text)} chars) into parts ({len(left_text)} + {len(right_text)} chars), retry {retry_count}\"\n            )\n            \n            # Small bounded async backoff\n            backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n            await asyncio.sleep(backoff_delay)\n            \n            # Recurse on each part with incremented retry count\n            left_embeddings = await self._embed_batch_with_retry(\n                client, [left_text], max_retries, min_sub_size, retry_count=retry_count + 1\n            )\n            right_embeddings = await self._embed_batch_with_retry(\n                client, [right_text], max_retries, min_sub_size, retry_count=retry_count + 1\n            )\n            \n            # Mean-pool the two embeddings into one\n            pooled = self._mean_pool_embeddings(left_embeddings[0], right_embeddings[0])\n            \n            # Return single embedding to preserve one-embedding-per-input contract\n            return [pooled]\n        \n        # Multi-item batch overflow - use existing split behavior\n        # Check if we've reached minimum split size\n        if len(texts) <= min_sub_size:\n            logger.warning(\n                f\"Cannot split batch further in {self.provider_mode} mode: {len(texts)} items below minimum split size ({min_sub_size})\"\n            )\n            raise EmbeddingError(\n                f\"Cannot split batch further: {len(texts)} items below minimum split size\"\n            )\n        \n        # Split at midpoint and recurse with backoff\n        midpoint = len(texts) // 2\n        left_texts = texts[:midpoint]\n        right_texts = texts[midpoint:]\n        \n        # Small bounded async backoff (exponential, capped at 1s)\n        backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n        await asyncio.sleep(backoff_delay)\n        \n        # Recurse on left then right to preserve order\n        left_embeddings = await self._embed_batch_with_retry(\n            client, left_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n        )\n        right_embeddings = await self._embed_batch_with_retry(\n            client, right_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n        )\n        \n        return left_embeddings + right_embeddings\n    \n    def _is_token_overflow_error(self, error_msg: str) -> bool:\n        \"\"\"\n        Detect if an error message indicates a token overflow from llama.cpp.\n        \n        Args:\n            error_msg: The error message string\n            \n        Returns:\n            True if this is a token overflow error, False otherwise\n        \"\"\"\n        error_lower = error_msg.lower()\n        \n        # Check for common llama.cpp token overflow patterns\n        # Pattern 1: \"input (X tokens) is too large\" - typical llama.cpp error\n        if \"input (\" in error_lower and \"tokens) is too large\" in error_lower:\n            return True\n        \n        # Pattern 2: \"too large to process\" with \"current batch size\" - OpenAI mode error\n        if \"too large to process\" in error_lower and \"current batch size\" in error_lower:\n            return True\n        \n        # Pattern 3: \"token limit exceeded\"\n        if \"token limit exceeded\" in error_lower:\n            return True\n        \n        # Pattern 4: \"batch size too small\"\n        if \"batch size too small\" in error_lower:\n            return True\n        \n        return False\n\n\n\n","timestamp":1771937042738,"originalBytes":26056}