{"id":"S41","summaryText":"[SUMMARY S41] 26.3 KB | code | 697 lines\n\"\"\"\nLanceDB vector store service for semantic search.\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\n→ Use /swarm retrieve S41 for full content","fullOutput":"\"\"\"\nLanceDB vector store service for semantic search.\n\"\"\"\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional\nimport lancedb\nimport pyarrow as pa\nimport numpy as np\nimport logging\n\nfrom app.config import settings\n\nlogger = logging.getLogger(__name__)\n\nlogger = logging.getLogger(__name__)\n\n\nclass VectorStoreError(Exception):\n    \"\"\"Custom exception for vector store errors.\"\"\"\n    pass\n\n\nclass VectorStoreConnectionError(VectorStoreError):\n    \"\"\"Exception raised when connection to LanceDB fails.\"\"\"\n    pass\n\n\nclass VectorStoreValidationError(VectorStoreError):\n    \"\"\"Exception raised when record validation fails.\"\"\"\n    pass\n\n\nclass VectorStore:\n    \"\"\"LanceDB-based vector store for document chunk embeddings.\"\"\"\n    \n    def __init__(self, db_path: Optional[Path] = None):\n        \"\"\"\n        Initialize the vector store.\n        \n        Args:\n            db_path: Path to LanceDB database. Defaults to settings.lancedb_path.\n        \"\"\"\n        self.db_path = db_path or settings.lancedb_path\n        self.db: Optional[lancedb.DBConnection] = None\n        self.table: Optional[lancedb.table.Table] = None\n        self._embedding_dim: Optional[int] = None\n    \n    def connect(self) -> \"VectorStore\":\n        \"\"\"Connect to LanceDB.\n        \n        Raises:\n            VectorStoreConnectionError: If connection to LanceDB fails.\n        \"\"\"\n        try:\n            self.db = lancedb.connect(str(self.db_path))\n        except Exception as e:\n            raise VectorStoreConnectionError(f\"Failed to connect to LanceDB at {self.db_path}: {e}\") from e\n        return self\n    \n    def init_table(self, embedding_dim: int) -> \"VectorStore\":\n        \"\"\"\n        Initialize or open the 'chunks' table.\n        \n        Args:\n            embedding_dim: Dimension of embedding vectors.\n            \n        Returns:\n            Self for method chaining.\n            \n        Raises:\n            VectorStoreConnectionError: If connection or table operations fail.\n        \"\"\"\n        if self.db is None:\n            self.connect()\n        \n        if self.db is None:\n            raise VectorStoreConnectionError(\"Database connection is not available.\")\n        \n        self._embedding_dim = embedding_dim\n        \n        # Define schema for chunks table\n        schema = pa.schema([\n            (\"id\", pa.string()),\n            (\"text\", pa.string()),\n            (\"file_id\", pa.string()),\n            (\"vault_id\", pa.string()),  # Vault isolation\n            (\"chunk_index\", pa.int32()),\n            (\"metadata\", pa.string()),  # JSON string for flexibility\n            (\"embedding\", pa.list_(pa.float32(), embedding_dim)),\n        ])\n        \n        # Create or open table with error handling\n        try:\n            if \"chunks\" in self.db.table_names():\n                try:\n                    self.table = self.db.open_table(\"chunks\")\n                except Exception:\n                    # Stale table reference — drop and recreate\n                    try:\n                        self.db.drop_table(\"chunks\")\n                    except Exception:\n                        pass\n                    self.table = self.db.create_table(\"chunks\", schema=schema, mode=\"overwrite\")\n            else:\n                self.table = self.db.create_table(\"chunks\", schema=schema)\n        except Exception as e:\n            raise VectorStoreConnectionError(f\"Failed to initialize 'chunks' table: {e}\") from e\n        \n        return self\n    \n    def _get_expected_embedding_dim(self) -> Optional[int]:\n        \"\"\"Get the expected embedding dimension from the table schema.\"\"\"\n        if self.table is None:\n            return self._embedding_dim\n        \n        try:\n            schema = self.table.schema\n            embedding_field = schema.field(\"embedding\")\n            if hasattr(embedding_field.type, 'list_size'):\n                return embedding_field.type.list_size\n        except Exception:\n            pass\n        return self._embedding_dim\n    \n    def add_chunks(self, records: List[Dict[str, Any]]) -> None:\n        \"\"\"\n        Add chunk records to the vector store.\n        \n        Args:\n            records: List of records with keys: id, text, file_id, chunk_index, \n                     metadata, embedding, vault_id (optional, defaults to \"1\").\n                     \n        Raises:\n            RuntimeError: If table is not initialized.\n            VectorStoreValidationError: If records validation fails.\n        \"\"\"\n        if self.table is None:\n            raise RuntimeError(\"Table not initialized. Call init_table() first.\")\n        \n        # Handle empty records\n        if not records:\n            return\n        \n        # Get expected embedding dimension from table schema\n        expected_dim = self._get_expected_embedding_dim()\n        \n        # Required fields for validation\n        required_fields = [\"id\", \"text\", \"file_id\", \"chunk_index\", \"embedding\"]\n        \n        # Convert records to arrow-compatible format\n        processed_records = []\n        for record in records:\n            # Validate required fields\n            missing_fields = [field for field in required_fields if field not in record]\n            if missing_fields:\n                raise VectorStoreValidationError(\n                    f\"Record missing required fields: {', '.join(missing_fields)}\"\n                )\n            \n            # Ensure embedding is a list (convert from numpy if needed)\n            embedding = record[\"embedding\"]\n            if isinstance(embedding, np.ndarray):\n                embedding = embedding.tolist()\n            elif not isinstance(embedding, list):\n                raise VectorStoreValidationError(\n                    f\"Embedding must be a list or numpy array, got {type(embedding).__name__}\"\n                )\n            \n            # Validate embedding dimension matches table schema\n            actual_dim = len(embedding)\n            if expected_dim is not None and actual_dim != expected_dim:\n                raise VectorStoreValidationError(\n                    f\"Embedding dimension mismatch: expected {expected_dim} dimensions, \"\n                    f\"got {actual_dim}. The table was created with a different embedding model. \"\n                    f\"Delete the lancedb directory at {self.db_path} and restart to use the new model.\"\n                )\n            \n            processed_record = {\n                \"id\": record[\"id\"],\n                \"text\": record[\"text\"],\n                \"file_id\": record[\"file_id\"],\n                \"vault_id\": record.get(\"vault_id\", \"1\"),  # Default to vault \"1\"\n                \"chunk_index\": record[\"chunk_index\"],\n                \"metadata\": record.get(\"metadata\", \"{}\"),\n                \"embedding\": embedding,\n            }\n            processed_records.append(processed_record)\n        \n        self.table.add(processed_records)\n    \n    def search(\n        self,\n        embedding: List[float],\n        limit: int = 10,\n        filter_expr: Optional[str] = None,\n        vault_id: Optional[str] = None\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for similar chunks by embedding.\n\n        Args:\n            embedding: Query embedding vector.\n            limit: Maximum number of results.\n            filter_expr: Optional filter expression (LanceDB syntax).\n            vault_id: Optional vault ID to filter results. If provided, only returns\n                     chunks from the specified vault.\n\n        Returns:\n            List of matching records with similarity scores. Each record includes:\n            - All original fields (id, text, file_id, chunk_index, metadata, etc.)\n            - _distance: Cosine distance from query embedding (lower = more similar)\n            Empty list if no table exists.\n            \n        Note:\n            For cosine distance metric:\n            - Distance of 0 = identical vectors (perfect match)\n            - Distance of 1 = orthogonal vectors\n            - Distance of 2 = opposite vectors (perfect mismatch)\n            The _distance field is provided by LanceDB's vector search.\n        \"\"\"\n        # Ensure DB connection exists\n        if self.db is None:\n            self.connect()\n        \n        # Try to open existing table if not already loaded\n        if self.table is None:\n            try:\n                table_names = self.db.table_names()\n            except Exception as e:\n                raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n            \n            if \"chunks\" not in table_names:\n                # No table exists yet - graceful no-docs behavior\n                return []\n            \n            # Table exists, try to open it\n            try:\n                self.table = self.db.open_table(\"chunks\")\n            except Exception as e:\n                raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n            \n            # Set embedding_dim from table schema if available\n            if self._embedding_dim is None:\n                try:\n                    schema = self.table.schema\n                    embedding_field = schema.field(\"embedding\")\n                    # Extract dimension from fixed size list type\n                    if hasattr(embedding_field.type, 'list_size'):\n                        self._embedding_dim = embedding_field.type.list_size\n                except Exception:\n                    # If we can't determine embedding_dim, leave it as None\n                    pass\n        \n        query = self.table.search(embedding, metric=\"cosine\")\n\n        # Apply vault filter if specified\n        if vault_id is not None:\n            safe_vault_id = str(vault_id).replace(\"'\", \"\\\\'\")\n            vault_filter = f\"vault_id = '{safe_vault_id}'\"\n            if filter_expr:\n                filter_expr = f\"({filter_expr}) AND ({vault_filter})\"\n            else:\n                filter_expr = vault_filter\n\n        if filter_expr:\n            query = query.where(filter_expr)\n        \n        results = query.limit(limit).to_list()\n        return results\n    \n    def delete_by_file(self, file_id: str) -> int:\n        \"\"\"\n        Delete all chunks for a given file_id.\n        \n        Args:\n            file_id: The file ID to delete chunks for.\n            \n        Returns:\n            Number of records deleted.\n        \"\"\"\n        # Ensure DB connection exists\n        if self.db is None:\n            self.connect()\n        \n        # Try to open existing table if not already loaded\n        if self.table is None:\n            try:\n                table_names = self.db.table_names()\n            except Exception as e:\n                raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n            \n            if \"chunks\" not in table_names:\n                # No table exists yet - nothing to delete\n                return 0\n            \n            # Table exists, try to open it\n            try:\n                self.table = self.db.open_table(\"chunks\")\n            except Exception as e:\n                raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n            \n            # Set embedding_dim from table schema if available\n            if self._embedding_dim is None:\n                try:\n                    schema = self.table.schema\n                    embedding_field = schema.field(\"embedding\")\n                    # Extract dimension from fixed size list type\n                    if hasattr(embedding_field.type, 'list_size'):\n                        self._embedding_dim = embedding_field.type.list_size\n                except Exception:\n                    # If we can't determine embedding_dim, leave it as None\n                    pass\n\n        # Query count before delete to return accurate deletion count\n        safe_file_id = str(file_id).replace('\"', '\\\\\"')\n        try:\n            count_before = self.table.count_rows(f'file_id = \"{safe_file_id}\"')\n        except Exception:\n            # If count_rows fails, safely default to 0\n            count_before = 0\n\n        # LanceDB delete using filter expression\n        self.table.delete(f'file_id = \"{safe_file_id}\"')\n\n        return count_before\n\n    def delete_by_vault(self, vault_id: str) -> int:\n        \"\"\"\n        Delete all chunks for a given vault_id.\n\n        Args:\n            vault_id: The vault ID to delete all chunks for.\n\n        Returns:\n            Number of records deleted.\n        \"\"\"\n        # Ensure DB connection exists\n        if self.db is None:\n            self.connect()\n\n        # Try to open existing table if not already loaded\n        if self.table is None:\n            try:\n                table_names = self.db.table_names()\n            except Exception as e:\n                raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n\n            if \"chunks\" not in table_names:\n                return 0\n\n            try:\n                self.table = self.db.open_table(\"chunks\")\n            except Exception as e:\n                raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n\n        safe_vault_id = str(vault_id).replace(\"'\", \"\\\\'\")\n        try:\n            count_before = self.table.count_rows(f\"vault_id = '{safe_vault_id}'\")\n        except Exception:\n            count_before = 0\n\n        self.table.delete(f\"vault_id = '{safe_vault_id}'\")\n        return count_before\n\n    def migrate_add_vault_id(self) -> int:\n        \"\"\"\n        Migration: Backfill vault_id='1' on existing chunks that lack it.\n\n        LanceDB doesn't support ALTER TABLE or UPDATE, so this reads all data,\n        adds the vault_id field, and rewrites the table. This is idempotent —\n        safe to call multiple times (no-op if all records already have vault_id).\n\n        Returns:\n            Number of records migrated. 0 if no migration was needed.\n        \"\"\"\n        if self.db is None:\n            self.connect()\n\n        if self.db is None:\n            logger.info(\"LanceDB vault_id migration: no connection available\")\n            return 0\n\n        try:\n            table_names = self.db.table_names()\n        except Exception as e:\n            logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n            return 0\n\n        if \"chunks\" not in table_names:\n            logger.info(\"LanceDB vault_id migration: no table exists\")\n            return 0\n\n        try:\n            table = self.db.open_table(\"chunks\")\n        except Exception as e:\n            logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n            return 0\n\n        # Check if vault_id column exists in schema\n        schema = table.schema\n        field_names = [schema.field(i).name for i in range(len(schema))]\n\n        if \"vault_id\" in field_names:\n            # Column exists — check if any rows have null vault_id\n            try:\n                df = table.to_pandas()\n                null_count = df[\"vault_id\"].isna().sum()\n                if null_count == 0:\n                    logger.info(\"LanceDB vault_id migration: no migration needed\")\n                    return 0  # All records already have vault_id\n\n                # Backfill null vault_ids with \"1\"\n                df[\"vault_id\"] = df[\"vault_id\"].fillna(\"1\")\n                count = int(null_count)\n\n                # Drop and recreate table with updated data\n                self.db.drop_table(\"chunks\")\n                try:\n                    self.table = self.db.create_table(\"chunks\", data=df)\n                except Exception as create_err:\n                    logger.critical(f\"LanceDB vault_id migration: table dropped but recreate failed: {create_err}. Data may need manual recovery from backup.\")\n                    raise\n                logger.info(f\"LanceDB vault_id migration: backfilled {count} records\")\n                return count\n            except Exception as e:\n                logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n                return 0\n        else:\n            # Column doesn't exist — add it to all records\n            try:\n                df = table.to_pandas()\n                if len(df) == 0:\n                    # Empty table — just drop and recreate with new schema\n                    # Try to get embedding_dim from existing schema before dropping\n                    if self._embedding_dim is None:\n                        try:\n                            embedding_field = table.schema.field(\"embedding\")\n                            if hasattr(embedding_field.type, 'list_size'):\n                                self._embedding_dim = embedding_field.type.list_size\n                        except Exception:\n                            pass\n\n                    self.db.drop_table(\"chunks\")\n                    try:\n                        if self._embedding_dim:\n                            self.init_table(self._embedding_dim)\n                    except Exception as create_err:\n                        logger.critical(f\"LanceDB vault_id migration: empty table dropped but recreate failed: {create_err}\")\n                        raise\n                    logger.info(\"LanceDB vault_id migration: empty table, recreated with new schema\")\n                    return 0\n\n                # Add vault_id column with default \"1\"\n                df[\"vault_id\"] = \"1\"\n                migrated_count = len(df)\n\n                # Drop and recreate table with updated data\n                self.db.drop_table(\"chunks\")\n                try:\n                    self.table = self.db.create_table(\"chunks\", data=df)\n                except Exception as create_err:\n                    logger.critical(f\"LanceDB vault_id migration: table dropped but recreate failed: {create_err}. Data may need manual recovery from backup.\")\n                    raise\n                logger.info(f\"LanceDB vault_id migration: backfilled {migrated_count} records\")\n                return migrated_count\n            except Exception as e:\n                logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n                return 0\n\n    def get_chunks_by_uid(self, chunk_uids: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Fetch chunks by their unique IDs.\n        \n        Args:\n            chunk_uids: List of chunk UIDs in format \"{file_id}_{chunk_index}\"\n            \n        Returns:\n            List of matching chunk records from LanceDB.\n        \"\"\"\n        if self.table is None:\n            return []\n        \n        if not chunk_uids:\n            return []\n        \n        try:\n            # Build IN clause for chunk_uids\n            # Each uid is in format \"{file_id}_{chunk_index}\"\n            # Escape single quotes in uids for SQL-like syntax\n            escaped_uids = [uid.replace(\"'\", \"''\") for uid in chunk_uids]\n            quoted_uids = [f\"'{uid}'\" for uid in escaped_uids]\n            uid_list = \", \".join(quoted_uids)\n            \n            # Query chunks where id is in the list of chunk_uids\n            query = f\"id IN ({uid_list})\"\n            results = self.table.search() \\\n                .where(query) \\\n                .to_list()\n            \n            return results\n        except Exception as e:\n            logger.warning(f\"Failed to fetch chunks by UID: {e}\")\n            return []\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get statistics about the vector store.\n        \n        Returns:\n            Dictionary with stats like total chunks, embedding dimension.\n        \"\"\"\n        if self.table is None:\n            return {\"total_chunks\": 0, \"embedding_dim\": self._embedding_dim}\n        \n        return {\n            \"total_chunks\": self.table.count_rows(),\n            \"embedding_dim\": self._embedding_dim,\n        }\n    \n    def close(self) -> None:\n        \"\"\"Close the database connection.\"\"\"\n        # LanceDB connections are typically stateless\n        self.db = None\n        self.table = None\n    \n    def get_stored_metadata(self) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Get stored metadata from the table's metadata.\n        \n        Returns:\n            Dictionary with stored metadata (embedding_model_id, embedding_dim, embedding_prefix_hash)\n            or None if table doesn't exist or no metadata is stored.\n        \"\"\"\n        if self.table is None:\n            return None\n        \n        try:\n            # Try to get table metadata\n            table_metadata = self.table.schema.metadata\n            if table_metadata:\n                # Convert bytes keys/values to strings if needed\n                metadata = {}\n                for key, value in table_metadata.items():\n                    if isinstance(key, bytes):\n                        key = key.decode('utf-8')\n                    if isinstance(value, bytes):\n                        value = value.decode('utf-8')\n                    metadata[key] = value\n                \n                # Extract our stored fields\n                result = {}\n                if b'embedding_model_id' in table_metadata or 'embedding_model_id' in metadata:\n                    result['embedding_model_id'] = metadata.get('embedding_model_id')\n                if b'embedding_dim' in table_metadata or 'embedding_dim' in metadata:\n                    result['embedding_dim'] = int(metadata.get('embedding_dim', 0))\n                if b'embedding_prefix_hash' in table_metadata or 'embedding_prefix_hash' in metadata:\n                    result['embedding_prefix_hash'] = metadata.get('embedding_prefix_hash')\n                \n                if result:\n                    return result\n        except Exception as e:\n            logger.debug(f\"Failed to read table metadata: {e}\")\n        \n        return None\n    \n    def validate_schema(self, embedding_model_id: str, embedding_dim: int) -> Dict[str, Any]:\n        \"\"\"\n        Validate that the table schema matches the current embedding configuration.\n        \n        Args:\n            embedding_model_id: The embedding model identifier\n            embedding_dim: The expected embedding dimension\n            \n        Returns:\n            Dictionary with validation results\n            \n        Raises:\n            VectorStoreValidationError: If embedding dimension mismatch is detected\n        \"\"\"\n        # Generate a probe embedding for \"dimension_probe\" text\n        probe_text = \"dimension_probe\"\n        try:\n            probe_embedding = self._generate_probe_embedding(probe_text, embedding_dim)\n        except Exception as e:\n            logger.warning(f\"Failed to generate probe embedding: {e}\")\n            probe_embedding = None\n        \n        # Get expected dimension from the provided parameter\n        expected_dim = embedding_dim\n        \n        # Check if table exists\n        table_exists = False\n        if self.db is not None:\n            try:\n                table_names = self.db.table_names()\n                table_exists = \"chunks\" in table_names\n            except Exception as e:\n                logger.warning(f\"Failed to check table existence: {e}\")\n        \n        stored_metadata = None\n        if table_exists:\n            try:\n                if self.table is None:\n                    self.table = self.db.open_table(\"chunks\")\n                \n                # Get schema and compare vector dimension\n                schema = self.table.schema\n                embedding_field = schema.field(\"embedding\")\n                actual_dim = None\n                if hasattr(embedding_field.type, 'list_size'):\n                    actual_dim = embedding_field.type.list_size\n                \n                if actual_dim is not None and actual_dim != expected_dim:\n                    error_msg = f\"Embedding dimension changed from {actual_dim} to {expected_dim}; reindex required.\"\n                    logger.error(error_msg)\n                    raise VectorStoreValidationError(error_msg)\n                \n                # Get stored metadata\n                stored_metadata = self.get_stored_metadata()\n                \n            except VectorStoreValidationError:\n                raise\n            except Exception as e:\n                logger.warning(f\"Failed to validate schema: {e}\")\n        \n        # Prepare metadata to store\n        import hashlib\n        prefix_hash = hashlib.sha256(embedding_model_id.encode('utf-8')).hexdigest()[:16]\n        \n        metadata_to_store = {\n            'embedding_model_id': embedding_model_id,\n            'embedding_dim': str(expected_dim),\n            'embedding_prefix_hash': prefix_hash\n        }\n        \n        # Update table metadata if table exists\n        if table_exists and self.table is not None:\n            try:\n                # Get existing metadata\n                current_metadata = dict(self.table.schema.metadata) if self.table.schema.metadata else {}\n                \n                # Update with our metadata\n                for key, value in metadata_to_store.items():\n                    if isinstance(value, str):\n                        current_metadata[key.encode('utf-8')] = value.encode('utf-8')\n                    else:\n                        current_metadata[key.encode('utf-8')] = str(value).encode('utf-8')\n                \n                # Note: LanceDB doesn't support direct metadata update on existing table\n                # We'll log the metadata that should be stored for future reference\n                logger.info(f\"Table metadata to store/update: {metadata_to_store}\")\n                \n            except Exception as e:\n                logger.warning(f\"Failed to update table metadata: {e}\")\n        \n        return {\n            'table_exists': table_exists,\n            'expected_dim': expected_dim,\n            'actual_dim': expected_dim if table_exists else None,\n            'stored_metadata': stored_metadata,\n            'probe_embedding_generated': probe_embedding is not None,\n            'metadata_to_store': metadata_to_store\n        }\n    \n    def _generate_probe_embedding(self, text: str, dim: int) -> List[float]:\n        \"\"\"\n        Generate a probe embedding for dimension validation.\n        \n        Args:\n            text: The text to generate embedding for\n            dim: Expected dimension\n            \n        Returns:\n            Generated embedding vector\n        \"\"\"\n        # Use a deterministic hash-based approach for probe embedding\n        import hashlib\n        import random\n        \n        # Create a deterministic seed from the text\n        seed_value = int(hashlib.md5(text.encode('utf-8')).hexdigest(), 16) % (2**32)\n        random.seed(seed_value)\n        \n        # Generate a random vector of expected dimension\n        # This simulates what a real embedding would look like\n        probe = [random.gauss(0, 1) for _ in range(dim)]\n        \n        # Normalize the vector (typical for embeddings)\n        magnitude = sum(x*x for x in probe) ** 0.5\n        if magnitude > 0:\n            probe = [x / magnitude for x in probe]\n        \n        return probe\n\n\n\n","timestamp":1771960470674,"originalBytes":26934}