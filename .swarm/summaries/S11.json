{"id":"S11","summaryText":"[SUMMARY S11] 28.3 KB | code | 664 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\vector_store.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: LanceDB vector store service for semantic search.\n3: \"\"\"\n→ Use /swarm retrieve S11 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\vector_store.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: LanceDB vector store service for semantic search.\n3: \"\"\"\n4: from pathlib import Path\n5: from typing import Any, Dict, List, Optional\n6: import lancedb\n7: import pyarrow as pa\n8: import numpy as np\n9: import logging\n10: \n11: from app.config import settings\n12: \n13: logger = logging.getLogger(__name__)\n14: \n15: \n16: class VectorStoreError(Exception):\n17:     \"\"\"Custom exception for vector store errors.\"\"\"\n18:     pass\n19: \n20: \n21: class VectorStoreConnectionError(VectorStoreError):\n22:     \"\"\"Exception raised when connection to LanceDB fails.\"\"\"\n23:     pass\n24: \n25: \n26: class VectorStoreValidationError(VectorStoreError):\n27:     \"\"\"Exception raised when record validation fails.\"\"\"\n28:     pass\n29: \n30: \n31: class VectorStore:\n32:     \"\"\"LanceDB-based vector store for document chunk embeddings.\"\"\"\n33:     \n34:     def __init__(self, db_path: Optional[Path] = None):\n35:         \"\"\"\n36:         Initialize the vector store.\n37:         \n38:         Args:\n39:             db_path: Path to LanceDB database. Defaults to settings.lancedb_path.\n40:         \"\"\"\n41:         self.db_path = db_path or settings.lancedb_path\n42:         self.db: Optional[lancedb.DBConnection] = None\n43:         self.table: Optional[lancedb.table.Table] = None\n44:         self._embedding_dim: Optional[int] = None\n45:     \n46:     def connect(self) -> \"VectorStore\":\n47:         \"\"\"Connect to LanceDB.\n48:         \n49:         Raises:\n50:             VectorStoreConnectionError: If connection to LanceDB fails.\n51:         \"\"\"\n52:         try:\n53:             self.db = lancedb.connect(str(self.db_path))\n54:         except Exception as e:\n55:             raise VectorStoreConnectionError(f\"Failed to connect to LanceDB at {self.db_path}: {e}\") from e\n56:         return self\n57:     \n58:     def init_table(self, embedding_dim: int) -> \"VectorStore\":\n59:         \"\"\"\n60:         Initialize or open the 'chunks' table.\n61:         \n62:         Args:\n63:             embedding_dim: Dimension of embedding vectors.\n64:             \n65:         Returns:\n66:             Self for method chaining.\n67:             \n68:         Raises:\n69:             VectorStoreConnectionError: If connection or table operations fail.\n70:         \"\"\"\n71:         if self.db is None:\n72:             self.connect()\n73:         \n74:         if self.db is None:\n75:             raise VectorStoreConnectionError(\"Database connection is not available.\")\n76:         \n77:         self._embedding_dim = embedding_dim\n78:         \n79:         # Define schema for chunks table\n80:         schema = pa.schema([\n81:             (\"id\", pa.string()),\n82:             (\"text\", pa.string()),\n83:             (\"file_id\", pa.string()),\n84:             (\"vault_id\", pa.string()),  # Vault isolation\n85:             (\"chunk_index\", pa.int32()),\n86:             (\"metadata\", pa.string()),  # JSON string for flexibility\n87:             (\"embedding\", pa.list_(pa.float32(), embedding_dim)),\n88:         ])\n89:         \n90:         # Create or open table with error handling\n91:         try:\n92:             if \"chunks\" in self.db.table_names():\n93:                 try:\n94:                     self.table = self.db.open_table(\"chunks\")\n95:                 except Exception:\n96:                     # Stale table reference — drop and recreate\n97:                     try:\n98:                         self.db.drop_table(\"chunks\")\n99:                     except Exception:\n100:                         pass\n101:                     self.table = self.db.create_table(\"chunks\", schema=schema, mode=\"overwrite\")\n102:             else:\n103:                 self.table = self.db.create_table(\"chunks\", schema=schema)\n104:         except Exception as e:\n105:             raise VectorStoreConnectionError(f\"Failed to initialize 'chunks' table: {e}\") from e\n106:         \n107:         return self\n108:     \n109:     def _get_expected_embedding_dim(self) -> Optional[int]:\n110:         \"\"\"Get the expected embedding dimension from the table schema.\"\"\"\n111:         if self.table is None:\n112:             return self._embedding_dim\n113:         \n114:         try:\n115:             schema = self.table.schema\n116:             embedding_field = schema.field(\"embedding\")\n117:             if hasattr(embedding_field.type, 'list_size'):\n118:                 return embedding_field.type.list_size\n119:         except Exception:\n120:             pass\n121:         return self._embedding_dim\n122:     \n123:     def add_chunks(self, records: List[Dict[str, Any]]) -> None:\n124:         \"\"\"\n125:         Add chunk records to the vector store.\n126:         \n127:         Args:\n128:             records: List of records with keys: id, text, file_id, chunk_index, \n129:                      metadata, embedding, vault_id (optional, defaults to \"1\").\n130:                      \n131:         Raises:\n132:             RuntimeError: If table is not initialized.\n133:             VectorStoreValidationError: If records validation fails.\n134:         \"\"\"\n135:         if self.table is None:\n136:             raise RuntimeError(\"Table not initialized. Call init_table() first.\")\n137:         \n138:         # Handle empty records\n139:         if not records:\n140:             return\n141:         \n142:         # Get expected embedding dimension from table schema\n143:         expected_dim = self._get_expected_embedding_dim()\n144:         \n145:         # Required fields for validation\n146:         required_fields = [\"id\", \"text\", \"file_id\", \"chunk_index\", \"embedding\"]\n147:         \n148:         # Convert records to arrow-compatible format\n149:         processed_records = []\n150:         for record in records:\n151:             # Validate required fields\n152:             missing_fields = [field for field in required_fields if field not in record]\n153:             if missing_fields:\n154:                 raise VectorStoreValidationError(\n155:                     f\"Record missing required fields: {', '.join(missing_fields)}\"\n156:                 )\n157:             \n158:             # Ensure embedding is a list (convert from numpy if needed)\n159:             embedding = record[\"embedding\"]\n160:             if isinstance(embedding, np.ndarray):\n161:                 embedding = embedding.tolist()\n162:             elif not isinstance(embedding, list):\n163:                 raise VectorStoreValidationError(\n164:                     f\"Embedding must be a list or numpy array, got {type(embedding).__name__}\"\n165:                 )\n166:             \n167:             # Validate embedding dimension matches table schema\n168:             actual_dim = len(embedding)\n169:             if expected_dim is not None and actual_dim != expected_dim:\n170:                 raise VectorStoreValidationError(\n171:                     f\"Embedding dimension mismatch: expected {expected_dim} dimensions, \"\n172:                     f\"got {actual_dim}. The table was created with a different embedding model. \"\n173:                     f\"Delete the lancedb directory at {self.db_path} and restart to use the new model.\"\n174:                 )\n175:             \n176:             processed_record = {\n177:                 \"id\": record[\"id\"],\n178:                 \"text\": record[\"text\"],\n179:                 \"file_id\": record[\"file_id\"],\n180:                 \"vault_id\": record.get(\"vault_id\", \"1\"),  # Default to vault \"1\"\n181:                 \"chunk_index\": record[\"chunk_index\"],\n182:                 \"metadata\": record.get(\"metadata\", \"{}\"),\n183:                 \"embedding\": embedding,\n184:             }\n185:             processed_records.append(processed_record)\n186:         \n187:         self.table.add(processed_records)\n188:     \n189:     def search(\n190:         self,\n191:         embedding: List[float],\n192:         limit: int = 10,\n193:         filter_expr: Optional[str] = None,\n194:         vault_id: Optional[str] = None\n195:     ) -> List[Dict[str, Any]]:\n196:         \"\"\"\n197:         Search for similar chunks by embedding.\n198: \n199:         Args:\n200:             embedding: Query embedding vector.\n201:             limit: Maximum number of results.\n202:             filter_expr: Optional filter expression (LanceDB syntax).\n203:             vault_id: Optional vault ID to filter results. If provided, only returns\n204:                      chunks from the specified vault.\n205: \n206:         Returns:\n207:             List of matching records with similarity scores. Each record includes:\n208:             - All original fields (id, text, file_id, chunk_index, metadata, etc.)\n209:             - _distance: Cosine distance from query embedding (lower = more similar)\n210:             Empty list if no table exists.\n211:             \n212:         Note:\n213:             For cosine distance metric:\n214:             - Distance of 0 = identical vectors (perfect match)\n215:             - Distance of 1 = orthogonal vectors\n216:             - Distance of 2 = opposite vectors (perfect mismatch)\n217:             The _distance field is provided by LanceDB's vector search.\n218:         \"\"\"\n219:         # Ensure DB connection exists\n220:         if self.db is None:\n221:             self.connect()\n222:         \n223:         # Try to open existing table if not already loaded\n224:         if self.table is None:\n225:             try:\n226:                 table_names = self.db.table_names()\n227:             except Exception as e:\n228:                 raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n229:             \n230:             if \"chunks\" not in table_names:\n231:                 # No table exists yet - graceful no-docs behavior\n232:                 return []\n233:             \n234:             # Table exists, try to open it\n235:             try:\n236:                 self.table = self.db.open_table(\"chunks\")\n237:             except Exception as e:\n238:                 raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n239:             \n240:             # Set embedding_dim from table schema if available\n241:             if self._embedding_dim is None:\n242:                 try:\n243:                     schema = self.table.schema\n244:                     embedding_field = schema.field(\"embedding\")\n245:                     # Extract dimension from fixed size list type\n246:                     if hasattr(embedding_field.type, 'list_size'):\n247:                         self._embedding_dim = embedding_field.type.list_size\n248:                 except Exception:\n249:                     # If we can't determine embedding_dim, leave it as None\n250:                     pass\n251:         \n252:         query = self.table.search(embedding, metric=\"cosine\")\n253: \n254:         # Apply vault filter if specified\n255:         if vault_id is not None:\n256:             safe_vault_id = str(vault_id).replace(\"'\", \"\\\\'\")\n257:             vault_filter = f\"vault_id = '{safe_vault_id}'\"\n258:             if filter_expr:\n259:                 filter_expr = f\"({filter_expr}) AND ({vault_filter})\"\n260:             else:\n261:                 filter_expr = vault_filter\n262: \n263:         if filter_expr:\n264:             query = query.where(filter_expr)\n265:         \n266:         results = query.limit(limit).to_list()\n267:         return results\n268:     \n269:     def delete_by_file(self, file_id: str) -> int:\n270:         \"\"\"\n271:         Delete all chunks for a given file_id.\n272:         \n273:         Args:\n274:             file_id: The file ID to delete chunks for.\n275:             \n276:         Returns:\n277:             Number of records deleted.\n278:         \"\"\"\n279:         # Ensure DB connection exists\n280:         if self.db is None:\n281:             self.connect()\n282:         \n283:         # Try to open existing table if not already loaded\n284:         if self.table is None:\n285:             try:\n286:                 table_names = self.db.table_names()\n287:             except Exception as e:\n288:                 raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n289:             \n290:             if \"chunks\" not in table_names:\n291:                 # No table exists yet - nothing to delete\n292:                 return 0\n293:             \n294:             # Table exists, try to open it\n295:             try:\n296:                 self.table = self.db.open_table(\"chunks\")\n297:             except Exception as e:\n298:                 raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n299:             \n300:             # Set embedding_dim from table schema if available\n301:             if self._embedding_dim is None:\n302:                 try:\n303:                     schema = self.table.schema\n304:                     embedding_field = schema.field(\"embedding\")\n305:                     # Extract dimension from fixed size list type\n306:                     if hasattr(embedding_field.type, 'list_size'):\n307:                         self._embedding_dim = embedding_field.type.list_size\n308:                 except Exception:\n309:                     # If we can't determine embedding_dim, leave it as None\n310:                     pass\n311: \n312:         # Query count before delete to return accurate deletion count\n313:         safe_file_id = str(file_id).replace('\"', '\\\\\"')\n314:         try:\n315:             count_before = self.table.count_rows(f'file_id = \"{safe_file_id}\"')\n316:         except Exception:\n317:             # If count_rows fails, safely default to 0\n318:             count_before = 0\n319: \n320:         # LanceDB delete using filter expression\n321:         self.table.delete(f'file_id = \"{safe_file_id}\"')\n322: \n323:         return count_before\n324: \n325:     def delete_by_vault(self, vault_id: str) -> int:\n326:         \"\"\"\n327:         Delete all chunks for a given vault_id.\n328: \n329:         Args:\n330:             vault_id: The vault ID to delete all chunks for.\n331: \n332:         Returns:\n333:             Number of records deleted.\n334:         \"\"\"\n335:         # Ensure DB connection exists\n336:         if self.db is None:\n337:             self.connect()\n338: \n339:         # Try to open existing table if not already loaded\n340:         if self.table is None:\n341:             try:\n342:                 table_names = self.db.table_names()\n343:             except Exception as e:\n344:                 raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n345: \n346:             if \"chunks\" not in table_names:\n347:                 return 0\n348: \n349:             try:\n350:                 self.table = self.db.open_table(\"chunks\")\n351:             except Exception as e:\n352:                 raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n353: \n354:         safe_vault_id = str(vault_id).replace(\"'\", \"\\\\'\")\n355:         try:\n356:             count_before = self.table.count_rows(f\"vault_id = '{safe_vault_id}'\")\n357:         except Exception:\n358:             count_before = 0\n359: \n360:         self.table.delete(f\"vault_id = '{safe_vault_id}'\")\n361:         return count_before\n362: \n363:     def migrate_add_vault_id(self) -> int:\n364:         \"\"\"\n365:         Migration: Backfill vault_id='1' on existing chunks that lack it.\n366: \n367:         LanceDB doesn't support ALTER TABLE or UPDATE, so this reads all data,\n368:         adds the vault_id field, and rewrites the table. This is idempotent —\n369:         safe to call multiple times (no-op if all records already have vault_id).\n370: \n371:         Returns:\n372:             Number of records migrated. 0 if no migration was needed.\n373:         \"\"\"\n374:         if self.db is None:\n375:             self.connect()\n376: \n377:         if self.db is None:\n378:             logger.info(\"LanceDB vault_id migration: no connection available\")\n379:             return 0\n380: \n381:         try:\n382:             table_names = self.db.table_names()\n383:         except Exception as e:\n384:             logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n385:             return 0\n386: \n387:         if \"chunks\" not in table_names:\n388:             logger.info(\"LanceDB vault_id migration: no table exists\")\n389:             return 0\n390: \n391:         try:\n392:             table = self.db.open_table(\"chunks\")\n393:         except Exception as e:\n394:             logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n395:             return 0\n396: \n397:         # Check if vault_id column exists in schema\n398:         schema = table.schema\n399:         field_names = [schema.field(i).name for i in range(len(schema))]\n400: \n401:         if \"vault_id\" in field_names:\n402:             # Column exists — check if any rows have null vault_id\n403:             try:\n404:                 df = table.to_pandas()\n405:                 null_count = df[\"vault_id\"].isna().sum()\n406:                 if null_count == 0:\n407:                     logger.info(\"LanceDB vault_id migration: no migration needed\")\n408:                     return 0  # All records already have vault_id\n409: \n410:                 # Backfill null vault_ids with \"1\"\n411:                 df[\"vault_id\"] = df[\"vault_id\"].fillna(\"1\")\n412:                 count = int(null_count)\n413: \n414:                 # Drop and recreate table with updated data\n415:                 self.db.drop_table(\"chunks\")\n416:                 try:\n417:                     self.table = self.db.create_table(\"chunks\", data=df)\n418:                 except Exception as create_err:\n419:                     logger.critical(f\"LanceDB vault_id migration: table dropped but recreate failed: {create_err}. Data may need manual recovery from backup.\")\n420:                     raise\n421:                 logger.info(f\"LanceDB vault_id migration: backfilled {count} records\")\n422:                 return count\n423:             except Exception as e:\n424:                 logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n425:                 return 0\n426:         else:\n427:             # Column doesn't exist — add it to all records\n428:             try:\n429:                 df = table.to_pandas()\n430:                 if len(df) == 0:\n431:                     # Empty table — just drop and recreate with new schema\n432:                     # Try to get embedding_dim from existing schema before dropping\n433:                     if self._embedding_dim is None:\n434:                         try:\n435:                             embedding_field = table.schema.field(\"embedding\")\n436:                             if hasattr(embedding_field.type, 'list_size'):\n437:                                 self._embedding_dim = embedding_field.type.list_size\n438:                         except Exception:\n439:                             pass\n440: \n441:                     self.db.drop_table(\"chunks\")\n442:                     try:\n443:                         if self._embedding_dim:\n444:                             self.init_table(self._embedding_dim)\n445:                     except Exception as create_err:\n446:                         logger.critical(f\"LanceDB vault_id migration: empty table dropped but recreate failed: {create_err}\")\n447:                         raise\n448:                     logger.info(\"LanceDB vault_id migration: empty table, recreated with new schema\")\n449:                     return 0\n450: \n451:                 # Add vault_id column with default \"1\"\n452:                 df[\"vault_id\"] = \"1\"\n453:                 migrated_count = len(df)\n454: \n455:                 # Drop and recreate table with updated data\n456:                 self.db.drop_table(\"chunks\")\n457:                 try:\n458:                     self.table = self.db.create_table(\"chunks\", data=df)\n459:                 except Exception as create_err:\n460:                     logger.critical(f\"LanceDB vault_id migration: table dropped but recreate failed: {create_err}. Data may need manual recovery from backup.\")\n461:                     raise\n462:                 logger.info(f\"LanceDB vault_id migration: backfilled {migrated_count} records\")\n463:                 return migrated_count\n464:             except Exception as e:\n465:                 logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n466:                 return 0\n467: \n468:     def get_stats(self) -> Dict[str, Any]:\n469:         \"\"\"\n470:         Get statistics about the vector store.\n471:         \n472:         Returns:\n473:             Dictionary with stats like total chunks, embedding dimension.\n474:         \"\"\"\n475:         if self.table is None:\n476:             return {\"total_chunks\": 0, \"embedding_dim\": self._embedding_dim}\n477:         \n478:         return {\n479:             \"total_chunks\": self.table.count_rows(),\n480:             \"embedding_dim\": self._embedding_dim,\n481:         }\n482:     \n483:     def close(self) -> None:\n484:         \"\"\"Close the database connection.\"\"\"\n485:         # LanceDB connections are typically stateless\n486:         self.db = None\n487:         self.table = None\n488:     \n489:     def get_stored_metadata(self) -> Optional[Dict[str, Any]]:\n490:         \"\"\"\n491:         Get stored metadata from the table's metadata.\n492:         \n493:         Returns:\n494:             Dictionary with stored metadata (embedding_model_id, embedding_dim, embedding_prefix_hash)\n495:             or None if table doesn't exist or no metadata is stored.\n496:         \"\"\"\n497:         if self.table is None:\n498:             return None\n499:         \n500:         try:\n501:             # Try to get table metadata\n502:             table_metadata = self.table.schema.metadata\n503:             if table_metadata:\n504:                 # Convert bytes keys/values to strings if needed\n505:                 metadata = {}\n506:                 for key, value in table_metadata.items():\n507:                     if isinstance(key, bytes):\n508:                         key = key.decode('utf-8')\n509:                     if isinstance(value, bytes):\n510:                         value = value.decode('utf-8')\n511:                     metadata[key] = value\n512:                 \n513:                 # Extract our stored fields\n514:                 result = {}\n515:                 if b'embedding_model_id' in table_metadata or 'embedding_model_id' in metadata:\n516:                     result['embedding_model_id'] = metadata.get('embedding_model_id')\n517:                 if b'embedding_dim' in table_metadata or 'embedding_dim' in metadata:\n518:                     result['embedding_dim'] = int(metadata.get('embedding_dim', 0))\n519:                 if b'embedding_prefix_hash' in table_metadata or 'embedding_prefix_hash' in metadata:\n520:                     result['embedding_prefix_hash'] = metadata.get('embedding_prefix_hash')\n521:                 \n522:                 if result:\n523:                     return result\n524:         except Exception as e:\n525:             logger.debug(f\"Failed to read table metadata: {e}\")\n526:         \n527:         return None\n528:     \n529:     def validate_schema(self, embedding_model_id: str, embedding_dim: int) -> Dict[str, Any]:\n530:         \"\"\"\n531:         Validate that the table schema matches the current embedding configuration.\n532:         \n533:         Args:\n534:             embedding_model_id: The embedding model identifier\n535:             embedding_dim: The expected embedding dimension\n536:             \n537:         Returns:\n538:             Dictionary with validation results\n539:             \n540:         Raises:\n541:             VectorStoreValidationError: If embedding dimension mismatch is detected\n542:         \"\"\"\n543:         # Generate a probe embedding for \"dimension_probe\" text\n544:         probe_text = \"dimension_probe\"\n545:         try:\n546:             probe_embedding = self._generate_probe_embedding(probe_text, embedding_dim)\n547:         except Exception as e:\n548:             logger.warning(f\"Failed to generate probe embedding: {e}\")\n549:             probe_embedding = None\n550:         \n551:         # Get expected dimension from the provided parameter\n552:         expected_dim = embedding_dim\n553:         \n554:         # Check if table exists\n555:         table_exists = False\n556:         if self.db is not None:\n557:             try:\n558:                 table_names = self.db.table_names()\n559:                 table_exists = \"chunks\" in table_names\n560:             except Exception as e:\n561:                 logger.warning(f\"Failed to check table existence: {e}\")\n562:         \n563:         stored_metadata = None\n564:         if table_exists:\n565:             try:\n566:                 if self.table is None:\n567:                     self.table = self.db.open_table(\"chunks\")\n568:                 \n569:                 # Get schema and compare vector dimension\n570:                 schema = self.table.schema\n571:                 embedding_field = schema.field(\"embedding\")\n572:                 actual_dim = None\n573:                 if hasattr(embedding_field.type, 'list_size'):\n574:                     actual_dim = embedding_field.type.list_size\n575:                 \n576:                 if actual_dim is not None and actual_dim != expected_dim:\n577:                     error_msg = f\"Embedding dimension changed from {actual_dim} to {expected_dim}; reindex required.\"\n578:                     logger.error(error_msg)\n579:                     raise VectorStoreValidationError(error_msg)\n580:                 \n581:                 # Get stored metadata\n582:                 stored_metadata = self.get_stored_metadata()\n583:                 \n584:             except VectorStoreValidationError:\n585:                 raise\n586:             except Exception as e:\n587:                 logger.warning(f\"Failed to validate schema: {e}\")\n588:         \n589:         # Prepare metadata to store\n590:         import hashlib\n591:         prefix_hash = hashlib.sha256(embedding_model_id.encode('utf-8')).hexdigest()[:16]\n592:         \n593:         metadata_to_store = {\n594:             'embedding_model_id': embedding_model_id,\n595:             'embedding_dim': str(expected_dim),\n596:             'embedding_prefix_hash': prefix_hash\n597:         }\n598:         \n599:         # Update table metadata if table exists\n600:         if table_exists and self.table is not None:\n601:             try:\n602:                 # Get existing metadata\n603:                 current_metadata = dict(self.table.schema.metadata) if self.table.schema.metadata else {}\n604:                 \n605:                 # Update with our metadata\n606:                 for key, value in metadata_to_store.items():\n607:                     if isinstance(value, str):\n608:                         current_metadata[key.encode('utf-8')] = value.encode('utf-8')\n609:                     else:\n610:                         current_metadata[key.encode('utf-8')] = str(value).encode('utf-8')\n611:                 \n612:                 # Note: LanceDB doesn't support direct metadata update on existing table\n613:                 # We'll log the metadata that should be stored for future reference\n614:                 logger.info(f\"Table metadata to store/update: {metadata_to_store}\")\n615:                 \n616:             except Exception as e:\n617:                 logger.warning(f\"Failed to update table metadata: {e}\")\n618:         \n619:         return {\n620:             'table_exists': table_exists,\n621:             'expected_dim': expected_dim,\n622:             'actual_dim': expected_dim if table_exists else None,\n623:             'stored_metadata': stored_metadata,\n624:             'probe_embedding_generated': probe_embedding is not None,\n625:             'metadata_to_store': metadata_to_store\n626:         }\n627:     \n628:     def _generate_probe_embedding(self, text: str, dim: int) -> List[float]:\n629:         \"\"\"\n630:         Generate a probe embedding for dimension validation.\n631:         \n632:         Args:\n633:             text: The text to generate embedding for\n634:             dim: Expected dimension\n635:             \n636:         Returns:\n637:             Generated embedding vector\n638:         \"\"\"\n639:         # Use a deterministic hash-based approach for probe embedding\n640:         import hashlib\n641:         import random\n642:         \n643:         # Create a deterministic seed from the text\n644:         seed_value = int(hashlib.md5(text.encode('utf-8')).hexdigest(), 16) % (2**32)\n645:         random.seed(seed_value)\n646:         \n647:         # Generate a random vector of expected dimension\n648:         # This simulates what a real embedding would look like\n649:         probe = [random.gauss(0, 1) for _ in range(dim)]\n650:         \n651:         # Normalize the vector (typical for embeddings)\n652:         magnitude = sum(x*x for x in probe) ** 0.5\n653:         if magnitude > 0:\n654:             probe = [x / magnitude for x in probe]\n655:         \n656:         return probe\n657: \n658: \n659: \n\n(End of file - total 659 lines)\n</content>","timestamp":1771866465436,"originalBytes":29016}