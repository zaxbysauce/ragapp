{"id":"S5","summaryText":"[SUMMARY S5] 25.9 KB | code | 660 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\email_service.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Email ingestion service for IMAP-based document processing.\n3: \n→ Use /swarm retrieve S5 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\email_service.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Email ingestion service for IMAP-based document processing.\n3: \n4: Provides EmailIngestionService class that periodically polls an IMAP inbox,\n5: extracts document attachments, and enqueues them for processing via BackgroundProcessor.\n6: Supports vault routing via subject tags [VaultName] or #vaultname.\n7: \"\"\"\n8: \n9: import asyncio\n10: import email\n11: import email.policy\n12: import logging\n13: import os\n14: import re\n15: import tempfile\n16: from datetime import datetime\n17: from pathlib import Path\n18: from typing import Optional\n19: \n20: import aioimaplib\n21: import bleach\n22: from email.message import EmailMessage\n23: from email.header import decode_header\n24: \n25: from app.config import Settings\n26: from app.models.database import SQLiteConnectionPool\n27: from app.services.background_tasks import BackgroundProcessor\n28: \n29: \n30: logger = logging.getLogger(__name__)\n31: \n32: # Allowed HTML tags for sanitization\n33: ALLOWED_TAGS = ['p', 'br', 'strong', 'em', 'ul', 'ol', 'li']\n34: \n35: # Maximum email size limit (50MB)\n36: MAX_EMAIL_SIZE = 50 * 1024 * 1024  # 50MB in bytes\n37: \n38: # Maximum number of attachments per email (default 10)\n39: MAX_ATTACHMENTS_PER_EMAIL = 10\n40: \n41: \n42: class EmailIngestionService:\n43:     \"\"\"\n44:     Email ingestion service for IMAP-based document processing.\n45: \n46:     Periodically polls an IMAP mailbox for UNSEEN emails, extracts document\n47:     attachments, and enqueues them for processing via BackgroundProcessor.\n48:     Supports vault routing via subject tags [VaultName] or #vaultname.\n49: \n50:     Attributes:\n51:         settings: Settings instance with IMAP configuration\n52:         pool: SQLiteConnectionPool for database access\n53:         background_processor: BackgroundProcessor for document processing\n54:         _stop_event: asyncio.Event for graceful shutdown\n55:         _polling_task: Reference to the polling coroutine\n56:         _running: Boolean indicating if service is active\n57:         _last_error: Last error message (for health check)\n58:     \"\"\"\n59: \n60:     def __init__(\n61:         self,\n62:         settings: Settings,\n63:         pool: SQLiteConnectionPool,\n64:         background_processor: BackgroundProcessor,\n65:     ):\n66:         \"\"\"\n67:         Initialize the email ingestion service.\n68: \n69:         Args:\n70:             settings: Settings instance with IMAP configuration\n71:             pool: SQLiteConnectionPool for database access\n72:             background_processor: BackgroundProcessor for document processing\n73:         \"\"\"\n74:         self.settings = settings\n75:         self.pool = pool\n76:         self.background_processor = background_processor\n77:         self._stop_event = asyncio.Event()\n78:         self._polling_task: Optional[asyncio.Task] = None\n79:         self._running = False\n80:         self._last_error: Optional[str] = None\n81:         self._last_poll_time: Optional[datetime] = None\n82:         self._current_backoff_delay: Optional[int] = None\n83: \n84:     async def start_polling(self) -> None:\n85:         \"\"\"\n86:         Start the IMAP polling loop.\n87: \n88:         Begins polling the IMAP mailbox at configured intervals if imap_enabled.\n89:         Safe to call multiple times - will not create duplicate pollers.\n90:         \"\"\"\n91:         if self._running:\n92:             logger.warning(\"Email ingestion service is already running\")\n93:             return\n94: \n95:         if not self.settings.imap_enabled:\n96:             logger.info(\"IMAP email ingestion is disabled, service not started\")\n97:             return\n98: \n99:         self._running = True\n100:         self._stop_event.clear()\n101:         self._polling_task = asyncio.create_task(self._polling_loop())\n102:         logger.info(\"Email ingestion service started\")\n103: \n104:     def stop_polling(self) -> None:\n105:         \"\"\"\n106:         Stop the email ingestion service gracefully.\n107: \n108:         Signals the service to shut down. The actual stop is handled\n109:         asynchronously in the polling loop.\n110:         \"\"\"\n111:         if not self._running:\n112:             logger.warning(\"Email ingestion service is not running\")\n113:             return\n114: \n115:         logger.info(\"Stopping email ingestion service...\")\n116:         self._stop_event.set()\n117: \n118:     def is_healthy(self) -> bool:\n119:         \"\"\"\n120:         Check if the email ingestion service is healthy.\n121: \n122:         Returns:\n123:             True if the service is running without recent errors, False otherwise\n124:         \"\"\"\n125:         return self._running and self._last_error is None\n126: \n127:     def get_last_poll_time(self) -> Optional[datetime]:\n128:         \"\"\"\n129:         Get the timestamp of the last successful poll.\n130: \n131:         Returns:\n132:             Datetime of last poll, or None if no poll has completed\n133:         \"\"\"\n134:         return self._last_poll_time\n135: \n136:     def get_current_backoff_delay(self) -> Optional[int]:\n137:         \"\"\"\n138:         Get the current backoff delay in seconds.\n139: \n140:         Returns:\n141:             Current backoff delay in seconds, or None if not in backoff\n142:         \"\"\"\n143:         return self._current_backoff_delay\n144: \n145:     async def _polling_loop(self) -> None:\n146:         \"\"\"\n147:         Main polling loop that periodically checks for new emails.\n148: \n149:         Continuously polls at configured intervals until stop_event is set.\n150:         Handles exceptions gracefully to keep the service running.\n151:         \"\"\"\n152:         interval_seconds = self.settings.imap_poll_interval\n153: \n154:         while not self._stop_event.is_set():\n155:             try:\n156:                 await self._poll_once()\n157:                 # Clear last error on successful poll\n158:                 self._last_error = None\n159:             except asyncio.CancelledError:\n160:                 logger.info(\"Polling loop cancelled\")\n161:                 break\n162:             except Exception as e:\n163:                 self._last_error = str(e)\n164:                 logger.error(f\"Error during email polling: {e}\", exc_info=True)\n165: \n166:             # Wait for next poll interval or shutdown\n167:             try:\n168:                 await asyncio.wait_for(\n169:                     self._stop_event.wait(),\n170:                     timeout=interval_seconds\n171:                 )\n172:             except asyncio.TimeoutError:\n173:                 # Timeout means interval elapsed, continue to next poll\n174:                 pass\n175: \n176:         self._running = False\n177:         logger.info(\"Email ingestion service stopped\")\n178: \n179:     async def _poll_once(self) -> None:\n180:         \"\"\"\n181:         Single poll iteration: connect, process emails, mark seen.\n182: \n183:         1. Connect to IMAP with backoff\n184:         2. Search for UNSEEN emails\n185:         3. Process each email\n186:         4. Mark emails as seen\n187: \n188:         Raises:\n189:             Exception: If connection or processing fails\n190:         \"\"\"\n191:         imap_client = None\n192:         try:\n193:             imap_client = await self._connect_with_backoff()\n194: \n195:             # Select mailbox\n196:             await imap_client.select(self.settings.imap_mailbox)\n197:             logger.debug(f\"Selected mailbox: {self.settings.imap_mailbox}\")\n198: \n199:             # Search for UNSEEN emails\n200:             result, data = await imap_client.search('UTF-8', 'UNSEEN')\n201:             if result != 'OK':\n202:                 logger.warning(f\"IMAP search failed: {result}\")\n203:                 return\n204: \n205:             uids = data[0].split()\n206:             logger.info(f\"Found {len(uids)} UNSEEN emails\")\n207: \n208:             # Process each email\n209:             for uid in uids:\n210:                 uid_str = uid.decode() if isinstance(uid, bytes) else str(uid)\n211:                 try:\n212:                     await self._process_email(imap_client, uid_str)\n213:                 except Exception as e:\n214:                     logger.error(f\"Error processing email UID {uid_str}: {e}\", exc_info=True)\n215:                     # Continue to next email even if one fails\n216: \n217:             # Update last poll time on successful completion\n218:             self._last_poll_time = datetime.now()\n219: \n220:         except Exception as e:\n221:             logger.error(f\"Error during poll iteration: {e}\", exc_info=True)\n222:             raise\n223:         finally:\n224:             if imap_client:\n225:                 try:\n226:                     await imap_client.logout()\n227:                     logger.debug(\"IMAP connection closed\")\n228:                 except Exception as e:\n229:                     logger.warning(f\"Error closing IMAP connection: {e}\")\n230: \n231:     async def _connect_with_backoff(self) -> aioimaplib.IMAP4_SSL:\n232:         \"\"\"\n233:         Connect to IMAP server with exponential backoff.\n234: \n235:         Implements exponential backoff: 5s → 15s → 30s → 60s (max)\n236:         Handles authentication errors specially (logs and raises).\n237: \n238:         Returns:\n239:             Connected aioimaplib.IMAP4_SSL client\n240: \n241:         Raises:\n242:             Exception: If connection fails after all retries or auth fails\n243:         \"\"\"\n244:         max_delay = 60\n245:         delay = 5\n246:         attempts = 0\n247: \n248:         while not self._stop_event.is_set():\n249:             attempts += 1\n250:             # Track current backoff delay for status endpoint\n251:             self._current_backoff_delay = delay if attempts > 1 else None\n252:             try:\n253:                 logger.debug(\n254:                     f\"Connecting to IMAP server {self.settings.imap_host}:\"\n255:                     f\"{self.settings.imap_port} (attempt {attempts})\"\n256:                 )\n257: \n258:                 imap_client = aioimaplib.IMAP4_SSL(\n259:                     host=self.settings.imap_host,\n260:                     port=self.settings.imap_port,\n261:                     timeout=30  # 30 second timeout for IMAP operations\n262:                 )\n263: \n264:                 # Authenticate (never log password)\n265:                 result = await imap_client.wait_hello_from_server()\n266:                 if result != 'OK':\n267:                     raise Exception(f\"IMAP server greeting failed: {result}\")\n268: \n269:                 result, _ = await imap_client.login(\n270:                     self.settings.imap_username,\n271:                     self.settings.imap_password.get_secret_value()\n272:                 )\n273:                 if result != 'OK':\n274:                     # Auth error is permanent, don't retry with backoff\n275:                     error_msg = \"IMAP authentication failed: check username/password\"\n276:                     logger.error(error_msg)\n277:                     raise Exception(error_msg)\n278: \n279:                 logger.info(f\"IMAP connection established after {attempts} attempt(s)\")\n280:                 # Clear backoff delay on successful connection\n281:                 self._current_backoff_delay = None\n282:                 return imap_client\n283: \n284:             except Exception as e:\n285:                 if \"authentication\" in str(e).lower():\n286:                     # Auth errors are permanent, don't retry\n287:                     raise\n288: \n289:                 logger.warning(\n290:                     f\"IMAP connection attempt {attempts} failed: {e}, \"\n291:                     f\"retrying in {delay}s...\"\n292:                 )\n293: \n294:                 if delay >= max_delay:\n295:                     # Max delay reached, give up\n296:                     error_msg = f\"IMAP connection failed after {attempts} attempts\"\n297:                     logger.error(error_msg)\n298:                     raise Exception(error_msg)\n299: \n300:                 # Wait for backoff delay or shutdown\n301:                 try:\n302:                     await asyncio.wait_for(self._stop_event.wait(), timeout=delay)\n303:                 except asyncio.TimeoutError:\n304:                     pass\n305: \n306:                 # Exponential backoff: 5s → 15s → 30s → 60s (max)\n307:                 delay = min(delay * 3, max_delay)\n308: \n309:         # Stop event was set\n310:         raise Exception(\"Email ingestion service stopped during connection attempt\")\n311: \n312:     async def _process_email(self, imap_client: aioimaplib.IMAP4_SSL, uid: str) -> None:\n313:         \"\"\"\n314:         Process a single email: extract content, save attachments, enqueue.\n315: \n316:         Fetches email content, parses with email.message_from_bytes(),\n317:         extracts subject and sender, resolves vault from subject tag,\n318:         extracts and validates attachments, saves to temp files,\n319:         and queues via background_processor.enqueue().\n320: \n321:         Args:\n322:             imap_client: Connected IMAP client\n323:             uid: Email UID to process\n324: \n325:         Raises:\n326:             Exception: If email processing fails\n327:         \"\"\"\n328:         # Check email size before fetching to prevent DoS\n329:         result, data = await imap_client.fetch(uid, '(RFC822.SIZE)')\n330:         if result != 'OK' or not data[0]:\n331:             logger.warning(f\"Failed to fetch email size for UID {uid}\")\n332:             return\n333: \n334:         # Extract size from response (format: \"123 (RFC822.SIZE {size})\")\n335:         size_match = re.search(r'RFC822\\.SIZE (\\d+)', str(data[0]))\n336:         if size_match:\n337:             email_size = int(size_match.group(1))\n338:             if email_size > MAX_EMAIL_SIZE:\n339:                 size_mb = email_size / (1024 * 1024)\n340:                 max_mb = MAX_EMAIL_SIZE / (1024 * 1024)\n341:                 logger.warning(\n342:                     f\"Email UID {uid} too large ({size_mb:.2f}MB > {max_mb:.2f}MB), skipping\"\n343:                 )\n344:                 return\n345: \n346:         # Fetch email content\n347:         result, data = await imap_client.fetch(uid, '(RFC822)')\n348:         if result != 'OK' or not data[0]:\n349:             logger.warning(f\"Failed to fetch email UID {uid}\")\n350:             return\n351: \n352:         raw_email = data[0][1]  # Extract email bytes\n353:         msg: EmailMessage = email.message_from_bytes(raw_email, policy=email.policy.default)\n354: \n355:         # Extract subject and sender\n356:         subject = self._decode_header_value(msg.get('Subject', ''))\n357:         sender = self._decode_header_value(msg.get('From', ''))\n358: \n359:         # Use sanitized values for logging to prevent log injection\n360:         logger.info(f\"Processing email from {self._sanitize_log_value(sender)}: {self._sanitize_log_value(subject)}\")\n361: \n362:         # Extract vault name from subject tag [VaultName] or #vaultname\n363:         vault_name = self._extract_vault_name(subject)\n364:         vault_id = await self._resolve_vault_id(vault_name)\n365:         logger.debug(f\"Resolved vault_id={vault_id} for vault_name={vault_name}\")\n366: \n367:         # Extract and process attachments\n368:         processed_attachments = 0\n369:         for part in msg.walk():\n370:             # Skip non-attachment parts\n371:             if part.get_content_disposition() != 'attachment':\n372:                 continue\n373: \n374:             # Check attachment count limit\n375:             if processed_attachments >= MAX_ATTACHMENTS_PER_EMAIL:\n376:                 logger.warning(\n377:                     f\"Email has more than {MAX_ATTACHMENTS_PER_EMAIL} attachments, \"\n378:                     f\"skipping remaining attachments\"\n379:                 )\n380:                 break\n381: \n382:             # Validate attachment\n383:             is_valid, reason = self._validate_attachment(part)\n384:             if not is_valid:\n385:                 logger.debug(f\"Skipping attachment: {reason}\")\n386:                 continue\n387: \n388:             # Save attachment to temp file\n389:             file_path = await self._save_attachment(part)\n390: \n391:             # Enqueue for processing\n392:             await self.background_processor.enqueue(\n393:                 file_path=file_path,\n394:                 source='email',\n395:                 email_subject=subject,\n396:                 email_sender=sender,\n397:             )\n398:             processed_attachments += 1\n399:             logger.info(f\"Enqueued attachment for processing: {file_path}\")\n400: \n401:         if processed_attachments == 0:\n402:             logger.info(f\"No valid document attachments found in email from {self._sanitize_log_value(sender)}\")\n403:         else:\n404:             logger.info(\n405:                 f\"Processed {processed_attachments} attachment(s) from email from \"\n406:                 f\"{self._sanitize_log_value(sender)}\"\n407:             )\n408: \n409:     def _sanitize_log_value(self, value: str) -> str:\n410:         \"\"\"\n411:         Sanitize a value for logging by removing newlines and control characters.\n412: \n413:         Prevents log injection attacks where malicious email content could\n414:         inject newlines or control characters into logs.\n415: \n416:         Args:\n417:             value: Raw value to sanitize\n418: \n419:         Returns:\n420:             Sanitized value with newlines and control characters replaced with spaces\n421:         \"\"\"\n422:         if not value:\n423:             return \"\"\n424:         # Replace newlines, carriage returns, tabs and other control characters\n425:         return re.sub(r'[\\r\\n\\t\\x00-\\x1f\\x7f-\\x9f]', ' ', str(value))\n426: \n427:     def _sanitize_filename(self, filename: str) -> str:\n428:         \"\"\"\n429:         Sanitize filename to prevent path traversal attacks.\n430: \n431:         Uses os.path.basename() to extract just the filename portion,\n432:         removing any directory traversal attempts.\n433: \n434:         Args:\n435:             filename: Raw filename from email attachment\n436: \n437:         Returns:\n438:             Sanitized filename with path traversal removed\n439:         \"\"\"\n440:         if not filename:\n441:             return filename\n442:         # Use basename to extract just the filename, removing any path components\n443:         return os.path.basename(filename)\n444: \n445:     def _extract_vault_name(self, subject: str) -> Optional[str]:\n446:         \"\"\"\n447:         Extract vault name from subject tag [VaultName] or #vaultname.\n448: \n449:         Args:\n450:             subject: Email subject line\n451: \n452:         Returns:\n453:             First matching vault name or None\n454:         \"\"\"\n455:         # Pattern for [VaultName] - captures content inside brackets\n456:         bracket_pattern = r'\\[([^\\]]+)\\]'\n457:         match = re.search(bracket_pattern, subject)\n458:         if match:\n459:             return match.group(1).strip()\n460: \n461:         # Pattern for #vaultname - captures hashtag word\n462:         hashtag_pattern = r'#(\\w+)'\n463:         match = re.search(hashtag_pattern, subject)\n464:         if match:\n465:             return match.group(1).strip()\n466: \n467:         return None\n468: \n469:     def _validate_attachment(self, part) -> tuple[bool, str]:\n470:         \"\"\"\n471:         Validate attachment by MIME type and file size.\n472: \n473:         Checks MIME type against whitelist (imap_allowed_mime_types)\n474:         and file size against max (imap_max_attachment_size).\n475:         Checks actual payload size instead of Content-Length header.\n476: \n477:         Args:\n478:             part: Email message part (attachment)\n479: \n480:         Returns:\n481:             Tuple of (is_valid, reason) where reason explains why invalid\n482:         \"\"\"\n483:         # Check MIME type\n484:         content_type = part.get_content_type()\n485:         if content_type not in self.settings.imap_allowed_mime_types:\n486:             return False, f\"Disallowed MIME type: {content_type}\"\n487: \n488:         # Check actual payload size (not Content-Length header which can be spoofed)\n489:         payload = part.get_payload(decode=True)\n490:         if payload:\n491:             size_bytes = len(payload)\n492:             if size_bytes > self.settings.imap_max_attachment_size:\n493:                 size_mb = size_bytes / (1024 * 1024)\n494:                 max_mb = self.settings.imap_max_attachment_size / (1024 * 1024)\n495:                 return False, f\"File too large: {size_mb:.2f}MB (max {max_mb:.2f}MB)\"\n496: \n497:         return True, \"\"\n498: \n499:     async def _save_attachment(self, part) -> str:\n500:         \"\"\"\n501:         Save attachment to a temporary file in uploads directory.\n502: \n503:         Uses tempfile.mkstemp() in settings.uploads_dir with appropriate\n504:         extension based on filename.\n505: \n506:         Args:\n507:             part: Email message part (attachment)\n508: \n509:         Returns:\n510:             Path to the saved temp file\n511: \n512:         Raises:\n513:             Exception: If saving fails\n514:         \"\"\"\n515:         # Get filename and sanitize to prevent path traversal\n516:         filename = part.get_filename()\n517:         if not filename:\n518:             # Generate a filename with timestamp\n519:             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n520:             filename = f\"email_attachment_{timestamp}.dat\"\n521:         else:\n522:             # Sanitize filename to prevent path traversal attacks\n523:             filename = self._sanitize_filename(filename)\n524: \n525:         # Determine file extension\n526:         content_type = part.get_content_type()\n527:         ext_map = {\n528:             'application/pdf': '.pdf',\n529:             'text/plain': '.txt',\n530:             'text/markdown': '.md',\n531:             'application/vnd.openxmlformats-officedocument.wordprocessingml.document': '.docx',\n532:             'text/csv': '.csv',\n533:             'application/json': '.json',\n534:             'application/sql': '.sql',\n535:             'text/x-python': '.py',\n536:             'application/javascript': '.js',\n537:             'text/html': '.html',\n538:             'text/css': '.css',\n539:             'application/xml': '.xml',\n540:             'application/x-yaml': '.yaml',\n541:         }\n542:         ext = ext_map.get(content_type, '.bin')\n543: \n544:         # Ensure uploads directory exists\n545:         self.settings.uploads_dir.mkdir(parents=True, exist_ok=True)\n546: \n547:         # Create temp file\n548:         fd, temp_path = tempfile.mkstemp(\n549:             prefix=\"email_\",\n550:             suffix=ext,\n551:             dir=self.settings.uploads_dir\n552:         )\n553: \n554:         try:\n555:             # Write attachment content\n556:             payload = part.get_payload(decode=True)\n557:             if payload:\n558:                 os.write(fd, payload)\n559: \n560:             logger.debug(f\"Saved attachment to: {temp_path}\")\n561:             return temp_path\n562:         except Exception as e:\n563:             # Clean up temp file on error\n564:             try:\n565:                 os.close(fd)\n566:                 os.unlink(temp_path)\n567:             except Exception:\n568:                 pass\n569:             raise Exception(f\"Failed to save attachment: {e}\")\n570:         finally:\n571:             try:\n572:                 os.close(fd)\n573:             except Exception:\n574:                 pass\n575: \n576:     async def _resolve_vault_id(self, vault_name: Optional[str]) -> int:\n577:         \"\"\"\n578:         Resolve vault ID from vault name (case-insensitive).\n579: \n580:         Queries database for vault by name (case-insensitive LIKE).\n581:         Returns vault_id=1 (default vault) if not found or vault_name is None.\n582: \n583:         Args:\n584:             vault_name: Vault name to resolve, or None for default\n585: \n586:         Returns:\n587:             Vault ID (1 if not found or vault_name is None)\n588:         \"\"\"\n589:         if vault_name is None:\n590:             return 1\n591: \n592:         try:\n593:             conn = self.pool.get_connection()\n594:             try:\n595:                 # Case-insensitive search\n596:                 cursor = conn.execute(\n597:                     \"SELECT id FROM vaults WHERE LOWER(name) = LOWER(?)\",\n598:                     (vault_name,)\n599:                 )\n600:                 row = cursor.fetchone()\n601:                 if row:\n602:                     vault_id = row[\"id\"]\n603:                     logger.debug(f\"Resolved vault '{vault_name}' to id={vault_id}\")\n604:                     return vault_id\n605:                 else:\n606:                     logger.warning(f\"Vault '{vault_name}' not found, using default vault (id=1)\")\n607:                     return 1\n608:             finally:\n609:                 self.pool.release_connection(conn)\n610:         except Exception as e:\n611:             logger.error(f\"Error resolving vault ID for '{vault_name}': {e}\")\n612:             return 1\n613: \n614:     def _decode_header_value(self, value: str) -> str:\n615:         \"\"\"\n616:         Decode email header value (handles encoded words).\n617: \n618:         Args:\n619:             value: Raw header value\n620: \n621:         Returns:\n622:             Decoded header value as string\n623:         \"\"\"\n624:         if not value:\n625:             return \"\"\n626: \n627:         decoded_parts = []\n628:         for part, encoding in decode_header(value):\n629:             if isinstance(part, bytes):\n630:                 try:\n631:                     decoded_parts.append(part.decode(encoding or 'utf-8', errors='replace'))\n632:                 except (LookupError, UnicodeDecodeError):\n633:                     decoded_parts.append(part.decode('utf-8', errors='replace'))\n634:             else:\n635:                 decoded_parts.append(str(part))\n636: \n637:         return ''.join(decoded_parts)\n638: \n639:     def _sanitize_html(self, html: str) -> str:\n640:         \"\"\"\n641:         Sanitize HTML content using bleach.\n642: \n643:         Args:\n644:             html: Raw HTML content\n645: \n646:         Returns:\n647:             Sanitized HTML with allowed tags only\n648:         \"\"\"\n649:         return bleach.clean(\n650:             html,\n651:             tags=ALLOWED_TAGS,\n652:             strip=True,\n653:             strip_comments=True\n654:         )\n655: \n\n(End of file - total 655 lines)\n</content>","timestamp":1771494766921,"originalBytes":26484}