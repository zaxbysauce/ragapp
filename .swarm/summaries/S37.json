{"id":"S37","summaryText":"[SUMMARY S37] 31.5 KB | code | 696 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\embeddings.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Dual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n3: \"\"\"\n→ Use /swarm retrieve S37 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\embeddings.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: Dual-provider embedding client service supporting Ollama and OpenAI-compatible APIs.\n3: \"\"\"\n4: import asyncio\n5: import httpx\n6: import logging\n7: from typing import List\n8: from urllib.parse import urlparse\n9: from app.config import settings\n10: \n11: logger = logging.getLogger(__name__)\n12: \n13: \n14: class EmbeddingError(Exception):\n15:     \"\"\"Exception raised for embedding service errors.\"\"\"\n16:     pass\n17: \n18: \n19: class EmbeddingService:\n20:     \"\"\"Service for generating text embeddings via Ollama or OpenAI-compatible APIs.\"\"\"\n21:     \n22:     # Hard caps for input validation\n23:     MAX_BATCH_SIZE = 512  # Maximum number of texts per batch call\n24:     MAX_TEXT_LENGTH = 8192  # Maximum characters per text (derived from chunk_size_chars=8192)\n25:     MIN_SPLIT_CHARS = 200  # Minimum text length to attempt single-text splitting\n26: \n27:     def __init__(self):\n28:         \"\"\"Initialize the embedding service with HTTP client and provider detection.\"\"\"\n29:         base_url = settings.ollama_embedding_url\n30: \n31:         # Validate base_url\n32:         if not base_url:\n33:             raise EmbeddingError(\"Embedding service is not configured\")\n34:         if not base_url.startswith(('http://', 'https://')):\n35:             raise EmbeddingError(\"Invalid embedding URL configuration\")\n36: \n37:         # Detect provider mode based on URL path\n38:         self.provider_mode, self.embeddings_url = self._detect_provider_mode(base_url)\n39:         self.timeout = 60.0\n40:         \n41:         # Read embedding prefixes from settings\n42:         self.embedding_doc_prefix = settings.embedding_doc_prefix\n43:         self.embedding_query_prefix = settings.embedding_query_prefix\n44:         \n45:         # Auto-apply Qwen3 instruction prefixes for better retrieval quality\n46:         # With llama.cpp -ub 8192, we have plenty of headroom for these prefixes\n47:         if settings.embedding_model.lower().find(\"qwen\") >= 0:\n48:             if not self.embedding_doc_prefix:\n49:                 self.embedding_doc_prefix = \"Instruct: Represent this technical documentation passage for retrieval.\\nDocument: \"\n50:             if not self.embedding_query_prefix:\n51:                 self.embedding_query_prefix = \"Instruct: Retrieve relevant technical documentation passages.\\nQuery: \"\n52:     \n53:     def _detect_provider_mode(self, base_url: str) -> tuple:\n54:         \"\"\"\n55:         Detect which embedding provider mode to use based on URL path.\n56:         \n57:         Detection strategy:\n58:         - If URL path includes '/api/embeddings' -> Ollama mode\n59:         - If URL path includes '/v1/embeddings' -> OpenAI mode\n60:         - If no explicit embeddings path:\n61:           - Port 1234 -> OpenAI mode (LM Studio default)\n62:           - Otherwise -> Ollama mode\n63:         \n64:         Args:\n65:             base_url: The configured embedding URL\n66:             \n67:         Returns:\n68:             Tuple of (provider_mode, embeddings_url)\n69:         \"\"\"\n70:         parsed = urlparse(base_url)\n71:         path = parsed.path\n72:         \n73:         # Check for explicit paths\n74:         if '/api/embeddings' in path:\n75:             # Already has Ollama path, use as-is\n76:             return ('ollama', base_url)\n77:         elif '/v1/embeddings' in path:\n78:             # Already has OpenAI path, use as-is\n79:             return ('openai', base_url)\n80:         \n81:         # No explicit path - determine by port\n82:         port = parsed.port\n83:         if port == 1234:\n84:             # LM Studio default port - use OpenAI mode\n85:             base_url = base_url.rstrip('/') + '/v1/embeddings'\n86:             return ('openai', base_url)\n87:         else:\n88:             # Default to Ollama mode\n89:             base_url = base_url.rstrip('/') + '/api/embeddings'\n90:             return ('ollama', base_url)\n91:     \n92:     def _build_payload(self, text: str) -> dict:\n93:         \"\"\"\n94:         Build the API request payload based on provider mode.\n95:         \n96:         Args:\n97:             text: The text to embed\n98:             \n99:         Returns:\n100:             Dictionary payload for the API request\n101:         \"\"\"\n102:         if self.provider_mode == 'openai':\n103:             return {\n104:                 \"model\": settings.embedding_model,\n105:                 \"input\": text\n106:             }\n107:         else:  # ollama mode\n108:             return {\n109:                 \"model\": settings.embedding_model,\n110:                 \"prompt\": text\n111:             }\n112:     \n113:     def _extract_embedding(self, data: dict) -> List[float]:\n114:         \"\"\"\n115:         Extract embedding vector from API response based on provider mode.\n116:         \n117:         Args:\n118:             data: Parsed JSON response from the API\n119:             \n120:         Returns:\n121:             List of float values representing the embedding vector\n122:             \n123:         Raises:\n124:             EmbeddingError: If embedding cannot be extracted\n125:         \"\"\"\n126:         if self.provider_mode == 'openai':\n127:             # OpenAI format: data[0].embedding\n128:             if \"data\" not in data:\n129:                 logger.error(\"Embedding API response missing 'data' field in OpenAI mode\")\n130:                 raise EmbeddingError(\"Embedding API response is invalid\")\n131:             if not isinstance(data[\"data\"], list) or len(data[\"data\"]) == 0:\n132:                 logger.error(\"Embedding API response 'data' field is empty or invalid in OpenAI mode\")\n133:                 raise EmbeddingError(\"Embedding API response is invalid\")\n134:             embedding = data[\"data\"][0].get(\"embedding\")\n135:             if embedding is None:\n136:                 logger.error(\"Embedding API response missing 'data[0].embedding' field in OpenAI mode\")\n137:                 raise EmbeddingError(\"Embedding API response is invalid\")\n138:         else:  # ollama mode\n139:             # Ollama format: embedding\n140:             embedding = data.get(\"embedding\")\n141:             if embedding is None:\n142:                 logger.error(\"Embedding API response missing 'embedding' field in Ollama mode\")\n143:                 raise EmbeddingError(\"Embedding API response is invalid\")\n144:         \n145:         return embedding\n146:     \n147:     async def embed_single(self, text: str) -> List[float]:\n148:         \"\"\"\n149:         Generate embedding for a single text.\n150: \n151:         Applies the query prefix (if configured) to the input text before embedding.\n152:         The query prefix is used for retrieval queries and must remain constant for\n153:         a given index to ensure consistent embedding space.\n154: \n155:         Args:\n156:             text: The text to embed.\n157: \n158:         Returns:\n159:             List of float values representing the embedding vector.\n160: \n161:         Raises:\n162:             EmbeddingError: If the API request fails or returns non-200 status.\n163:         \"\"\"\n164:         # Validate text input\n165:         if text is None:\n166:             raise EmbeddingError(\"Text cannot be None\")\n167:         if not text.strip():\n168:             raise EmbeddingError(\"Text cannot be empty or whitespace only\")\n169: \n170:         # Apply query prefix for retrieval queries\n171:         text_to_embed = self.embedding_query_prefix + text if self.embedding_query_prefix else text\n172: \n173:         async with httpx.AsyncClient(timeout=self.timeout) as client:\n174:             try:\n175:                 response = await client.post(\n176:                     self.embeddings_url,\n177:                     json=self._build_payload(text_to_embed)\n178:                 )\n179:                 \n180:                 if response.status_code != 200:\n181:                     logger.warning(\n182:                         f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n183:                     )\n184:                     raise EmbeddingError(\n185:                         f\"Embedding API returned status {response.status_code}\"\n186:                     )\n187: \n188:                 try:\n189:                     data = response.json()\n190:                 except ValueError as e:\n191:                     logger.warning(\n192:                         f\"Invalid JSON response from embedding API for {self.provider_mode} mode: {e}, response: {response.text}\"\n193:                     )\n194:                     raise EmbeddingError(\"Invalid response from embedding service\")\n195: \n196:                 return self._extract_embedding(data)\n197:                 \n198:             except httpx.TimeoutException as e:\n199:                 raise EmbeddingError(\"Embedding request timed out\")\n200:             except httpx.HTTPError as e:\n201:                 raise EmbeddingError(\"Embedding HTTP error occurred\")\n202:     \n203:     async def validate_embedding_dimension(self, expected_dim: int) -> bool:\n204:         \"\"\"\n205:         Validate that the embedding dimension matches the expected value.\n206: \n207:         Args:\n208:             expected_dim: The expected dimension of the embedding vector.\n209:                 Must be a positive integer.\n210: \n211:         Returns:\n212:             True if the dimension matches.\n213: \n214:         Raises:\n215:             EmbeddingError: If expected_dim is invalid or if the dimension\n216:                 does not match the expected value.\n217:         \"\"\"\n218:         # Validate expected_dim input\n219:         if expected_dim is None:\n220:             raise EmbeddingError(\"expected_dim cannot be None\")\n221:         if not isinstance(expected_dim, int) or expected_dim <= 0:\n222:             raise EmbeddingError(f\"expected_dim must be a positive integer, got {expected_dim}\")\n223: \n224:         embedding = await self.embed_single('dimension_check')\n225:         actual_dim = len(embedding)\n226:         if actual_dim != expected_dim:\n227:             raise EmbeddingError(\n228:                 f\"Embedding dimension mismatch: expected {expected_dim}, got {actual_dim}\"\n229:             )\n230:         return True\n231: \n232:     async def embed_batch(self, texts: List[str], batch_size: int | None = None) -> List[List[float]]:\n233:         \"\"\"\n234:         Generate embeddings for a batch of texts using true API batching.\n235: \n236:         Sends multiple texts per API request for efficient GPU utilization.\n237:         Processes in batches of up to 512 (configurable) with up to 4\n238:         concurrent batch requests.\n239: \n240:         Applies the document prefix (if configured) to each input text before embedding.\n241:         The document prefix is used for document embeddings and must remain constant for\n242:         a given index to ensure consistent embedding space.\n243: \n244:         Args:\n245:             texts: List of texts to embed.\n246:             batch_size: Number of texts per API request (default: 512).\n247: \n248:         Returns:\n249:             List of embedding vectors, one for each input text, in order.\n250: \n251:         Raises:\n252:             EmbeddingError: If any API request fails.\n253:         \"\"\"\n254:         if not texts:\n255:             return []\n256:         \n257:         # Input validation guards\n258:         for idx, text in enumerate(texts):\n259:             if text is None:\n260:                 raise EmbeddingError(f\"Text at index {idx} is None\")\n261:             if not text.strip():\n262:                 raise EmbeddingError(f\"Text at index {idx} is empty or whitespace only\")\n263:             if len(text) > self.MAX_TEXT_LENGTH:\n264:                 raise EmbeddingError(\n265:                     f\"Text at index {idx} exceeds maximum length ({self.MAX_TEXT_LENGTH} characters)\"\n266:                 )\n267:         \n268:         # Use configured batch size if not specified\n269:         if batch_size is None:\n270:             batch_size = settings.embedding_batch_size\n271:         \n272:         # Clamp batch_size to valid range\n273:         batch_size = max(1, min(batch_size, self.MAX_BATCH_SIZE))\n274:         \n275:         # Apply document prefix to all texts\n276:         texts_to_embed = []\n277:         for text in texts:\n278:             if self.embedding_doc_prefix:\n279:                 texts_to_embed.append(self.embedding_doc_prefix + text)\n280:             else:\n281:                 texts_to_embed.append(text)\n282:         \n283:         # Process in batches using true API batching\n284:         all_embeddings: List[List[float]] = []\n285:         for i in range(0, len(texts_to_embed), batch_size):\n286:             batch = texts_to_embed[i:i + batch_size]\n287:             embeddings = await self._embed_batch_api(batch)\n288:             all_embeddings.extend(embeddings)\n289:         \n290:         return all_embeddings\n291: \n292:     async def _embed_batch_api(self, texts: List[str]) -> List[List[float]]:\n293:         \"\"\"\n294:         Send a batch of texts to the embedding API in a single request.\n295:         \n296:         Implements adaptive batching: automatically retries with smaller sub-batches\n297:         when llama.cpp token overflow errors occur.\n298: \n299:         Args:\n300:             texts: List of texts to embed (already prefixed).\n301: \n302:         Returns:\n303:             List of embedding vectors in the same order as input texts.\n304: \n305:         Raises:\n306:             EmbeddingError: If the API request fails after all retries.\n307:         \"\"\"\n308:         max_retries = settings.embedding_batch_max_retries\n309:         min_sub_size = settings.embedding_batch_min_sub_size\n310:         \n311:         async with httpx.AsyncClient(timeout=self.timeout) as client:\n312:             return await self._embed_batch_with_retry(client, texts, max_retries, min_sub_size)\n313:     \n314:     async def _embed_batch_with_retry(self, client: httpx.AsyncClient, texts: List[str], \n315:                                       max_retries: int, min_sub_size: int, retry_count: int = 0) -> List[List[float]]:\n316:         \"\"\"\n317:         Internal method that handles the retry logic for adaptive batching.\n318:         \n319:         Args:\n320:             client: HTTP client for making requests\n321:             texts: List of texts to embed\n322:             max_retries: Maximum number of retry attempts\n323:             min_sub_size: Minimum sub-batch size before giving up\n324:             \n325:         Returns:\n326:             List of embedding vectors\n327:             \n328:         Raises:\n329:             EmbeddingError: If all retries fail\n330:         \"\"\"\n331:         # Empty-input guard\n332:         if not texts:\n333:             return []\n334:         \n335:         try:\n336:             # Build payload with array of inputs\n337:             if self.provider_mode == 'openai':\n338:                 payload = {\n339:                     \"model\": settings.embedding_model,\n340:                     \"input\": texts\n341:                 }\n342:             else:  # ollama mode\n343:                 payload = {\n344:                     \"model\": settings.embedding_model,\n345:                     \"input\": texts\n346:                 }\n347:             \n348:             response = await client.post(\n349:                 self.embeddings_url,\n350:                 json=payload\n351:             )\n352:             \n353:             # Check for token overflow error in HTTP 500 responses\n354:             if response.status_code == 500:\n355:                 error_text = response.text.lower()\n356:                 if self._is_token_overflow_error(error_text):\n357:                     logger.warning(\n358:                         f\"Token overflow error for {self.provider_mode} mode: {response.text}\"\n359:                     )\n360:                     # Handle overflow using the shared helper\n361:                     return await self._handle_overflow_retry(\n362:                         client, texts, max_retries, min_sub_size, retry_count\n363:                     )\n364:             \n365:             if response.status_code != 200:\n366:                 logger.warning(\n367:                     f\"Embedding API returned status {response.status_code} for {self.provider_mode} mode: {response.text}\"\n368:                 )\n369:                 raise EmbeddingError(\n370:                     f\"Embedding API returned status {response.status_code}\"\n371:                 )\n372: \n373:             data = response.json()\n374:             \n375:             # Extract embeddings from response\n376:             if self.provider_mode == 'openai':\n377:                 # OpenAI format: data[].embedding\n378:                 embeddings = [item['embedding'] for item in data['data']]\n379:             else:\n380:                 # Ollama format may vary - try common formats\n381:                 if 'embeddings' in data:\n382:                     embeddings = data['embeddings']\n383:                 elif 'embedding' in data:\n384:                     # Single embedding returned - shouldn't happen with batch\n385:                     embeddings = [data['embedding']]\n386:                 else:\n387:                     logger.error(\n388:                         f\"Unexpected response format for {self.provider_mode} mode: {data.keys()}\"\n389:                     )\n390:                     raise EmbeddingError(\"Unexpected response from embedding service\")\n391:             \n392:             # Validate embedding structure\n393:             if not isinstance(embeddings, list):\n394:                 logger.error(\"Embedding API response 'embeddings' is not a list\")\n395:                 raise EmbeddingError(\"Embedding API response is invalid\")\n396:             for i, emb in enumerate(embeddings):\n397:                 if not isinstance(emb, list):\n398:                     logger.error(f\"Embedding at index {i} is not a list\")\n399:                     raise EmbeddingError(\"Embedding API response is invalid\")\n400:                 for j, val in enumerate(emb):\n401:                     if not isinstance(val, (int, float)):\n402:                         logger.error(f\"Embedding value at [{i}][{j}] is not a number\")\n403:                         raise EmbeddingError(\"Embedding API response is invalid\")\n404:             \n405:             # Validate embedding count matches input count\n406:             if len(embeddings) != len(texts):\n407:                 logger.error(\n408:                     f\"Embedding count mismatch for {self.provider_mode} mode: expected {len(texts)}, got {len(embeddings)}\"\n409:                 )\n410:                 raise EmbeddingError(\n411:                     f\"Embedding count mismatch: expected {len(texts)}, got {len(embeddings)}\"\n412:                 )\n413:             \n414:             return embeddings\n415:             \n416:         except httpx.TimeoutException as e:\n417:             logger.warning(\n418:                 f\"Embedding batch request timed out for {self.provider_mode} mode: {e}\"\n419:             )\n420:             # For multi-item batches: split and retry each half so the server\n421:             # gets smaller workloads — same recovery strategy as token overflow.\n422:             if len(texts) > 1 and retry_count < max_retries:\n423:                 logger.info(\n424:                     f\"Timeout with {len(texts)} items — splitting batch and retrying \"\n425:                     f\"(attempt {retry_count + 1}/{max_retries})\"\n426:                 )\n427:                 return await self._handle_overflow_retry(\n428:                     client, texts, max_retries, min_sub_size, retry_count\n429:                 )\n430:             # For single-item batches: simple backoff retry\n431:             if retry_count < max_retries:\n432:                 backoff_delay = min(0.5 * (2 ** retry_count), 2.0)\n433:                 logger.info(f\"Retrying single-item timeout (attempt {retry_count + 1}/{max_retries}) after {backoff_delay}s\")\n434:                 await asyncio.sleep(backoff_delay)\n435:                 return await self._embed_batch_with_retry(\n436:                     client, texts, max_retries, min_sub_size, retry_count + 1\n437:                 )\n438:             raise EmbeddingError(f\"Embedding request timed out after {max_retries} retries\")\n439:         except httpx.HTTPError as e:\n440:             # Check if this is a token overflow error\n441:             error_msg = str(e)\n442:             response_text = \"\"\n443:             \n444:             # Try to get response text from the exception if available\n445:             try:\n446:                 resp = getattr(e, 'response', None)\n447:                 if resp is not None:\n448:                     response_text = resp.text.lower()\n449:             except Exception:\n450:                 pass\n451:             \n452:             if self._is_token_overflow_error(error_msg) or self._is_token_overflow_error(response_text):\n453:                 logger.warning(\n454:                     f\"Token overflow error for {self.provider_mode} mode: {response_text}\"\n455:                 )\n456:                 # Handle overflow using the shared helper\n457:                 return await self._handle_overflow_retry(\n458:                     client, texts, max_retries, min_sub_size, retry_count\n459:                 )\n460:             else:\n461:                 # Not a token overflow error, re-raise\n462:                 logger.error(\n463:                     f\"Embedding batch HTTP error for {self.provider_mode} mode: {e}\"\n464:                 )\n465:                 raise EmbeddingError(\"Embedding batch HTTP error occurred\")\n466: \n467:     def _split_text_at_midpoint(self, text: str) -> tuple:\n468:         \"\"\"\n469:         Split a single text into two parts at a boundary-aware midpoint.\n470:         \n471:         Prefers splitting at newline or space characters near the midpoint\n472:         to produce more natural splits. Falls back to strict midpoint if\n473:         boundary-aware split would result in empty sides.\n474:         \n475:         Args:\n476:             text: The text to split\n477:             \n478:         Returns:\n479:             Tuple of (left_text, right_text), both non-empty for splittable text\n480:         \"\"\"\n481:         if len(text) <= 1:\n482:             return (text, \"\")\n483:         \n484:         midpoint = len(text) // 2\n485:         \n486:         # Try to find a better boundary near the midpoint\n487:         # Look for newline first, then space\n488:         search_start = max(0, midpoint - 50)\n489:         search_end = min(len(text), midpoint + 50)\n490:         \n491:         # Search for newline near midpoint (forward)\n492:         for i in range(midpoint, search_end):\n493:             if text[i] == '\\n':\n494:                 left, right = text[:i+1], text[i+1:]\n495:                 # Ensure both sides are non-empty for splittable text\n496:                 if left and right:\n497:                     return (left, right)\n498:         \n499:         # Search for newline near midpoint (backward)\n500:         for i in range(midpoint - 1, search_start - 1, -1):\n501:             if text[i] == '\\n':\n502:                 left, right = text[:i+1], text[i+1:]\n503:                 if left and right:\n504:                     return (left, right)\n505:         \n506:         # Search for space near midpoint (forward)\n507:         for i in range(midpoint, search_end):\n508:             if text[i] == ' ':\n509:                 left, right = text[:i], text[i:]\n510:                 if left and right:\n511:                     return (left, right)\n512:         \n513:         # Search for space near midpoint (backward)\n514:         for i in range(midpoint - 1, search_start - 1, -1):\n515:             if text[i] == ' ':\n516:                 left, right = text[:i], text[i:]\n517:                 if left and right:\n518:                     return (left, right)\n519:         \n520:         # Fall back to strict midpoint (guaranteed non-empty for len > 1)\n521:         left, right = text[:midpoint], text[midpoint:]\n522:         # Double-check: if either is empty, adjust to ensure both non-empty\n523:         if not left:\n524:             left = text[:1]\n525:             right = text[1:]\n526:         elif not right:\n527:             right = text[-1:]\n528:             left = text[:-1]\n529:         \n530:         return (left, right)\n531:     \n532:     def _mean_pool_embeddings(self, emb1: List[float], emb2: List[float]) -> List[float]:\n533:         \"\"\"\n534:         Mean-pool two embedding vectors into one.\n535:         \n536:         Args:\n537:             emb1: First embedding vector\n538:             emb2: Second embedding vector\n539:             \n540:         Returns:\n541:             Mean-pooled embedding vector\n542:         \"\"\"\n543:         if len(emb1) != len(emb2):\n544:             raise EmbeddingError(\n545:                 f\"Cannot mean-pool embeddings of different dimensions: {len(emb1)} vs {len(emb2)}\"\n546:             )\n547:         \n548:         return [(a + b) / 2.0 for a, b in zip(emb1, emb2)]\n549:     \n550:     async def _handle_overflow_retry(self, client: httpx.AsyncClient, texts: List[str],\n551:                                      max_retries: int, min_sub_size: int, retry_count: int) -> List[List[float]]:\n552:         \"\"\"\n553:         Helper method to handle overflow retry logic with bounded retries and minimum split size.\n554:         \n555:         For single-item overflow, attempts to split the text and mean-pool the results.\n556:         For multi-item overflow, splits the batch and processes each half.\n557:         \n558:         Args:\n559:             client: HTTP client for making requests\n560:             texts: List of texts to embed\n561:             max_retries: Maximum number of retry attempts\n562:             min_sub_size: Minimum sub-batch size before giving up\n563:             retry_count: Current retry attempt count\n564:             \n565:         Returns:\n566:             List of embedding vectors\n567:             \n568:         Raises:\n569:             EmbeddingError: If bounded retries exhausted or split size too small\n570:         \"\"\"\n571:         # Check if we've exhausted retries\n572:         if retry_count > max_retries:\n573:             logger.error(\n574:                 f\"Max retries ({max_retries}) exhausted for embedding batch in {self.provider_mode} mode\"\n575:             )\n576:             raise EmbeddingError(\n577:                 f\"Max retries ({max_retries}) exhausted for embedding batch\"\n578:             )\n579:         \n580:         # Handle single-item overflow with text splitting\n581:         if len(texts) == 1:\n582:             single_text = texts[0]\n583:             \n584:             # Check if text is too short to split - raise actionable error\n585:             if len(single_text) < self.MIN_SPLIT_CHARS:\n586:                 logger.warning(\n587:                     f\"Single input ({len(single_text)} chars) is below minimum split threshold ({self.MIN_SPLIT_CHARS}) in {self.provider_mode} mode\"\n588:                 )\n589:                 raise EmbeddingError(\n590:                     f\"Single input ({len(single_text)} chars) exceeds token limit and is too short to split. \"\n591:                     f\"Ensure chunk_size_chars is below server batch size limit (minimum {self.MIN_SPLIT_CHARS} chars required for recovery).\"\n592:                 )\n593:             \n594:             # Split text at boundary-aware midpoint and recurse\n595:             left_text, right_text = self._split_text_at_midpoint(single_text)\n596:             \n597:             # Guard: if either side is empty after split, raise actionable error\n598:             if not left_text or not right_text:\n599:                 logger.error(\n600:                     f\"Text split produced empty side (left={len(left_text)}, right={len(right_text)}) for text of length {len(single_text)}\"\n601:                 )\n602:                 raise EmbeddingError(\n603:                     f\"Cannot split text for embedding recovery: split produced empty segment. \"\n604:                     f\"Ensure chunk_size_chars is within server limits.\"\n605:                 )\n606:             \n607:             logger.info(\n608:                 f\"Splitting single input ({len(single_text)} chars) into parts ({len(left_text)} + {len(right_text)} chars), retry {retry_count}\"\n609:             )\n610:             \n611:             # Small bounded async backoff\n612:             backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n613:             await asyncio.sleep(backoff_delay)\n614:             \n615:             # Recurse on each part with incremented retry count\n616:             left_embeddings = await self._embed_batch_with_retry(\n617:                 client, [left_text], max_retries, min_sub_size, retry_count=retry_count + 1\n618:             )\n619:             right_embeddings = await self._embed_batch_with_retry(\n620:                 client, [right_text], max_retries, min_sub_size, retry_count=retry_count + 1\n621:             )\n622:             \n623:             # Mean-pool the two embeddings into one\n624:             logger.debug(\"Using mean-pooling for overflow recovery\")\n625:             pooled = self._mean_pool_embeddings(left_embeddings[0], right_embeddings[0])\n626:             \n627:             # Return single embedding to preserve one-embedding-per-input contract\n628:             return [pooled]\n629:         \n630:         # Multi-item batch overflow - use existing split behavior\n631:         # Check if we've reached minimum split size\n632:         if len(texts) <= min_sub_size:\n633:             logger.warning(\n634:                 f\"Cannot split batch further in {self.provider_mode} mode: {len(texts)} items below minimum split size ({min_sub_size})\"\n635:             )\n636:             raise EmbeddingError(\n637:                 f\"Cannot split batch further: {len(texts)} items below minimum split size\"\n638:             )\n639:         \n640:         # Split at midpoint and recurse with backoff\n641:         midpoint = len(texts) // 2\n642:         left_texts = texts[:midpoint]\n643:         right_texts = texts[midpoint:]\n644:         \n645:         # Small bounded async backoff (exponential, capped at 1s)\n646:         backoff_delay = min(0.5 * (2 ** retry_count), 1.0)\n647:         await asyncio.sleep(backoff_delay)\n648:         \n649:         # Recurse on left then right to preserve order\n650:         left_embeddings = await self._embed_batch_with_retry(\n651:             client, left_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n652:         )\n653:         right_embeddings = await self._embed_batch_with_retry(\n654:             client, right_texts, max_retries, min_sub_size, retry_count=retry_count + 1\n655:         )\n656:         \n657:         return left_embeddings + right_embeddings\n658:     \n659:     def _is_token_overflow_error(self, error_msg: str) -> bool:\n660:         \"\"\"\n661:         Detect if an error message indicates a token overflow from llama.cpp.\n662:         \n663:         Args:\n664:             error_msg: The error message string\n665:             \n666:         Returns:\n667:             True if this is a token overflow error, False otherwise\n668:         \"\"\"\n669:         error_lower = error_msg.lower()\n670:         \n671:         # Check for common llama.cpp token overflow patterns\n672:         # Pattern 1: \"input (X tokens) is too large\" - typical llama.cpp error\n673:         if \"input (\" in error_lower and \"tokens) is too large\" in error_lower:\n674:             return True\n675:         \n676:         # Pattern 2: \"too large to process\" with \"current batch size\" - OpenAI mode error\n677:         if \"too large to process\" in error_lower and \"current batch size\" in error_lower:\n678:             return True\n679:         \n680:         # Pattern 3: \"token limit exceeded\"\n681:         if \"token limit exceeded\" in error_lower:\n682:             return True\n683:         \n684:         # Pattern 4: \"batch size too small\"\n685:         if \"batch size too small\" in error_lower:\n686:             return True\n687:         \n688:         return False\n689: \n690: \n691: \n\n(End of file - total 691 lines)\n</content>","timestamp":1771960450246,"originalBytes":32225}