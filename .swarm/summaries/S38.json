{"id":"S38","summaryText":"[SUMMARY S38] 29.7 KB | code | 701 lines\n<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\vector_store.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: LanceDB vector store service for semantic search.\n3: \"\"\"\n→ Use /swarm retrieve S38 for full content","fullOutput":"<path>C:\\opencode\\RAGAPPv2\\backend\\app\\services\\vector_store.py</path>\n<type>file</type>\n<content>1: \"\"\"\n2: LanceDB vector store service for semantic search.\n3: \"\"\"\n4: from pathlib import Path\n5: from typing import Any, Dict, List, Optional\n6: import lancedb\n7: import pyarrow as pa\n8: import numpy as np\n9: import logging\n10: \n11: from app.config import settings\n12: \n13: logger = logging.getLogger(__name__)\n14: \n15: logger = logging.getLogger(__name__)\n16: \n17: \n18: class VectorStoreError(Exception):\n19:     \"\"\"Custom exception for vector store errors.\"\"\"\n20:     pass\n21: \n22: \n23: class VectorStoreConnectionError(VectorStoreError):\n24:     \"\"\"Exception raised when connection to LanceDB fails.\"\"\"\n25:     pass\n26: \n27: \n28: class VectorStoreValidationError(VectorStoreError):\n29:     \"\"\"Exception raised when record validation fails.\"\"\"\n30:     pass\n31: \n32: \n33: class VectorStore:\n34:     \"\"\"LanceDB-based vector store for document chunk embeddings.\"\"\"\n35:     \n36:     def __init__(self, db_path: Optional[Path] = None):\n37:         \"\"\"\n38:         Initialize the vector store.\n39:         \n40:         Args:\n41:             db_path: Path to LanceDB database. Defaults to settings.lancedb_path.\n42:         \"\"\"\n43:         self.db_path = db_path or settings.lancedb_path\n44:         self.db: Optional[lancedb.DBConnection] = None\n45:         self.table: Optional[lancedb.table.Table] = None\n46:         self._embedding_dim: Optional[int] = None\n47:     \n48:     def connect(self) -> \"VectorStore\":\n49:         \"\"\"Connect to LanceDB.\n50:         \n51:         Raises:\n52:             VectorStoreConnectionError: If connection to LanceDB fails.\n53:         \"\"\"\n54:         try:\n55:             self.db = lancedb.connect(str(self.db_path))\n56:         except Exception as e:\n57:             raise VectorStoreConnectionError(f\"Failed to connect to LanceDB at {self.db_path}: {e}\") from e\n58:         return self\n59:     \n60:     def init_table(self, embedding_dim: int) -> \"VectorStore\":\n61:         \"\"\"\n62:         Initialize or open the 'chunks' table.\n63:         \n64:         Args:\n65:             embedding_dim: Dimension of embedding vectors.\n66:             \n67:         Returns:\n68:             Self for method chaining.\n69:             \n70:         Raises:\n71:             VectorStoreConnectionError: If connection or table operations fail.\n72:         \"\"\"\n73:         if self.db is None:\n74:             self.connect()\n75:         \n76:         if self.db is None:\n77:             raise VectorStoreConnectionError(\"Database connection is not available.\")\n78:         \n79:         self._embedding_dim = embedding_dim\n80:         \n81:         # Define schema for chunks table\n82:         schema = pa.schema([\n83:             (\"id\", pa.string()),\n84:             (\"text\", pa.string()),\n85:             (\"file_id\", pa.string()),\n86:             (\"vault_id\", pa.string()),  # Vault isolation\n87:             (\"chunk_index\", pa.int32()),\n88:             (\"metadata\", pa.string()),  # JSON string for flexibility\n89:             (\"embedding\", pa.list_(pa.float32(), embedding_dim)),\n90:         ])\n91:         \n92:         # Create or open table with error handling\n93:         try:\n94:             if \"chunks\" in self.db.table_names():\n95:                 try:\n96:                     self.table = self.db.open_table(\"chunks\")\n97:                 except Exception:\n98:                     # Stale table reference — drop and recreate\n99:                     try:\n100:                         self.db.drop_table(\"chunks\")\n101:                     except Exception:\n102:                         pass\n103:                     self.table = self.db.create_table(\"chunks\", schema=schema, mode=\"overwrite\")\n104:             else:\n105:                 self.table = self.db.create_table(\"chunks\", schema=schema)\n106:         except Exception as e:\n107:             raise VectorStoreConnectionError(f\"Failed to initialize 'chunks' table: {e}\") from e\n108:         \n109:         return self\n110:     \n111:     def _get_expected_embedding_dim(self) -> Optional[int]:\n112:         \"\"\"Get the expected embedding dimension from the table schema.\"\"\"\n113:         if self.table is None:\n114:             return self._embedding_dim\n115:         \n116:         try:\n117:             schema = self.table.schema\n118:             embedding_field = schema.field(\"embedding\")\n119:             if hasattr(embedding_field.type, 'list_size'):\n120:                 return embedding_field.type.list_size\n121:         except Exception:\n122:             pass\n123:         return self._embedding_dim\n124:     \n125:     def add_chunks(self, records: List[Dict[str, Any]]) -> None:\n126:         \"\"\"\n127:         Add chunk records to the vector store.\n128:         \n129:         Args:\n130:             records: List of records with keys: id, text, file_id, chunk_index, \n131:                      metadata, embedding, vault_id (optional, defaults to \"1\").\n132:                      \n133:         Raises:\n134:             RuntimeError: If table is not initialized.\n135:             VectorStoreValidationError: If records validation fails.\n136:         \"\"\"\n137:         if self.table is None:\n138:             raise RuntimeError(\"Table not initialized. Call init_table() first.\")\n139:         \n140:         # Handle empty records\n141:         if not records:\n142:             return\n143:         \n144:         # Get expected embedding dimension from table schema\n145:         expected_dim = self._get_expected_embedding_dim()\n146:         \n147:         # Required fields for validation\n148:         required_fields = [\"id\", \"text\", \"file_id\", \"chunk_index\", \"embedding\"]\n149:         \n150:         # Convert records to arrow-compatible format\n151:         processed_records = []\n152:         for record in records:\n153:             # Validate required fields\n154:             missing_fields = [field for field in required_fields if field not in record]\n155:             if missing_fields:\n156:                 raise VectorStoreValidationError(\n157:                     f\"Record missing required fields: {', '.join(missing_fields)}\"\n158:                 )\n159:             \n160:             # Ensure embedding is a list (convert from numpy if needed)\n161:             embedding = record[\"embedding\"]\n162:             if isinstance(embedding, np.ndarray):\n163:                 embedding = embedding.tolist()\n164:             elif not isinstance(embedding, list):\n165:                 raise VectorStoreValidationError(\n166:                     f\"Embedding must be a list or numpy array, got {type(embedding).__name__}\"\n167:                 )\n168:             \n169:             # Validate embedding dimension matches table schema\n170:             actual_dim = len(embedding)\n171:             if expected_dim is not None and actual_dim != expected_dim:\n172:                 raise VectorStoreValidationError(\n173:                     f\"Embedding dimension mismatch: expected {expected_dim} dimensions, \"\n174:                     f\"got {actual_dim}. The table was created with a different embedding model. \"\n175:                     f\"Delete the lancedb directory at {self.db_path} and restart to use the new model.\"\n176:                 )\n177:             \n178:             processed_record = {\n179:                 \"id\": record[\"id\"],\n180:                 \"text\": record[\"text\"],\n181:                 \"file_id\": record[\"file_id\"],\n182:                 \"vault_id\": record.get(\"vault_id\", \"1\"),  # Default to vault \"1\"\n183:                 \"chunk_index\": record[\"chunk_index\"],\n184:                 \"metadata\": record.get(\"metadata\", \"{}\"),\n185:                 \"embedding\": embedding,\n186:             }\n187:             processed_records.append(processed_record)\n188:         \n189:         self.table.add(processed_records)\n190:     \n191:     def search(\n192:         self,\n193:         embedding: List[float],\n194:         limit: int = 10,\n195:         filter_expr: Optional[str] = None,\n196:         vault_id: Optional[str] = None\n197:     ) -> List[Dict[str, Any]]:\n198:         \"\"\"\n199:         Search for similar chunks by embedding.\n200: \n201:         Args:\n202:             embedding: Query embedding vector.\n203:             limit: Maximum number of results.\n204:             filter_expr: Optional filter expression (LanceDB syntax).\n205:             vault_id: Optional vault ID to filter results. If provided, only returns\n206:                      chunks from the specified vault.\n207: \n208:         Returns:\n209:             List of matching records with similarity scores. Each record includes:\n210:             - All original fields (id, text, file_id, chunk_index, metadata, etc.)\n211:             - _distance: Cosine distance from query embedding (lower = more similar)\n212:             Empty list if no table exists.\n213:             \n214:         Note:\n215:             For cosine distance metric:\n216:             - Distance of 0 = identical vectors (perfect match)\n217:             - Distance of 1 = orthogonal vectors\n218:             - Distance of 2 = opposite vectors (perfect mismatch)\n219:             The _distance field is provided by LanceDB's vector search.\n220:         \"\"\"\n221:         # Ensure DB connection exists\n222:         if self.db is None:\n223:             self.connect()\n224:         \n225:         # Try to open existing table if not already loaded\n226:         if self.table is None:\n227:             try:\n228:                 table_names = self.db.table_names()\n229:             except Exception as e:\n230:                 raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n231:             \n232:             if \"chunks\" not in table_names:\n233:                 # No table exists yet - graceful no-docs behavior\n234:                 return []\n235:             \n236:             # Table exists, try to open it\n237:             try:\n238:                 self.table = self.db.open_table(\"chunks\")\n239:             except Exception as e:\n240:                 raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n241:             \n242:             # Set embedding_dim from table schema if available\n243:             if self._embedding_dim is None:\n244:                 try:\n245:                     schema = self.table.schema\n246:                     embedding_field = schema.field(\"embedding\")\n247:                     # Extract dimension from fixed size list type\n248:                     if hasattr(embedding_field.type, 'list_size'):\n249:                         self._embedding_dim = embedding_field.type.list_size\n250:                 except Exception:\n251:                     # If we can't determine embedding_dim, leave it as None\n252:                     pass\n253:         \n254:         query = self.table.search(embedding, metric=\"cosine\")\n255: \n256:         # Apply vault filter if specified\n257:         if vault_id is not None:\n258:             safe_vault_id = str(vault_id).replace(\"'\", \"\\\\'\")\n259:             vault_filter = f\"vault_id = '{safe_vault_id}'\"\n260:             if filter_expr:\n261:                 filter_expr = f\"({filter_expr}) AND ({vault_filter})\"\n262:             else:\n263:                 filter_expr = vault_filter\n264: \n265:         if filter_expr:\n266:             query = query.where(filter_expr)\n267:         \n268:         results = query.limit(limit).to_list()\n269:         return results\n270:     \n271:     def delete_by_file(self, file_id: str) -> int:\n272:         \"\"\"\n273:         Delete all chunks for a given file_id.\n274:         \n275:         Args:\n276:             file_id: The file ID to delete chunks for.\n277:             \n278:         Returns:\n279:             Number of records deleted.\n280:         \"\"\"\n281:         # Ensure DB connection exists\n282:         if self.db is None:\n283:             self.connect()\n284:         \n285:         # Try to open existing table if not already loaded\n286:         if self.table is None:\n287:             try:\n288:                 table_names = self.db.table_names()\n289:             except Exception as e:\n290:                 raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n291:             \n292:             if \"chunks\" not in table_names:\n293:                 # No table exists yet - nothing to delete\n294:                 return 0\n295:             \n296:             # Table exists, try to open it\n297:             try:\n298:                 self.table = self.db.open_table(\"chunks\")\n299:             except Exception as e:\n300:                 raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n301:             \n302:             # Set embedding_dim from table schema if available\n303:             if self._embedding_dim is None:\n304:                 try:\n305:                     schema = self.table.schema\n306:                     embedding_field = schema.field(\"embedding\")\n307:                     # Extract dimension from fixed size list type\n308:                     if hasattr(embedding_field.type, 'list_size'):\n309:                         self._embedding_dim = embedding_field.type.list_size\n310:                 except Exception:\n311:                     # If we can't determine embedding_dim, leave it as None\n312:                     pass\n313: \n314:         # Query count before delete to return accurate deletion count\n315:         safe_file_id = str(file_id).replace('\"', '\\\\\"')\n316:         try:\n317:             count_before = self.table.count_rows(f'file_id = \"{safe_file_id}\"')\n318:         except Exception:\n319:             # If count_rows fails, safely default to 0\n320:             count_before = 0\n321: \n322:         # LanceDB delete using filter expression\n323:         self.table.delete(f'file_id = \"{safe_file_id}\"')\n324: \n325:         return count_before\n326: \n327:     def delete_by_vault(self, vault_id: str) -> int:\n328:         \"\"\"\n329:         Delete all chunks for a given vault_id.\n330: \n331:         Args:\n332:             vault_id: The vault ID to delete all chunks for.\n333: \n334:         Returns:\n335:             Number of records deleted.\n336:         \"\"\"\n337:         # Ensure DB connection exists\n338:         if self.db is None:\n339:             self.connect()\n340: \n341:         # Try to open existing table if not already loaded\n342:         if self.table is None:\n343:             try:\n344:                 table_names = self.db.table_names()\n345:             except Exception as e:\n346:                 raise VectorStoreConnectionError(f\"Failed to list table names: {e}\") from e\n347: \n348:             if \"chunks\" not in table_names:\n349:                 return 0\n350: \n351:             try:\n352:                 self.table = self.db.open_table(\"chunks\")\n353:             except Exception as e:\n354:                 raise VectorStoreConnectionError(f\"Failed to open 'chunks' table: {e}\") from e\n355: \n356:         safe_vault_id = str(vault_id).replace(\"'\", \"\\\\'\")\n357:         try:\n358:             count_before = self.table.count_rows(f\"vault_id = '{safe_vault_id}'\")\n359:         except Exception:\n360:             count_before = 0\n361: \n362:         self.table.delete(f\"vault_id = '{safe_vault_id}'\")\n363:         return count_before\n364: \n365:     def migrate_add_vault_id(self) -> int:\n366:         \"\"\"\n367:         Migration: Backfill vault_id='1' on existing chunks that lack it.\n368: \n369:         LanceDB doesn't support ALTER TABLE or UPDATE, so this reads all data,\n370:         adds the vault_id field, and rewrites the table. This is idempotent —\n371:         safe to call multiple times (no-op if all records already have vault_id).\n372: \n373:         Returns:\n374:             Number of records migrated. 0 if no migration was needed.\n375:         \"\"\"\n376:         if self.db is None:\n377:             self.connect()\n378: \n379:         if self.db is None:\n380:             logger.info(\"LanceDB vault_id migration: no connection available\")\n381:             return 0\n382: \n383:         try:\n384:             table_names = self.db.table_names()\n385:         except Exception as e:\n386:             logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n387:             return 0\n388: \n389:         if \"chunks\" not in table_names:\n390:             logger.info(\"LanceDB vault_id migration: no table exists\")\n391:             return 0\n392: \n393:         try:\n394:             table = self.db.open_table(\"chunks\")\n395:         except Exception as e:\n396:             logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n397:             return 0\n398: \n399:         # Check if vault_id column exists in schema\n400:         schema = table.schema\n401:         field_names = [schema.field(i).name for i in range(len(schema))]\n402: \n403:         if \"vault_id\" in field_names:\n404:             # Column exists — check if any rows have null vault_id\n405:             try:\n406:                 df = table.to_pandas()\n407:                 null_count = df[\"vault_id\"].isna().sum()\n408:                 if null_count == 0:\n409:                     logger.info(\"LanceDB vault_id migration: no migration needed\")\n410:                     return 0  # All records already have vault_id\n411: \n412:                 # Backfill null vault_ids with \"1\"\n413:                 df[\"vault_id\"] = df[\"vault_id\"].fillna(\"1\")\n414:                 count = int(null_count)\n415: \n416:                 # Drop and recreate table with updated data\n417:                 self.db.drop_table(\"chunks\")\n418:                 try:\n419:                     self.table = self.db.create_table(\"chunks\", data=df)\n420:                 except Exception as create_err:\n421:                     logger.critical(f\"LanceDB vault_id migration: table dropped but recreate failed: {create_err}. Data may need manual recovery from backup.\")\n422:                     raise\n423:                 logger.info(f\"LanceDB vault_id migration: backfilled {count} records\")\n424:                 return count\n425:             except Exception as e:\n426:                 logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n427:                 return 0\n428:         else:\n429:             # Column doesn't exist — add it to all records\n430:             try:\n431:                 df = table.to_pandas()\n432:                 if len(df) == 0:\n433:                     # Empty table — just drop and recreate with new schema\n434:                     # Try to get embedding_dim from existing schema before dropping\n435:                     if self._embedding_dim is None:\n436:                         try:\n437:                             embedding_field = table.schema.field(\"embedding\")\n438:                             if hasattr(embedding_field.type, 'list_size'):\n439:                                 self._embedding_dim = embedding_field.type.list_size\n440:                         except Exception:\n441:                             pass\n442: \n443:                     self.db.drop_table(\"chunks\")\n444:                     try:\n445:                         if self._embedding_dim:\n446:                             self.init_table(self._embedding_dim)\n447:                     except Exception as create_err:\n448:                         logger.critical(f\"LanceDB vault_id migration: empty table dropped but recreate failed: {create_err}\")\n449:                         raise\n450:                     logger.info(\"LanceDB vault_id migration: empty table, recreated with new schema\")\n451:                     return 0\n452: \n453:                 # Add vault_id column with default \"1\"\n454:                 df[\"vault_id\"] = \"1\"\n455:                 migrated_count = len(df)\n456: \n457:                 # Drop and recreate table with updated data\n458:                 self.db.drop_table(\"chunks\")\n459:                 try:\n460:                     self.table = self.db.create_table(\"chunks\", data=df)\n461:                 except Exception as create_err:\n462:                     logger.critical(f\"LanceDB vault_id migration: table dropped but recreate failed: {create_err}. Data may need manual recovery from backup.\")\n463:                     raise\n464:                 logger.info(f\"LanceDB vault_id migration: backfilled {migrated_count} records\")\n465:                 return migrated_count\n466:             except Exception as e:\n467:                 logger.warning(f\"LanceDB vault_id migration failed: {e}\")\n468:                 return 0\n469: \n470:     def get_chunks_by_uid(self, chunk_uids: List[str]) -> List[Dict[str, Any]]:\n471:         \"\"\"\n472:         Fetch chunks by their unique IDs.\n473:         \n474:         Args:\n475:             chunk_uids: List of chunk UIDs in format \"{file_id}_{chunk_index}\"\n476:             \n477:         Returns:\n478:             List of matching chunk records from LanceDB.\n479:         \"\"\"\n480:         if self.table is None:\n481:             return []\n482:         \n483:         if not chunk_uids:\n484:             return []\n485:         \n486:         try:\n487:             # Build IN clause for chunk_uids\n488:             # Each uid is in format \"{file_id}_{chunk_index}\"\n489:             # Escape single quotes in uids for SQL-like syntax\n490:             escaped_uids = [uid.replace(\"'\", \"''\") for uid in chunk_uids]\n491:             quoted_uids = [f\"'{uid}'\" for uid in escaped_uids]\n492:             uid_list = \", \".join(quoted_uids)\n493:             \n494:             # Query chunks where id is in the list of chunk_uids\n495:             query = f\"id IN ({uid_list})\"\n496:             results = self.table.search() \\\n497:                 .where(query) \\\n498:                 .to_list()\n499:             \n500:             return results\n501:         except Exception as e:\n502:             logger.warning(f\"Failed to fetch chunks by UID: {e}\")\n503:             return []\n504:     \n505:     def get_stats(self) -> Dict[str, Any]:\n506:         \"\"\"\n507:         Get statistics about the vector store.\n508:         \n509:         Returns:\n510:             Dictionary with stats like total chunks, embedding dimension.\n511:         \"\"\"\n512:         if self.table is None:\n513:             return {\"total_chunks\": 0, \"embedding_dim\": self._embedding_dim}\n514:         \n515:         return {\n516:             \"total_chunks\": self.table.count_rows(),\n517:             \"embedding_dim\": self._embedding_dim,\n518:         }\n519:     \n520:     def close(self) -> None:\n521:         \"\"\"Close the database connection.\"\"\"\n522:         # LanceDB connections are typically stateless\n523:         self.db = None\n524:         self.table = None\n525:     \n526:     def get_stored_metadata(self) -> Optional[Dict[str, Any]]:\n527:         \"\"\"\n528:         Get stored metadata from the table's metadata.\n529:         \n530:         Returns:\n531:             Dictionary with stored metadata (embedding_model_id, embedding_dim, embedding_prefix_hash)\n532:             or None if table doesn't exist or no metadata is stored.\n533:         \"\"\"\n534:         if self.table is None:\n535:             return None\n536:         \n537:         try:\n538:             # Try to get table metadata\n539:             table_metadata = self.table.schema.metadata\n540:             if table_metadata:\n541:                 # Convert bytes keys/values to strings if needed\n542:                 metadata = {}\n543:                 for key, value in table_metadata.items():\n544:                     if isinstance(key, bytes):\n545:                         key = key.decode('utf-8')\n546:                     if isinstance(value, bytes):\n547:                         value = value.decode('utf-8')\n548:                     metadata[key] = value\n549:                 \n550:                 # Extract our stored fields\n551:                 result = {}\n552:                 if b'embedding_model_id' in table_metadata or 'embedding_model_id' in metadata:\n553:                     result['embedding_model_id'] = metadata.get('embedding_model_id')\n554:                 if b'embedding_dim' in table_metadata or 'embedding_dim' in metadata:\n555:                     result['embedding_dim'] = int(metadata.get('embedding_dim', 0))\n556:                 if b'embedding_prefix_hash' in table_metadata or 'embedding_prefix_hash' in metadata:\n557:                     result['embedding_prefix_hash'] = metadata.get('embedding_prefix_hash')\n558:                 \n559:                 if result:\n560:                     return result\n561:         except Exception as e:\n562:             logger.debug(f\"Failed to read table metadata: {e}\")\n563:         \n564:         return None\n565:     \n566:     def validate_schema(self, embedding_model_id: str, embedding_dim: int) -> Dict[str, Any]:\n567:         \"\"\"\n568:         Validate that the table schema matches the current embedding configuration.\n569:         \n570:         Args:\n571:             embedding_model_id: The embedding model identifier\n572:             embedding_dim: The expected embedding dimension\n573:             \n574:         Returns:\n575:             Dictionary with validation results\n576:             \n577:         Raises:\n578:             VectorStoreValidationError: If embedding dimension mismatch is detected\n579:         \"\"\"\n580:         # Generate a probe embedding for \"dimension_probe\" text\n581:         probe_text = \"dimension_probe\"\n582:         try:\n583:             probe_embedding = self._generate_probe_embedding(probe_text, embedding_dim)\n584:         except Exception as e:\n585:             logger.warning(f\"Failed to generate probe embedding: {e}\")\n586:             probe_embedding = None\n587:         \n588:         # Get expected dimension from the provided parameter\n589:         expected_dim = embedding_dim\n590:         \n591:         # Check if table exists\n592:         table_exists = False\n593:         if self.db is not None:\n594:             try:\n595:                 table_names = self.db.table_names()\n596:                 table_exists = \"chunks\" in table_names\n597:             except Exception as e:\n598:                 logger.warning(f\"Failed to check table existence: {e}\")\n599:         \n600:         stored_metadata = None\n601:         if table_exists:\n602:             try:\n603:                 if self.table is None:\n604:                     self.table = self.db.open_table(\"chunks\")\n605:                 \n606:                 # Get schema and compare vector dimension\n607:                 schema = self.table.schema\n608:                 embedding_field = schema.field(\"embedding\")\n609:                 actual_dim = None\n610:                 if hasattr(embedding_field.type, 'list_size'):\n611:                     actual_dim = embedding_field.type.list_size\n612:                 \n613:                 if actual_dim is not None and actual_dim != expected_dim:\n614:                     error_msg = f\"Embedding dimension changed from {actual_dim} to {expected_dim}; reindex required.\"\n615:                     logger.error(error_msg)\n616:                     raise VectorStoreValidationError(error_msg)\n617:                 \n618:                 # Get stored metadata\n619:                 stored_metadata = self.get_stored_metadata()\n620:                 \n621:             except VectorStoreValidationError:\n622:                 raise\n623:             except Exception as e:\n624:                 logger.warning(f\"Failed to validate schema: {e}\")\n625:         \n626:         # Prepare metadata to store\n627:         import hashlib\n628:         prefix_hash = hashlib.sha256(embedding_model_id.encode('utf-8')).hexdigest()[:16]\n629:         \n630:         metadata_to_store = {\n631:             'embedding_model_id': embedding_model_id,\n632:             'embedding_dim': str(expected_dim),\n633:             'embedding_prefix_hash': prefix_hash\n634:         }\n635:         \n636:         # Update table metadata if table exists\n637:         if table_exists and self.table is not None:\n638:             try:\n639:                 # Get existing metadata\n640:                 current_metadata = dict(self.table.schema.metadata) if self.table.schema.metadata else {}\n641:                 \n642:                 # Update with our metadata\n643:                 for key, value in metadata_to_store.items():\n644:                     if isinstance(value, str):\n645:                         current_metadata[key.encode('utf-8')] = value.encode('utf-8')\n646:                     else:\n647:                         current_metadata[key.encode('utf-8')] = str(value).encode('utf-8')\n648:                 \n649:                 # Note: LanceDB doesn't support direct metadata update on existing table\n650:                 # We'll log the metadata that should be stored for future reference\n651:                 logger.info(f\"Table metadata to store/update: {metadata_to_store}\")\n652:                 \n653:             except Exception as e:\n654:                 logger.warning(f\"Failed to update table metadata: {e}\")\n655:         \n656:         return {\n657:             'table_exists': table_exists,\n658:             'expected_dim': expected_dim,\n659:             'actual_dim': expected_dim if table_exists else None,\n660:             'stored_metadata': stored_metadata,\n661:             'probe_embedding_generated': probe_embedding is not None,\n662:             'metadata_to_store': metadata_to_store\n663:         }\n664:     \n665:     def _generate_probe_embedding(self, text: str, dim: int) -> List[float]:\n666:         \"\"\"\n667:         Generate a probe embedding for dimension validation.\n668:         \n669:         Args:\n670:             text: The text to generate embedding for\n671:             dim: Expected dimension\n672:             \n673:         Returns:\n674:             Generated embedding vector\n675:         \"\"\"\n676:         # Use a deterministic hash-based approach for probe embedding\n677:         import hashlib\n678:         import random\n679:         \n680:         # Create a deterministic seed from the text\n681:         seed_value = int(hashlib.md5(text.encode('utf-8')).hexdigest(), 16) % (2**32)\n682:         random.seed(seed_value)\n683:         \n684:         # Generate a random vector of expected dimension\n685:         # This simulates what a real embedding would look like\n686:         probe = [random.gauss(0, 1) for _ in range(dim)]\n687:         \n688:         # Normalize the vector (typical for embeddings)\n689:         magnitude = sum(x*x for x in probe) ** 0.5\n690:         if magnitude > 0:\n691:             probe = [x / magnitude for x in probe]\n692:         \n693:         return probe\n694: \n695: \n696: \n\n(End of file - total 696 lines)\n</content>","timestamp":1771960457129,"originalBytes":30447}