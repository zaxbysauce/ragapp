"""
Application configuration using Pydantic Settings.
"""
import logging
from pathlib import Path
from pydantic import SecretStr, field_validator, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

logger = logging.getLogger(__name__)


class Settings(BaseSettings):
    """Application settings with environment variable support."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore"
    )
    
    # Server configuration
    port: int = 8080
    
    # Base data directory
    data_dir: Path = Path("/data/knowledgevault")
    
    # Ollama configuration
    ollama_embedding_url: str = "http://host.docker.internal:11434"
    ollama_chat_url: str = "http://host.docker.internal:11434"
    
    # Model configuration
    embedding_model: str = "nomic-embed-text"
    chat_model: str = "qwen2.5:32b"
    
    # Embedding dimension (auto-detected from model, but can be overridden)
    embedding_dim: int = 768
    
    # Document processing configuration (character-based - NEW)
    chunk_size_chars: int | None = None
    """Character-based chunk size for document processing. Default 1200 chars (~300 tokens) leaves room for instruction prefix."""
    chunk_overlap_chars: int | None = None
    """Character-based overlap between chunks. Default 120 chars (~30 tokens)."""
    document_parsing_strategy: str = "fast"
    """Document parsing strategy for unstructured.io: 'fast' (fastest), 'hi_res' (best quality), 'auto' (automatic selection)."""
    retrieval_top_k: int | None = None
    """Number of top chunks to retrieve (unifies max_context_chunks and vector_top_k)."""
    vector_metric: str = "cosine"
    """Distance metric for vector similarity search."""
    max_distance_threshold: float = 0.5
    """Maximum distance threshold for relevance filtering (replaces rag_relevance_threshold).
    
    For cosine distance: 0=identical, 1=orthogonal, 2=opposite.
    0.5 is a good default balance between precision and recall.
    Can be overridden via MAX_DISTANCE_THRESHOLD env var.
    """
    embedding_doc_prefix: str = ""
    """Prefix to prepend to documents during embedding."""
    embedding_query_prefix: str = ""
    """Prefix to prepend to queries during embedding."""
    retrieval_window: int = 1
    """Window size for retrieval context expansion."""
    embedding_batch_size: int = 512
    """Number of texts to send per embedding API request. Higher = better GPU utilization."""
    embedding_batch_max_retries: int = 3
    """Maximum number of retries for adaptive batching when token overflow occurs."""
    embedding_batch_min_sub_size: int = 1
    """Minimum sub-batch size for adaptive batching fallback."""

    # ── Reranker configuration ────────────────────────────────────────────────
    reranker_url: str = ""
    """TEI-compatible reranker endpoint URL. Empty = use sentence-transformers locally."""
    reranker_model: str = "BAAI/bge-reranker-v2-m3"
    """HuggingFace model ID for local reranking, or model name sent to TEI endpoint."""
    reranking_enabled: bool = False
    """Enable cross-encoder reranking after vector retrieval."""
    reranker_top_n: int = 5
    """Number of chunks to keep after reranking."""
    initial_retrieval_top_k: int = 20
    """Chunks fetched from vector store BEFORE reranking."""

    # ── Hybrid search configuration ─────────────────────────────────────────
    hybrid_search_enabled: bool = True
    """Combine BM25 keyword search with dense vector search using RRF fusion."""
    hybrid_alpha: float = 0.5
    """Weight for dense vs sparse scores in RRF. 0.0 = pure BM25, 1.0 = pure dense."""

    # ── Contextual chunking configuration ─────────────────────────────────────
    contextual_chunking_enabled: bool = False
    """Enable LLM-based contextual chunking (prepends document context to each chunk)."""
    contextual_chunking_concurrency: int = 5
    """Maximum concurrent LLM calls for contextual chunking."""

    # ── Multi-scale chunk indexing configuration ──────────────────────────────
    multi_scale_indexing_enabled: bool = False
    """Enable multi-scale chunk indexing (index chunks at multiple sizes for varied recall)."""
    multi_scale_chunk_sizes: str = "512,1024,2048"
    """Comma-separated list of chunk sizes (in characters) for multi-scale indexing."""
    multi_scale_overlap_ratio: float = 0.1
    """Overlap ratio between adjacent chunks at each scale (0.0-1.0)."""

    # ── Query transformation configuration ────────────────────────────────────
    query_transformation_enabled: bool = False
    """Enable query transformation using step-back prompting for broader retrieval."""

    # ── Retrieval evaluation configuration ────────────────────────────────────
    retrieval_evaluation_enabled: bool = False
    """Enable CRAG-style retrieval evaluation (CONFIDENT/AMBIGUOUS/NO_MATCH classification)."""

    # ── Tri-vector embedding configuration (BGE-M3) ────────────────────────────
    tri_vector_search_enabled: bool = False
    """Enable BGE-M3 tri-vector embeddings (dense + sparse). Requires FlagEmbedding server."""
    flag_embedding_url: str = "http://embedding-server:18080"
    """URL of the FlagEmbedding server for BGE-M3 tri-vector embeddings."""

    # Document processing configuration (legacy - DEPRECATED)
    chunk_size: int | None = None
    """[DEPRECATED] Token-based chunk size. Use chunk_size_chars instead."""
    chunk_overlap: int | None = None
    """[DEPRECATED] Token-based chunk overlap. Use chunk_overlap_chars instead."""
    max_context_chunks: int = 10
    """[DEPRECATED] Number of context chunks. Use retrieval_top_k instead."""

    # RAG configuration (legacy - DEPRECATED)
    rag_relevance_threshold: float | None = None
    """[DEPRECATED] Relevance threshold. Use max_distance_threshold instead."""
    vector_top_k: int | None = None
    """[DEPRECATED] Vector top K. Use retrieval_top_k instead."""
    maintenance_mode: bool = False
    redis_url: str = "redis://localhost:6379/0"
    csrf_token_ttl: int = 900
    admin_rate_limit: str = "10/minute"
    health_check_api_key: str = "health-api-key"
    
    # Auto-scan configuration
    auto_scan_enabled: bool = True
    auto_scan_interval_minutes: int = 60
    
    # Logging configuration
    log_level: str = "INFO"

    # Feature flags
    enable_model_validation: bool = False

    # Admin security
    admin_secret_token: str = "admin-secret-token"
    audit_hmac_key_version: str = "v1"

    # Security settings
    max_file_size_mb: int = 50
    allowed_extensions: set[str] = {
        ".txt", ".md", ".pdf", ".docx", ".csv", ".json",
        ".sql", ".py", ".js", ".ts", ".html", ".css",
        ".xml", ".yaml", ".yml"
    }

    # IMAP Email Ingestion configuration
    imap_enabled: bool = False
    imap_host: str = ""
    imap_port: int = 993
    imap_username: str = ""
    imap_password: SecretStr = SecretStr("")
    imap_use_ssl: bool = True
    imap_mailbox: str = "INBOX"
    imap_poll_interval: int = 60  # seconds
    imap_max_attachment_size: int = 10 * 1024 * 1024  # 10MB
    imap_allowed_mime_types: set[str] = {
        "application/pdf",
        "text/plain",
        "text/markdown",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "text/csv",
        "application/json",
        "application/sql",
        "text/x-python",
        "application/javascript",
        "text/html",
        "text/css",
        "application/xml",
        "application/x-yaml",
    }

    # CORS settings
    backend_cors_origins: list[str] = ["http://localhost:5173"]

    # Migration validators for backward compatibility
    @field_validator("chunk_size_chars", mode="before")
    @classmethod
    def migrate_chunk_size_chars(cls, v: int | None, values) -> int:
        """Auto-convert from legacy chunk_size if chunk_size_chars not provided."""
        if v is not None:
            return v
        legacy_chunk_size = values.data.get("chunk_size")
        if legacy_chunk_size is not None:
            logger.warning(
                "Deprecated: 'chunk_size' is deprecated. Use 'chunk_size_chars' instead. "
                f"Auto-converting chunk_size={legacy_chunk_size} to chunk_size_chars={legacy_chunk_size * 4}."
            )
            return legacy_chunk_size * 4
        return 2000  # ~500 tokens with llama.cpp -ub 8192 batch size

    @field_validator("chunk_overlap_chars", mode="before")
    @classmethod
    def migrate_chunk_overlap_chars(cls, v: int | None, values) -> int:
        """Auto-convert from legacy chunk_overlap if chunk_overlap_chars not provided."""
        if v is not None:
            return v
        legacy_chunk_overlap = values.data.get("chunk_overlap")
        if legacy_chunk_overlap is not None:
            logger.warning(
                "Deprecated: 'chunk_overlap' is deprecated. Use 'chunk_overlap_chars' instead. "
                f"Auto-converting chunk_overlap={legacy_chunk_overlap} to chunk_overlap_chars={legacy_chunk_overlap * 4}."
            )
            return legacy_chunk_overlap * 4
        return 200  # ~50 tokens overlap

    @field_validator("retrieval_top_k", mode="before")
    @classmethod
    def migrate_retrieval_top_k(cls, v: int | None, values) -> int:
        """Auto-convert from legacy vector_top_k if retrieval_top_k not provided."""
        if v is not None:
            return v
        legacy_vector_top_k = values.data.get("vector_top_k")
        if legacy_vector_top_k is not None:
            logger.warning(
                "Deprecated: 'vector_top_k' is deprecated. Use 'retrieval_top_k' instead. "
                f"Auto-copying vector_top_k={legacy_vector_top_k} to retrieval_top_k={legacy_vector_top_k}."
            )
            return legacy_vector_top_k
        return 12

    @field_validator("embedding_batch_max_retries", mode="after")
    @classmethod
    def validate_embedding_batch_max_retries(cls, v: int) -> int:
        """Validate embedding batch max retries is an integer in range 0..10."""
        if v < 0 or v > 10:
            raise ValueError("embedding_batch_max_retries must be in range 0..10")
        return v

    @field_validator("embedding_batch_min_sub_size", mode="after")
    @classmethod
    def validate_embedding_batch_min_sub_size(cls, v: int) -> int:
        """Validate embedding batch minimum sub-size is an integer >= 1."""
        if v < 1:
            raise ValueError("embedding_batch_min_sub_size must be >= 1")
        return v

    @field_validator("embedding_batch_size", mode="after")
    @classmethod
    def validate_embedding_batch_size(cls, v: int) -> int:
        """Validate embedding batch size is >= 1."""
        if v < 1:
            raise ValueError("embedding_batch_size must be >= 1")
        return v

    @field_validator("document_parsing_strategy", mode="after")
    @classmethod
    def validate_document_parsing_strategy(cls, v: str) -> str:
        """Validate document parsing strategy is one of: fast, hi_res, auto."""
        allowed = {"fast", "hi_res", "auto"}
        if v not in allowed:
            raise ValueError(f"document_parsing_strategy must be one of: {', '.join(sorted(allowed))}")
        return v

    @field_validator("multi_scale_chunk_sizes", mode="after")
    @classmethod
    def validate_multi_scale_chunk_sizes(cls, v: str) -> str:
        """Validate multi_scale_chunk_sizes is a comma-separated list of unique positive integers."""
        sizes = [int(x.strip()) for x in v.split(",") if x.strip()]
        if not sizes:
            raise ValueError("multi_scale_chunk_sizes cannot be empty")
        unique_sizes = sorted(set(sizes))
        if len(unique_sizes) != len(sizes):
            raise ValueError("multi_scale_chunk_sizes must contain unique values")
        for size in unique_sizes:
            if size <= 0:
                raise ValueError("multi_scale_chunk_sizes must contain only positive integers")
        return ",".join(str(x) for x in unique_sizes)

    @field_validator("multi_scale_overlap_ratio", mode="after")
    @classmethod
    def validate_multi_scale_overlap_ratio(cls, v: float) -> float:
        """Validate multi_scale_overlap_ratio is in range 0.0-1.0."""
        if v < 0.0 or v > 1.0:
            raise ValueError("multi_scale_overlap_ratio must be in range 0.0-1.0")
        return v

    @model_validator(mode="after")
    def validate_batch_config_consistency(self) -> "Settings":
        """Validate embedding batch configuration consistency."""
        if self.embedding_batch_min_sub_size > self.embedding_batch_size:
            raise ValueError(
                "embedding_batch_min_sub_size must be <= embedding_batch_size"
            )
        return self

    @property
    def documents_dir(self) -> Path:
        """Directory for storing documents."""
        return self.data_dir / "documents"
    
    @property
    def uploads_dir(self) -> Path:
        """Directory for temporary uploads."""
        return self.data_dir / "uploads"
    
    @property
    def vaults_dir(self) -> Path:
        """Directory for vault-specific data."""
        path = self.data_dir / "vaults"
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    @property
    def orphan_vault_id(self) -> int:
        """Default vault ID for files not associated with a specific vault."""
        return 1
    
    @property
    def library_dir(self) -> Path:
        """Directory for library files."""
        return self.data_dir / "library"
    
    @property
    def lancedb_path(self) -> Path:
        """Path to LanceDB database."""
        return self.data_dir / "lancedb"
    
    @property
    def sqlite_path(self) -> Path:
        """Path to SQLite database."""
        return self.data_dir / "app.db"

    @property
    def effective_embedding_doc_prefix(self) -> str:
        """Effective document prefix for embedding. BGE-M3 doesn't use prefixes."""
        if self.embedding_doc_prefix:
            return self.embedding_doc_prefix
        # Auto-apply Qwen3 instruction prefix only for Qwen models
        if "qwen" in self.embedding_model.lower():
            return "Instruct: Represent this technical documentation passage for retrieval.\nDocument: "
        return ""

    @property
    def effective_embedding_query_prefix(self) -> str:
        """Effective query prefix for embedding. BGE-M3 doesn't use prefixes."""
        if self.embedding_query_prefix:
            return self.embedding_query_prefix
        if "qwen" in self.embedding_model.lower():
            return "Instruct: Retrieve relevant technical documentation passages.\nQuery: "
        return ""


# Global settings instance
settings = Settings()
