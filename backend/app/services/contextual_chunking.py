"""
Contextual chunking service using LLM to generate context prefixes for document chunks.

This service enhances chunks with context generated by an LLM to improve retrieval
quality by providing document-level context to each chunk.
"""

import asyncio
import logging
from typing import List

from app.config import settings
from app.services.chunking import ProcessedChunk
from app.services.llm_client import LLMClient, LLMError

logger = logging.getLogger(__name__)


class ContextualChunker:
    """
    Service for adding LLM-generated context to document chunks.
    
    Generates a short (1-2 sentence) context prefix for each chunk using an LLM,
    improving retrieval quality by providing document-level context.
    
    The service is stateless except for the concurrency semaphore, which limits
    the number of concurrent LLM calls to prevent overwhelming the LLM server.
    """

    # Maximum document length before truncation
    _MAX_DOCUMENT_LENGTH = 100_000
    # Characters to keep from start when truncating
    _TRUNCATE_CHARS = 50_000
    # Default concurrency limit
    _DEFAULT_CONCURRENCY = 5
    # Maximum tokens for context generation
    _MAX_TOKENS = 150

    def __init__(self, llm_client: LLMClient):
        """
        Initialize the contextual chunker.
        
        Args:
            llm_client: LLMClient instance for generating context.
                        Must be started before calling contextualize_chunks.
        """
        self._llm_client = llm_client
        # Get concurrency limit from settings, falling back to default
        concurrency = getattr(
            settings, 
            'contextual_chunking_concurrency', 
            self._DEFAULT_CONCURRENCY
        )
        self._semaphore = asyncio.Semaphore(concurrency)
        logger.info(
            f"ContextualChunker initialized with concurrency limit: {concurrency}"
        )

    def _truncate_document(self, document_text: str) -> str:
        """
        Truncate document text if it exceeds the maximum length.
        
        When the document exceeds _MAX_DOCUMENT_LENGTH characters, keeps the first
        _TRUNCATE_CHARS characters and the last _TRUNCATE_CHARS characters,
        with a [...] marker in between.
        
        Args:
            document_text: The full document text to potentially truncate.
            
        Returns:
            The document text, truncated if necessary.
        """
        if len(document_text) <= self._MAX_DOCUMENT_LENGTH:
            return document_text
        
        truncated = (
            document_text[:self._TRUNCATE_CHARS] 
            + "\n\n[...truncated...]\n\n" 
            + document_text[-self._TRUNCATE_CHARS:]
        )
        logger.warning(
            f"Document truncated from {len(document_text)} to {len(truncated)} characters"
        )
        return truncated

    def _build_prompt(
        self, 
        document_text: str, 
        chunk_text: str, 
        chunk_index: int,
        total_chunks: int,
        source_filename: str
    ) -> List[dict]:
        """
        Build the prompt for generating chunk context.
        
        Creates a prompt that asks the LLM to generate a short (1-2 sentence)
        context description that helps understand where this chunk fits in the
        overall document.
        
        Args:
            document_text: The (potentially truncated) full document text.
            chunk_text: The text content of the chunk to contextualize.
            chunk_index: The index of the chunk in the document.
            total_chunks: Total number of chunks in the document.
            source_filename: The name of the source file.
            
        Returns:
            List of message dicts for LLM chat completion.
        """
        return [
            {
                "role": "system",
                "content": "You are a helpful assistant that generates brief context "
                           "descriptions for document chunks. Generate a short (1-2 sentence) "
                           "description that explains where this chunk fits in the document. "
                           "Be concise and focus on the main topic or section."
            },
            {
                "role": "user",
                "content": f"Source file: {source_filename}\n"
                           f"Chunk {chunk_index + 1} of {total_chunks}\n\n"
                           f"Full document (for context):\n"
                           f"{document_text}\n\n"
                           f"Chunk to contextualize:\n"
                           f"{chunk_text}\n\n"
                           f"Provide a brief context description (1-2 sentences) for this chunk."
            }
        ]

    async def contextualize_chunks(
        self, 
        document_text: str, 
        chunks: List[ProcessedChunk], 
        source_filename: str
    ) -> List[ProcessedChunk]:
        """
        Add LLM-generated context to each chunk in the document.
        
        For each chunk, generates a short (1-2 sentence) context prefix using
        the LLM and prepends it to the chunk text. Uses a semaphore to limit
        concurrent LLM calls.
        
        Args:
            document_text: The full document text.
            chunks: List of ProcessedChunk objects to contextualize.
            source_filename: The name of the source file for context.
            
        Returns:
            The same chunks list with context added to each chunk's text,
            with metadata['contextualized'] set to True.
        """
        if not chunks:
            logger.debug("No chunks to contextualize")
            return chunks
        
        # Truncate document if too long
        truncated_doc = self._truncate_document(document_text)
        
        total_chunks = len(chunks)
        logger.info(
            f"Contextualizing {total_chunks} chunks for file: {source_filename}"
        )
        
        # Process all chunks concurrently with semaphore limiting
        tasks = [
            self._contextualize_single_chunk(
                chunk=chunk,
                document_text=truncated_doc,
                chunk_index=idx,
                total_chunks=total_chunks,
                source_filename=source_filename
            )
            for idx, chunk in enumerate(chunks)
        ]
        
        await asyncio.gather(*tasks)
        
        logger.info(
            f"Completed contextualization for {total_chunks} chunks"
        )
        
        return chunks

    async def _contextualize_single_chunk(
        self,
        chunk: ProcessedChunk,
        document_text: str,
        chunk_index: int,
        total_chunks: int,
        source_filename: str
    ) -> None:
        """
        Generate and add context to a single chunk.
        
        Uses the LLM to generate a context prefix and prepends it to the
        chunk text. Always sets contextualized=True in metadata, even on failure.
        
        Args:
            chunk: The ProcessedChunk to modify.
            document_text: The truncated document text for context.
            chunk_index: The index of this chunk.
            total_chunks: Total number of chunks.
            source_filename: Source filename for the prompt.
        """
        async with self._semaphore:
            try:
                prompt = self._build_prompt(
                    document_text=document_text,
                    chunk_text=chunk.text,
                    chunk_index=chunk_index,
                    total_chunks=total_chunks,
                    source_filename=source_filename
                )
                
                context = await self._llm_client.chat_completion(
                    messages=prompt,
                    temperature=0.3,
                    max_tokens=self._MAX_TOKENS
                )
                
                # Strip whitespace and check if we got a valid response
                context = context.strip()
                
                if context:
                    # Prepend context to chunk text with two newlines
                    chunk.text = f"{context}\n\n{chunk.text}"
                    logger.debug(
                        f"Added context to chunk {chunk_index}: {context[:50]}..."
                    )
                else:
                    logger.warning(
                        f"Empty context response for chunk {chunk_index}"
                    )
                
            except LLMError as e:
                logger.warning(
                    f"LLM error generating context for chunk {chunk_index}: {e}"
                )
            except Exception as e:
                logger.warning(
                    f"Unexpected error generating context for chunk {chunk_index}: {e}"
                )
            finally:
                # Always mark as contextualized, even on failure
                chunk.metadata['contextualized'] = True
