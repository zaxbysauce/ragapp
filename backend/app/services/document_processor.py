"""
Document processing service with orchestration, status tracking, and deduplication.

Provides DocumentProcessor class that coordinates parsing, chunking, and schema extraction
while tracking processing status in SQLite and handling file deduplication.
"""

import asyncio
import json
import logging
import os
import sqlite3
from dataclasses import dataclass
from datetime import datetime, UTC
from pathlib import Path
from typing import List, Any, Optional, Tuple

from unstructured.partition.auto import partition

from ..config import settings
from ..models.database import SQLiteConnectionPool, get_pool
from ..utils.file_utils import compute_file_hash
from ..utils.retry import with_retry
from .chunking import SemanticChunker, ProcessedChunk
from .contextual_chunking import ContextualChunker
from .embeddings import EmbeddingService
from .llm_client import LLMClient
from .schema_parser import SchemaParser
from .vector_store import VectorStore

logger = logging.getLogger(__name__)


@dataclass
class ProcessedDocument:
    """
    Result of processing a document file.

    Attributes:
        file_id: The database ID of the processed file
        chunks: List of processed chunks from the document
    """
    file_id: int
    chunks: List[ProcessedChunk]


class DuplicateFileError(Exception):
    """Exception raised when a file with the same hash already exists and is indexed."""
    pass


class DocumentProcessingError(Exception):
    """Exception raised when document processing fails due to database errors."""
    pass


class DocumentParseError(Exception):
    """Exception raised when document parsing fails."""
    pass


class DocumentParser:
    """
    Parser for extracting text elements from documents using unstructured.io.
    
    Supports various formats: PDF, DOCX, TXT, HTML, and more.
    Uses configurable strategy from settings (default: fast for speed).
    """
    
    def parse(self, file_path: str) -> List[Any]:
        """
        Parse a document and extract text elements.
        
        Args:
            file_path: Path to the document file to parse.
            
        Returns:
            List of extracted text elements from the document.
            
        Raises:
            FileNotFoundError: If the specified file does not exist.
            DocumentParseError: If parsing fails for any reason.
        """
        # Validate file exists
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"Document file not found: {file_path}")
        
        if not path.is_file():
            raise FileNotFoundError(f"Path is not a file: {file_path}")
        
        try:
            # Use unstructured with configured strategy from settings
            elements = partition(
                filename=str(path),
                strategy=settings.document_parsing_strategy
            )
            return elements
        except Exception as e:
            # Wrap exceptions with clear, actionable message
            raise DocumentParseError(
                f"Failed to parse document '{file_path}': {str(e)}"
            ) from e


class DocumentProcessor:
    """
    Orchestrates document processing with status tracking and deduplication.

    Coordinates DocumentParser, SemanticChunker, and SchemaParser to process
    files while maintaining processing status in SQLite and handling duplicates.
    """

    # File extensions that should use SchemaParser instead of DocumentParser
    SCHEMA_EXTENSIONS = {'.sql', '.ddl'}

    def __init__(
        self,
        chunk_size_chars: int = 2000,
        chunk_overlap_chars: int = 200,
        vector_store: Optional[VectorStore] = None,
        embedding_service: Optional[EmbeddingService] = None,
        pool: Optional['SQLiteConnectionPool'] = None,
        llm_client: Optional[LLMClient] = None,
        contextual_chunker: Optional[ContextualChunker] = None,
    ):
        """
        Initialize the document processor.

        Args:
            chunk_size_chars: Target chunk size in characters for semantic chunking
            chunk_overlap_chars: Overlap between chunks in characters
            vector_store: VectorStore instance for storing chunk embeddings
            embedding_service: EmbeddingService instance for generating embeddings
            pool: SQLiteConnectionPool instance for database connections
            llm_client: LLMClient instance for contextual chunking (optional)
            contextual_chunker: Pre-configured ContextualChunker instance (optional)
        """
        self.parser = DocumentParser()
        self.chunker = SemanticChunker(
            chunk_size_chars=chunk_size_chars,
            chunk_overlap_chars=chunk_overlap_chars
        )
        self.schema_parser = SchemaParser()
        # Fallback to creating a pool from settings if not provided
        if pool is None:
            pool = get_pool(str(settings.sqlite_path), max_size=2)
        self.pool = pool
        self.vector_store = vector_store
        self.embedding_service = embedding_service
        self._llm_client = llm_client
        self._contextual_chunker = contextual_chunker

    def _get_contextual_chunker(self) -> Optional[ContextualChunker]:
        """
        Lazily create a ContextualChunker when needed.

        Returns:
            ContextualChunker instance if contextual_chunking_enabled is True
            and llm_client exists, None otherwise.
        """
        if not settings.contextual_chunking_enabled:
            return None
        if self._llm_client is None:
            logger.warning("Contextual chunking enabled but no LLM client available")
            return None
        if self._contextual_chunker is None:
            self._contextual_chunker = ContextualChunker(self._llm_client)
        return self._contextual_chunker

    @with_retry(max_attempts=3, retry_exceptions=(sqlite3.Error,), raise_last_exception=True)
    def _check_duplicate(self, file_hash: str, conn: sqlite3.Connection, vault_id: int = 1) -> Optional[sqlite3.Row]:
        """
        Check if a file with the given hash already exists and is indexed.

        Args:
            file_hash: The hash of the file to check
            conn: Database connection
            vault_id: The vault ID to check for duplicates in (defaults to 1)

        Returns:
            The existing file row if found and indexed, None otherwise
        """
        cursor = conn.execute(
            "SELECT * FROM files WHERE file_hash = ? AND vault_id = ? AND status = 'indexed'",
            (file_hash, vault_id)
        )
        return cursor.fetchone()

    @with_retry(max_attempts=3, retry_exceptions=(sqlite3.Error,), raise_last_exception=True)
    def _insert_or_get_file_record(
        self,
        file_path: str,
        file_hash: str,
        conn: sqlite3.Connection,
        vault_id: int = 1,
        source: str = 'upload',
        email_subject: Optional[str] = None,
        email_sender: Optional[str] = None,
    ) -> int:
        """
        Insert a new file record or update existing one, returning the file ID.

        Args:
            file_path: Path to the file
            file_hash: Computed hash of the file
            conn: Database connection
            vault_id: The vault ID for the file (defaults to 1)
            source: Source of the file ('upload', 'scan', 'email')
            email_subject: Subject line for email-sourced files
            email_sender: Sender address for email-sourced files

        Returns:
            The file ID (database row ID)

        Raises:
            DocumentProcessingError: If database operations fail
        """
        path = Path(file_path)
        file_name = path.name
        file_size = path.stat().st_size
        file_type = path.suffix.lower() if path.suffix else None
        now = datetime.now(UTC).isoformat()
        path_str = str(file_path)

        try:
            # Check if file record already exists by path
            cursor = conn.execute(
                "SELECT id FROM files WHERE file_path = ?",
                (path_str,)
            )
            existing = cursor.fetchone()

            if existing:
                # Validate existing row id
                existing_id = existing['id']
                if existing_id is None:
                    raise DocumentProcessingError(
                        f"Existing file record for '{path_str}' has invalid NULL id"
                    )
                file_id = int(existing_id)

                # Update existing record
                conn.execute(
                    """UPDATE files
                       SET file_hash = ?, file_size = ?, file_type = ?, vault_id = ?,
                           source = ?, email_subject = ?, email_sender = ?,
                           status = 'pending', error_message = NULL,
                           modified_at = ?, processed_at = NULL
                       WHERE id = ?""",
                    (file_hash, file_size, file_type, vault_id, source, email_subject, email_sender, now, file_id)
                )
            else:
                # Insert new record
                cursor = conn.execute(
                    """INSERT INTO files
                       (file_path, file_name, file_hash, file_size, file_type, vault_id,
                        source, email_subject, email_sender, status, created_at, modified_at)
                       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'pending', ?, ?)""",
                    (path_str, file_name, file_hash, file_size, file_type, vault_id, source, email_subject, email_sender, now, now)
                )
                lastrowid = cursor.lastrowid
                if lastrowid is None:
                    raise DocumentProcessingError(
                        f"Failed to insert file record for '{path_str}': lastrowid is None"
                    )
                file_id = int(lastrowid)

            # Commit within the context of this method
            conn.commit()
            return file_id

        except sqlite3.Error as e:
            # Rollback on error and wrap in DocumentProcessingError
            conn.rollback()
            raise DocumentProcessingError(
                f"Database error while inserting/updating file record for '{path_str}': {str(e)}"
            ) from e

    @with_retry(max_attempts=3, retry_exceptions=(sqlite3.Error,), raise_last_exception=True)
    def _update_status(
        self,
        file_id: int,
        status: str,
        conn: sqlite3.Connection,
        chunk_count: Optional[int] = None,
        error_message: Optional[str] = None
    ) -> None:
        """
        Update the processing status of a file.

        Args:
            file_id: The database ID of the file
            status: New status ('pending', 'processing', 'indexed', 'error')
            conn: Database connection
            chunk_count: Number of chunks produced (optional)
            error_message: Error message if status is 'error' (optional)

        Note:
            This method does not commit - caller is responsible for transaction management.
        """
        now = datetime.now(UTC).isoformat()

        if status == 'indexed':
            conn.execute(
                """UPDATE files
                   SET status = ?, chunk_count = ?, processed_at = ?, modified_at = ?
                   WHERE id = ?""",
                (status, chunk_count, now, now, file_id)
            )
        elif status == 'error':
            conn.execute(
                """UPDATE files
                   SET status = ?, error_message = ?, modified_at = ?
                   WHERE id = ?""",
                (status, error_message, now, file_id)
            )
        else:
            conn.execute(
                """UPDATE files
                   SET status = ?, modified_at = ?
                   WHERE id = ?""",
                (status, now, file_id)
            )
        # Note: No commit here - caller manages transactions

    def _is_schema_file(self, file_path: str) -> bool:
        """
        Check if a file should be processed as a schema file.

        Args:
            file_path: Path to the file

        Returns:
            True if the file has a schema extension (.sql, .ddl)
        """
        return Path(file_path).suffix.lower() in self.SCHEMA_EXTENSIONS

    async def _process_schema_file(self, file_path: str) -> List[ProcessedChunk]:
        """
        Process a schema file using SchemaParser.

        Args:
            file_path: Path to the schema file

        Returns:
            List of ProcessedChunk objects
        """
        schema_chunks = await asyncio.to_thread(self.schema_parser.parse, file_path)

        processed_chunks = []
        for idx, chunk_data in enumerate(schema_chunks):
            chunk = ProcessedChunk(
                text=chunk_data['text'],
                metadata={
                    **chunk_data['metadata'],
                    'chunk_index': idx,
                    'total_chunks': len(schema_chunks),
                    'chunk_scale': 'default'  # Schema files don't use multi-scale
                },
                chunk_index=idx
            )
            processed_chunks.append(chunk)

        return processed_chunks

    async def _process_document_file(
        self,
        file_path: str,
        file_id: Optional[int] = None
    ) -> Tuple[List[ProcessedChunk], str]:
        """
        Process a document file using DocumentParser and SemanticChunker.

        Args:
            file_path: Path to the document file
            file_id: Database ID of the file (required for multi-scale indexing)

        Returns:
            Tuple of (List of ProcessedChunk objects, document text as string)
        """
        elements = await asyncio.to_thread(self.parser.parse, file_path)
        # Join all element texts for use as context in contextual chunking
        document_text = "\n".join([str(e) for e in elements])

        # Check if multi-scale indexing is enabled
        if settings.multi_scale_indexing_enabled:
            # Parse chunk sizes from settings
            scale_strs = settings.multi_scale_chunk_sizes.split(',')
            scales = [int(s.strip()) for s in scale_strs if s.strip()]

            all_chunks = []
            for scale in scales:
                # Create chunker for this scale
                chunk_overlap = int(scale * settings.multi_scale_overlap_ratio)
                scale_chunker = SemanticChunker(
                    chunk_size_chars=scale,
                    chunk_overlap_chars=chunk_overlap
                )

                # Chunk elements with this scale's chunker
                scale_chunks = await asyncio.to_thread(scale_chunker.chunk_elements, elements)

                # Add chunk_scale metadata to each chunk
                for idx, chunk in enumerate(scale_chunks):
                    chunk.metadata['chunk_scale'] = str(scale)
                    # Use scale-aware index format for multi-scale
                    if file_id is not None:
                        chunk.metadata['chunk_index'] = f"{scale}_{idx}"

                all_chunks.extend(scale_chunks)

            total_chunks = len(all_chunks)
            scale_list = [str(s) for s in scales]
            logger.info(
                "Multi-scale chunking processed %d chunks across scales %s",
                total_chunks,
                scale_list
            )

            return all_chunks, document_text
        else:
            # Existing single-scale behavior
            chunks = await asyncio.to_thread(self.chunker.chunk_elements, elements)
            return chunks, document_text

    async def process_file(
        self,
        file_path: str,
        vault_id: int = 1,
        source: str = 'upload',
        email_subject: Optional[str] = None,
        email_sender: Optional[str] = None,
    ) -> ProcessedDocument:
        """
        Process a file with status tracking and deduplication.

        Args:
            file_path: Path to the file to process
            vault_id: The vault ID to associate the file with (defaults to 1)
            source: Source of the file ('upload', 'scan', 'email')
            email_subject: Subject line for email-sourced files
            email_sender: Sender address for email-sourced files

        Returns:
            ProcessedDocument containing file_id and chunks

        Raises:
            FileNotFoundError: If the file does not exist
            DuplicateFileError: If a file with the same hash is already indexed
            DocumentParseError: If parsing fails
        """
        # Validate file exists
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        if not path.is_file():
            raise FileNotFoundError(f"Path is not a file: {file_path}")

        # Compute file hash
        file_hash = compute_file_hash(file_path)

        # Phase 1: Quick DB operations - get connection, do quick ops, release
        conn = self.pool.get_connection()
        try:
            # Check for duplicates
            duplicate = self._check_duplicate(file_hash, conn, vault_id)
            if duplicate:
                raise DuplicateFileError(
                    f"File with hash {file_hash} already indexed as '{duplicate['file_path']}'"
                )

            # Insert or get file record (handles its own commit)
            file_id = self._insert_or_get_file_record(
                file_path, file_hash, conn, vault_id, source, email_subject, email_sender
            )

            # Update status to processing
            self._update_status(file_id, 'processing', conn)
            conn.commit()
        finally:
            # Release connection before long-running operations
            self.pool.release_connection(conn)

        # Phase 2: Long operations (NO connection held!)
        try:
            # Process the file based on type
            if self._is_schema_file(file_path):
                chunks = await self._process_schema_file(file_path)
                document_text = ""
            else:
                chunks, document_text = await self._process_document_file(file_path, file_id)

            # Apply contextual chunking if enabled
            if settings.contextual_chunking_enabled and chunks and document_text:
                chunker = self._get_contextual_chunker()
                if chunker is not None:
                    source_filename = Path(file_path).name
                    logger.info(
                        "Contextual chunking: processing %d chunks for %s",
                        len(chunks),
                        source_filename
                    )
                    try:
                        await chunker.contextualize_chunks(
                            document_text=document_text,
                            chunks=chunks,
                            source_filename=source_filename
                        )
                        logger.info(
                            "Contextual chunking: completed for %s (%d chunks contextualized)",
                            source_filename,
                            len(chunks)
                        )
                    except Exception as e:
                        logger.warning(
                            "Contextual chunking failed for %s: %s",
                            source_filename,
                            str(e)
                        )
                # else: _get_contextual_chunker already logged a warning if needed

            # Generate embeddings and store in vector store
            if self.embedding_service is not None and self.vector_store is not None:
                # Skip embedding/indexing if no chunks (status indexed with 0 chunks is acceptable)
                if chunks:
                    # Extract texts from chunks
                    texts = [c.text for c in chunks]
                    
                    # Check if tri-vector embeddings are supported
                    use_tri_vector = self.embedding_service.supports_tri_vector
                    if use_tri_vector:
                        logger.info("Tri-vector embedding enabled: generating dense + sparse vectors")
                        # Generate tri-vector embeddings (dense + sparse + colbert)
                        tri_vectors = await self.embedding_service.embed_multi(texts)
                        # Validate tri-vectors count matches chunks count
                        if len(tri_vectors) != len(chunks):
                            raise DocumentProcessingError(
                                f"Tri-vector embedding count mismatch: expected {len(chunks)}, got {len(tri_vectors)}"
                            )
                        embeddings = [tv["dense"] for tv in tri_vectors]
                        sparse_embeddings = [tv.get("sparse") for tv in tri_vectors]
                    else:
                        # Generate standard embeddings (dense only)
                        embeddings = await self.embedding_service.embed_batch(texts)
                        sparse_embeddings = [None] * len(chunks)
                    
                    # Validate embeddings count matches chunks count
                    if len(embeddings) != len(chunks):
                        raise DocumentProcessingError(
                            f"Embedding count mismatch: expected {len(chunks)}, got {len(embeddings)}"
                        )
                    # Validate first embedding is a non-empty list
                    if not embeddings[0] or not isinstance(embeddings[0], list):
                        raise DocumentProcessingError(
                            "First embedding is empty or not a list"
                        )
                    # Map chunks to records for vector store
                    records = []
                    for chunk, embedding, sparse_emb in zip(chunks, embeddings, sparse_embeddings):
                        # Determine chunk_scale from metadata (set during chunking)
                        chunk_scale = chunk.metadata.get('chunk_scale', 'default')

                        # Create chunk_uid for windowing support
                        # Use scale-aware format when multi-scale indexing is enabled
                        if settings.multi_scale_indexing_enabled and chunk_scale != 'default':
                            # Multi-scale: format is {file_id}_{scale}_{index}
                            chunk_index_value = chunk.metadata.get('chunk_index', chunk.chunk_index)
                            if isinstance(chunk_index_value, str) and '_' in chunk_index_value:
                                # Already formatted as scale_index from _process_document_file
                                chunk_uid = f"{file_id}_{chunk_index_value}"
                            else:
                                chunk_uid = f"{file_id}_{chunk_scale}_{chunk.chunk_index}"
                        else:
                            # Standard format: {file_id}_{index}
                            chunk_uid = f"{file_id}_{chunk.chunk_index}"
                        
                        # Add chunk_uid to metadata for adjacent chunk lookups
                        chunk_metadata = chunk.metadata.copy()
                        chunk_metadata['chunk_uid'] = chunk_uid
                        chunk_metadata['file_id'] = str(file_id)
                        chunk_metadata['chunk_count'] = chunk.metadata.get('total_chunks', len(chunks))
                        # Ensure chunk_scale is included in metadata
                        chunk_metadata['chunk_scale'] = chunk_scale
                        
                        record = {
                            "id": chunk_uid,
                            "text": chunk.text,
                            "file_id": str(file_id),
                            "chunk_index": chunk.chunk_index,
                            "vault_id": str(vault_id),
                            "metadata": json.dumps(chunk_metadata),
                            "embedding": embedding
                        }
                        # Add sparse embedding if available (tri-vector mode)
                        if sparse_emb is not None:
                            try:
                                record["sparse_embedding"] = json.dumps(sparse_emb)
                            except (TypeError, ValueError) as e:
                                logger.warning(f"Failed to serialize sparse embedding: {e}")
                                record["sparse_embedding"] = None
                        records.append(record)
                    
                    # Log tri-vector usage
                    if use_tri_vector:
                        logger.info(f"Tri-vector embedding: stored {len(records)} chunks with sparse vectors")
                    
                    # Initialize vector table with embedding dimension and add chunks
                    embedding_dim = len(embeddings[0])
                    await asyncio.to_thread(self.vector_store.init_table, embedding_dim)
                    await asyncio.to_thread(self.vector_store.add_chunks, records)
        except Exception as e:
            # Phase 3: Update status to error on failure
            # Get connection again to update error status
            conn = self.pool.get_connection()
            try:
                self._update_status(file_id, 'error', conn, error_message=str(e))
                conn.commit()
            finally:
                self.pool.release_connection(conn)
            raise

        # Phase 3: Final DB operations - update status to indexed
        conn = self.pool.get_connection()
        try:
            self._update_status(file_id, 'indexed', conn, chunk_count=len(chunks))
            conn.commit()
        finally:
            self.pool.release_connection(conn)

        return ProcessedDocument(file_id=file_id, chunks=chunks)
